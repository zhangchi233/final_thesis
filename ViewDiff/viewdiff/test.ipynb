{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/ViewDiff/viewdiff'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustom_stable_diffusion_pipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomStableDiffusionPipeline\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustom_stable_instructPix2pix_pipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomInstructPix2pixDiffusionPipeline\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio_util\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     setup_output_directories,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     make_output_directories,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     convert_to_tensorboard_dict,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     SaveConfig\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_metrics\u001b[39;00m \u001b[39mimport\u001b[39;00m load_lpips_vgg_model\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     replace_self_attention_with_cross_frame_attention,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     update_last_layer_mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     build_cross_attention_kwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Union, Optional, Literal, Tuple\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tyro\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate.logging import get_logger\n",
    "from diffusers import DPMSolverMultistepScheduler, UniPCMultistepScheduler, DDPMScheduler, DDIMScheduler\n",
    "\n",
    "from model.custom_unet_2d_condition import (\n",
    "    UNet2DConditionCrossFrameInExistingAttnModel,\n",
    ")\n",
    "from model.util import (\n",
    "    replace_self_attention_with_cross_frame_attention,\n",
    "    add_pose_cond_to_attention_layers,\n",
    "    update_cross_frame_attention_config,\n",
    "    update_last_layer_mode,\n",
    "    update_vol_rend_inject_noise_sigma,\n",
    "    update_n_novel_images,\n",
    "    CrossFrameAttentionConfig,\n",
    "    ModelConfig,\n",
    ")\n",
    "from model.custom_stable_diffusion_pipeline import CustomStableDiffusionPipeline\n",
    "from model.custom_stable_instructPix2pix_pipeline import CustomInstructPix2pixDiffusionPipeline\n",
    "\n",
    "from .io_util import (\n",
    "    setup_output_directories,\n",
    "    make_output_directories,\n",
    "    convert_to_tensorboard_dict,\n",
    "    SaveConfig\n",
    ")\n",
    "\n",
    "from metrics.image_metrics import load_lpips_vgg_model\n",
    "\n",
    "from .model.util import (\n",
    "    replace_self_attention_with_cross_frame_attention,\n",
    "    update_last_layer_mode,\n",
    "    update_vol_rend_inject_noise_sigma,\n",
    "    update_n_novel_images,\n",
    "    update_cross_frame_attention_config,\n",
    "    add_pose_cond_to_attention_layers,\n",
    "    collapse_prompt_to_batch_dim,\n",
    "    collapse_tensor_to_batch_dim,\n",
    "    expand_output_to_k,\n",
    "    expand_tensor_to_k,\n",
    "    tokenize_captions,\n",
    "    ModelConfig,\n",
    "    CrossFrameAttentionConfig,\n",
    "    build_cross_attention_kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from .train_util import FinetuneConfig\n",
    "from diffusers.loaders import LoraLoaderMixin\n",
    "\n",
    "from dacite import from_dict, Config\n",
    "\n",
    "from .train import test_step\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataconfig:\n",
    "    root_dir =\"/root/autodl-tmp/mvs_training/dtu/\"\n",
    "    split = \"val\"\n",
    "\n",
    "    target_light = 6\n",
    "    n_views:int=3 \n",
    "    levels:int=3 \n",
    "    depth_interval:int =2.65\n",
    "    img_wh:int=None\n",
    "    abs_error:Optional[str] =\"abs\"\n",
    "    output_total:Optional[bool]=False\n",
    "    threshold: Optional[int] = 4.7\n",
    "    prompt_dir: Optional[str] = \"/root/autodl-tmp/mvs_training/dtu/co3d_blip2_captions_final.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/project/dp_simple/')\n",
    "from CasMVSNet_pl.datasets.utils import read_pfm\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "class DTUDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        img_wh should be set to a tuple ex: (1152, 864) to enable test mode!\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_dir = config.root_dir\n",
    "        self.split = config.split\n",
    "        assert self.split in ['train', 'val', 'test'], \\\n",
    "            'split must be either \"train\", \"val\" or \"test\"!'\n",
    "        \n",
    "        \n",
    "        self.light_class = config.target_light\n",
    "        self.img_wh = None\n",
    "\n",
    "        \n",
    "        self.threshold = config.threshold\n",
    "        self.build_metas()\n",
    "        self.n_views = config.n_views\n",
    "        self.levels = config.levels # FPN levels\n",
    "        self.depth_interval = config.depth_interval\n",
    "        self.build_proj_mats()\n",
    "        self.define_transforms()\n",
    "        self.output_total = config.output_total\n",
    "        prompt_dir = config.prompt_dir\n",
    "        if prompt_dir != None:\n",
    "            import json\n",
    "            captions = json.load(open(prompt_dir))\n",
    "        self.prompt_dir =captions\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_metas(self):\n",
    "        self.metas = []\n",
    "        with open(f'/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/datasets/lists/dtu/{self.split}.txt') as f:\n",
    "            self.scans = [line.rstrip() for line in f.readlines()]\n",
    "        output_pkl = f'/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/datasets/lists/dtu/{self.split}_abs.pkl'\n",
    "        import pickle\n",
    "        with open(output_pkl, 'rb') as f:\n",
    "            self.output_pkl = pickle.load(f)\n",
    "        # light conditions 0-6 for training\n",
    "        # light condition 3 for testing (the brightest?)\n",
    "        outputs_total = {}\n",
    "        for scan in self.output_pkl.keys():\n",
    "            scan_index = scan.split('_')[0]\n",
    "            if scan_index not in outputs_total:\n",
    "                outputs_total[scan_index] = []\n",
    "            outputs_total[scan_index].append(self.output_pkl[scan])\n",
    "        for scan in outputs_total.keys():\n",
    "            outputs_total[scan] = np.mean(np.array(outputs_total[scan]), axis=0)\n",
    "            print(f\"scan {scan} mean output: {outputs_total[scan]}\")\n",
    "        self.total_pkl = outputs_total\n",
    "\n",
    "\n",
    "        light_idxs = list(range(7))\n",
    "\n",
    "        pair_file = \"Cameras/pair.txt\"\n",
    "        for scan in self.scans:\n",
    "            with open(os.path.join(self.root_dir, pair_file)) as f:\n",
    "                num_viewpoint = int(f.readline())\n",
    "                # viewpoints (49)\n",
    "                for _ in range(num_viewpoint):\n",
    "                    ref_view = int(f.readline().rstrip())\n",
    "                    src_views = [int(x) for x in f.readline().rstrip().split()[1::2]]\n",
    "                    \n",
    "\n",
    "                    for light_idx in light_idxs:\n",
    "                        output_key = f\"{scan}_{ref_view}_{src_views[0]}_{src_views[1]}\"\n",
    "                        losses = self.output_pkl[output_key]\n",
    "                        if np.argmin(losses)==self.light_class and self.split==\"train\":\n",
    "                            self.metas += [(scan, ref_view,light_idx, src_views,int(np.argmin(losses)))]\n",
    "                        elif self.split!=\"train\":\n",
    "                            if light_idx!=0:\n",
    "                                continue\n",
    "                            else:\n",
    "                                self.metas += [(scan, ref_view,light_idx, src_views,int(np.argmin(losses)))]\n",
    "                                \n",
    "                                \n",
    "                           \n",
    "                         \n",
    "    def build_proj_mats(self):\n",
    "        proj_mats = []\n",
    "        for vid in range(49): # total 49 view ids\n",
    "            if self.img_wh is None:\n",
    "                proj_mat_filename = os.path.join(self.root_dir,\n",
    "                                                 f'Cameras/train/{vid:08d}_cam.txt')\n",
    "            else:\n",
    "                proj_mat_filename = os.path.join(self.root_dir,\n",
    "                                                 f'Cameras/{vid:08d}_cam.txt')\n",
    "            intrinsics, extrinsics, depth_min = \\\n",
    "                self.read_cam_file(proj_mat_filename)\n",
    "            if self.img_wh is not None: # resize the intrinsics to the coarsest level\n",
    "                intrinsics[0] *= self.img_wh[0]/1600/4\n",
    "                intrinsics[1] *= self.img_wh[1]/1200/4\n",
    "            K = intrinsics\n",
    "            R = extrinsics\n",
    "            # multiply intrinsics and extrinsics to get projection matrix\n",
    "            proj_mat_ls = []\n",
    "            for l in reversed(range(self.levels)):\n",
    "                proj_mat_l = np.eye(4)\n",
    "                proj_mat_l[:3, :4] = intrinsics @ extrinsics[:3, :4]\n",
    "                intrinsics[:2] *= 2 # 1/4->1/2->1\n",
    "                proj_mat_ls += [torch.FloatTensor(proj_mat_l)]\n",
    "            # (self.levels, 4, 4) from fine to coarse\n",
    "            proj_mat_ls = torch.stack(proj_mat_ls[::-1])\n",
    "           \n",
    "            proj_mats += [(proj_mat_ls, depth_min,K,R)]\n",
    "\n",
    "        self.proj_mats = proj_mats\n",
    "\n",
    "    def read_cam_file(self, filename):\n",
    "        with open(filename) as f:\n",
    "            lines = [line.rstrip() for line in f.readlines()]\n",
    "        # extrinsics: line [1,5), 4x4 matrix\n",
    "        extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n",
    "        extrinsics = extrinsics.reshape((4, 4))\n",
    "        # intrinsics: line [7-10), 3x3 matrix\n",
    "        intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n",
    "        intrinsics = intrinsics.reshape((3, 3))\n",
    "        # depth_min & depth_interval: line 11\n",
    "        depth_min = float(lines[11].split()[0])\n",
    "        return intrinsics, extrinsics, depth_min\n",
    "\n",
    "    def read_depth(self, filename):\n",
    "        depth = np.array(read_pfm(filename)[0], dtype=np.float32) # (1200, 1600)\n",
    "        if self.img_wh is None:\n",
    "            depth = cv2.resize(depth, None, fx=0.5, fy=0.5,\n",
    "                            interpolation=cv2.INTER_NEAREST) # (600, 800)\n",
    "            depth_0 = depth[44:556, 80:720] # (512, 640)\n",
    "        else:\n",
    "            depth_0 = cv2.resize(depth, self.img_wh,\n",
    "                                 interpolation=cv2.INTER_NEAREST)\n",
    "        depth_1 = cv2.resize(depth_0, None, fx=0.5, fy=0.5,\n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "        depth_2 = cv2.resize(depth_1, None, fx=0.5, fy=0.5,\n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        depths = {\"level_0\": torch.FloatTensor(depth_0),\n",
    "                  \"level_1\": torch.FloatTensor(depth_1),\n",
    "                  \"level_2\": torch.FloatTensor(depth_2)}\n",
    "        \n",
    "        return depths\n",
    "\n",
    "    def read_mask(self, filename):\n",
    "        mask = cv2.imread(filename, 0) # (1200, 1600)\n",
    "       \n",
    "        if self.img_wh is None:\n",
    "            mask = cv2.resize(mask, None, fx=0.5, fy=0.5,\n",
    "                            interpolation=cv2.INTER_NEAREST) # (600, 800)\n",
    "            mask_0 = mask[44:556, 80:720] # (512, 640)\n",
    "        else:\n",
    "            mask_0 = cv2.resize(mask, self.img_wh,\n",
    "                                interpolation=cv2.INTER_NEAREST)\n",
    "        mask_1 = cv2.resize(mask_0, None, fx=0.5, fy=0.5,\n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "        mask_2 = cv2.resize(mask_1, None, fx=0.5, fy=0.5,\n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        masks = {\"level_0\": torch.BoolTensor(mask_0),\n",
    "                 \"level_1\": torch.BoolTensor(mask_1),\n",
    "                 \"level_2\": torch.BoolTensor(mask_2)}\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def define_transforms(self):\n",
    "        if self.split == 'train': # you can add augmentation here\n",
    "            self.transform = T.Compose([T.ToTensor(),\n",
    "                                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                    std=[0.229, 0.224, 0.225]),\n",
    "                                       ])\n",
    "        else:\n",
    "            self.transform = T.Compose([T.ToTensor(),\n",
    "                                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                    std=[0.229, 0.224, 0.225]),\n",
    "                                       ])\n",
    "        self.unpreprocess = T.Compose([\n",
    "            T.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "            T.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
    "        ])\n",
    "    \n",
    "    def decode_batch(self, batch):\n",
    "        imgs = batch['imgs']\n",
    "        proj_mats = batch['proj_mats']\n",
    "        depths = batch['depths']\n",
    "        masks = batch['masks']\n",
    "        init_depth_min = batch['init_depth_min']\n",
    "        depth_interval = batch['depth_interval']\n",
    "        return imgs, proj_mats, depths, masks, init_depth_min, depth_interval\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metas)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def  __getitem__(self, idx):\n",
    "       \n",
    "        scan, ref_view,light_idx, src_views,target_light = self.metas[idx]\n",
    "        # use only the reference view and first nviews-1 source views\n",
    "        view_ids = [ref_view] + src_views[:self.n_views-1]\n",
    "\n",
    "        # output_key = f\"{scan}_{ref_view}_{src_views[0]}_{src_views[1]}\"\n",
    "        # if self.total_pkl:\n",
    "        #     target_light = self.total_pkl[scan]\n",
    "        #     target_light = np.argmin(target_light)\n",
    "        # else:\n",
    "        #     target_light = self.output_pkl[output_key]\n",
    "        #     target_light = np.argmin(target_light)\n",
    "\n",
    "        \n",
    "\n",
    "        sample = {}\n",
    "        imgs = []\n",
    "        cams = []\n",
    "        proj_mats = []\n",
    "        target_imgs = []\n",
    "        Ks = []\n",
    "        Rs = []\n",
    "        intensity_stats =[]\n",
    "        prompt = str(np.random.choice(self.prompt_dir[scan][str(ref_view)],1)[0])\n",
    "         \n",
    "        sample['prompt'] = [f\"modify the lightness of image to light_class_{self.light_class} style\"]\n",
    "        for i, vid in enumerate(view_ids):\n",
    "        # NOTE that the id in image file names is from 1 to 49 (not 0~48)\n",
    "        \n",
    "            img_filename = os.path.join(self.root_dir,\n",
    "                            f'Rectified/{scan}_train/rect_{vid+1:03d}_{light_idx}_r5000.png')\n",
    "            target_filename = os.path.join(self.root_dir,\n",
    "                            f'Rectified/{scan}_train/rect_{vid+1:03d}_{self.light_class}_r5000.png')\n",
    "            mask_filename = os.path.join(self.root_dir,\n",
    "                            f'Depths/{scan}/depth_visual_{vid:04d}.png')\n",
    "            depth_filename = os.path.join(self.root_dir,\n",
    "                            f'Depths/{scan}/depth_map_{vid:04d}.pfm')\n",
    "    \n",
    "\n",
    "            img = Image.open(img_filename)\n",
    "            target_img = Image.open(target_filename)\n",
    "            if self.img_wh is not None:\n",
    "                img = img.resize(self.img_wh, Image.BILINEAR)\n",
    "                target_img = target_img.resize(self.img_wh, Image.BILINEAR)\n",
    "\n",
    "            img = self.transform(img)\n",
    "            target_img = self.transform(target_img)\n",
    "            imgs += [img]\n",
    "            target_imgs += [target_img]\n",
    "\n",
    "            proj_mat_ls, depth_min,K,R = self.proj_mats[vid]\n",
    "            Ks += [K]\n",
    "            Rs += [R]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            if i == 0:  # reference view\n",
    "                \n",
    "                sample['init_depth_min'] = torch.FloatTensor([depth_min])\n",
    "                \n",
    "                sample['masks'] = self.read_mask(mask_filename)\n",
    "                for key in sample['masks']:\n",
    "                    sample['masks'][key] = sample['masks'][key]\n",
    "                sample['depths'] = self.read_depth(depth_filename)\n",
    "                for key in sample['depths']:\n",
    "                    sample['depths'][key] = sample['depths'][key]\n",
    "                sample[\"depth\"] = sample[\"depths\"][\"level_0\"]\n",
    "                ref_proj_inv = torch.inverse(proj_mat_ls)\n",
    "            else:\n",
    "                \n",
    "                proj_mats += [proj_mat_ls @ ref_proj_inv]\n",
    "            var, mean = torch.var_mean(img)\n",
    "            intensity_stat = torch.stack([mean, var], dim=0)\n",
    "            intensity_stats.append(intensity_stat)\n",
    "    \n",
    "    \n",
    "        imgs = torch.stack(imgs) # (V, 3, H, W)\n",
    "        target_imgs = torch.stack(target_imgs)\n",
    "        proj_mats = torch.stack(proj_mats)[:,:,:3] # (V-1, self.levels, 3, 4) from fine to coarse\n",
    "        \n",
    "        imgs = self.unpreprocess(imgs)\n",
    "        target_imgs = self.unpreprocess(target_imgs)\n",
    "       \n",
    "        Ks = np.stack(Ks)\n",
    "        Rs = np.stack(Rs)\n",
    "        sample['pose'] = torch.tensor(Rs)\n",
    "        sample['K'] = torch.tensor(Ks)\n",
    "        sample['images'] = imgs\n",
    "        sample[\"intensity_stats\"] = torch.stack(intensity_stats)\n",
    "        sample['proj_mats'] = proj_mats\n",
    "        sample['depth_interval'] = torch.FloatTensor([self.depth_interval])\n",
    "        sample['scan_vid'] = (scan, ref_view)\n",
    "        \n",
    "\n",
    "        sample['target_imgs'] = target_imgs\n",
    "        sample[\"bbox\"] =torch.tensor([[-1, -1, -1], [1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2805)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_data[0][\"images\"]-val_data[0][\"target_imgs\"]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan scan3 mean output: [3.05263341 2.92222905 2.79727794 2.86347311 2.84872966 2.86011306\n",
      " 2.89029897]\n",
      "scan scan5 mean output: [1.27925969 1.21662743 1.19024779 1.17714057 1.17585228 1.13693983\n",
      " 1.13944231]\n",
      "scan scan17 mean output: [4.23918102 4.3646479  4.26873698 4.152941   4.16164234 4.44407791\n",
      " 4.15546879]\n",
      "scan scan21 mean output: [6.7044504  6.78296111 6.79752936 6.78882487 6.74292021 6.64370545\n",
      " 6.71224226]\n",
      "scan scan28 mean output: [8.33500304 8.26974845 8.08564879 7.80853114 7.63656337 7.53135754\n",
      " 7.33638762]\n",
      "scan scan35 mean output: [1.18667069 1.32775589 1.20760566 1.06743625 1.15983699 0.91010288\n",
      " 0.87788923]\n",
      "scan scan37 mean output: [21.1885944  21.17826699 21.25191359 21.18731575 20.97766127 20.14655309\n",
      " 19.91200681]\n",
      "scan scan38 mean output: [1.69360119 1.60652492 1.57134287 1.54112932 1.56236016 1.63116413\n",
      " 1.62020702]\n",
      "scan scan40 mean output: [3.64535844 3.69948315 3.71793726 3.69029421 3.68635351 3.71370282\n",
      " 3.58273512]\n",
      "scan scan43 mean output: [2.2589177  2.24700747 2.22983368 2.24613731 2.19530878 2.33009434\n",
      " 2.42131581]\n",
      "scan scan56 mean output: [2.31583979 2.17296925 2.15377764 2.14338661 2.1395326  2.20913124\n",
      " 2.29832114]\n",
      "scan scan59 mean output: [2.25470706 2.34287849 2.48965601 2.58392481 2.6572768  2.5697732\n",
      " 2.31332584]\n",
      "scan scan66 mean output: [1.92438559 2.08761549 1.80486628 1.61805749 1.56760059 1.44017948\n",
      " 1.4255248 ]\n",
      "scan scan67 mean output: [3.06216342 3.51998134 3.27188233 3.01847513 3.0300974  3.01376892\n",
      " 2.77910937]\n",
      "scan scan82 mean output: [8.87980272 9.1904472  9.19312354 9.1552566  9.26186729 9.03365678\n",
      " 7.82553422]\n",
      "scan scan86 mean output: [16.68869462 13.0623592  11.09205004 10.73961877 10.67555843 10.71248814\n",
      " 11.94098382]\n",
      "scan scan106 mean output: [1.91600927 1.62643204 1.56631822 1.54953243 1.58691446 1.61378269\n",
      " 1.72092495]\n",
      "scan scan117 mean output: [1.6601294  1.54156241 1.50466732 1.4715858  1.46958674 1.49934888\n",
      " 1.62797041]\n"
     ]
    }
   ],
   "source": [
    "val_data = DTUDataset(dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class runfig:\n",
    "    pretrained_model_name_or_path = \"/root/autodl-tmp/ViewDiff/output_var_second/all/subset_all/input_3/train/class6/saved_model_from_checkpoint-15000/\"\n",
    "    n_input_images =3\n",
    "    n_output_noise =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = config_path = os.path.join(runfig.pretrained_model_name_or_path, \"config.json\")\n",
    "if not os.path.isfile(str(config_path)):\n",
    "        raise ValueError(\"cannot find config.json in \", config_path)\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_data = json.load(f)\n",
    "finetune_config = from_dict(FinetuneConfig, data=config_data, config=Config(cast=[tuple, int]))\n",
    "runfig.cross_frame_attention = finetune_config.cross_frame_attention\n",
    "runfig.model = finetune_config.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c5cd1eb8b64728b0c62762c0dac308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionCrossFrameInExistingAttnModel: \n",
      " ['down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight']\n"
     ]
    }
   ],
   "source": [
    "pipeline = CustomInstructPix2pixDiffusionPipeline.from_pretrained(\n",
    "        runfig.pretrained_model_name_or_path\n",
    "    )\n",
    "pipeline.scheduler.config.prediction_type = finetune_config.training.noise_prediction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({},\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.0085, -0.0219,  0.0140,  ..., -0.0102, -0.0116,  0.0034],\n",
       "          [-0.0202, -0.0160,  0.0122,  ..., -0.0062, -0.0178, -0.0263],\n",
       "          [-0.0167,  0.0100,  0.0129,  ...,  0.0100,  0.0184, -0.0160],\n",
       "          ...,\n",
       "          [ 0.0174, -0.0044, -0.0089,  ...,  0.0210, -0.0241,  0.0076],\n",
       "          [-0.0103,  0.0065,  0.0131,  ..., -0.0170, -0.0080,  0.0261],\n",
       "          [ 0.0270,  0.0232, -0.0066,  ...,  0.0250,  0.0082, -0.0013]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-2.6313e-03, -2.4328e-02,  1.2488e-02, -1.7717e-02,  2.6822e-02,\n",
       "          -1.9169e-02, -8.4139e-03,  8.1025e-03,  2.6243e-02, -1.4524e-02,\n",
       "          -2.0218e-02, -1.5498e-02, -2.4912e-02,  2.3118e-02,  2.0096e-02,\n",
       "          -2.4976e-02,  1.6888e-02,  8.9045e-03, -2.1103e-02, -1.9572e-03,\n",
       "           5.5837e-03,  8.4881e-03, -4.9214e-03, -1.1827e-02, -1.1982e-03,\n",
       "          -1.0095e-02, -1.0001e-02, -2.1398e-02, -2.4084e-02,  1.4497e-02,\n",
       "          -2.6465e-02, -1.7778e-02, -2.0267e-02,  2.5324e-03, -1.2374e-02,\n",
       "           2.3177e-02, -1.4907e-02,  1.5469e-02, -1.9620e-02, -1.0237e-02,\n",
       "          -9.4514e-03, -6.2413e-04, -4.2223e-03, -1.4584e-02,  9.3761e-03,\n",
       "          -1.0917e-02, -1.0062e-02, -5.4914e-03,  2.0726e-02, -4.3932e-03,\n",
       "           5.9742e-03, -1.7077e-02, -2.7466e-02, -2.4272e-02,  5.6803e-03,\n",
       "           9.0903e-03,  9.3968e-03, -1.3377e-02, -4.4437e-03, -2.1644e-02,\n",
       "           2.5918e-02,  7.3751e-05,  1.5664e-02, -1.3291e-02, -3.0081e-04,\n",
       "           5.5391e-03,  4.0905e-03,  5.0027e-03, -3.8603e-03,  1.7666e-02,\n",
       "           1.0370e-02, -9.3858e-03, -2.6749e-02, -3.9896e-03, -1.1135e-02,\n",
       "          -2.4716e-02,  2.1081e-02, -9.4410e-03,  3.3334e-03,  1.8987e-02,\n",
       "          -1.8451e-02, -8.8703e-03, -1.4018e-02, -9.1464e-03, -6.4064e-03,\n",
       "          -7.1824e-03,  3.5797e-03,  9.0061e-03,  1.2479e-02, -4.5199e-04,\n",
       "          -1.8599e-02,  2.0622e-02, -3.5963e-03, -1.8492e-03, -2.7036e-02,\n",
       "           1.1785e-03, -2.7861e-02,  1.5243e-02,  6.4714e-04, -2.0677e-02,\n",
       "           7.4415e-03, -2.7233e-02,  2.0251e-03, -2.4825e-02, -2.3422e-02,\n",
       "          -1.4688e-03, -1.3378e-02,  2.4677e-02,  2.7562e-02, -2.7571e-02,\n",
       "          -9.3312e-03, -1.2741e-02,  1.6352e-02,  1.0157e-02, -2.4431e-02,\n",
       "          -2.5347e-02,  7.3753e-03,  9.5573e-03,  7.3134e-03,  1.4210e-04,\n",
       "           1.7499e-02,  7.5539e-03, -8.8647e-03,  1.8750e-02, -3.7957e-03,\n",
       "          -7.0142e-03,  2.4679e-02,  2.1066e-02, -5.5084e-03, -2.4893e-03,\n",
       "           9.9149e-03, -1.5747e-02,  7.6991e-03, -2.1415e-02, -2.4916e-02,\n",
       "          -1.5655e-02,  1.6638e-02, -2.3990e-02, -8.3286e-04,  2.4059e-02,\n",
       "           1.8996e-02,  1.9512e-03,  1.7958e-02,  2.4795e-02,  1.8950e-02,\n",
       "           1.4302e-02,  3.1051e-03,  1.4722e-02,  2.2239e-02, -1.8465e-02,\n",
       "           2.6680e-02,  2.2784e-02,  1.4366e-02, -1.7626e-02,  4.8393e-03,\n",
       "          -1.5920e-02, -2.1118e-02,  2.2426e-02, -5.7079e-03,  1.9743e-02,\n",
       "           2.2919e-02,  2.0193e-02, -1.0907e-03,  5.3977e-03, -2.6274e-02,\n",
       "          -1.1633e-02, -1.2360e-02, -2.5841e-02, -2.2740e-02,  2.4520e-02,\n",
       "          -5.6593e-03,  2.5457e-02, -2.7851e-02,  1.3968e-02, -1.7489e-02,\n",
       "          -1.6368e-03, -2.7312e-02,  1.9257e-02,  1.0585e-02,  1.7122e-02,\n",
       "           1.2834e-02, -2.7119e-02, -2.5803e-02, -2.0071e-02,  8.5074e-03,\n",
       "           2.6237e-02,  2.1367e-02, -1.4003e-02, -9.4984e-03,  7.3959e-03,\n",
       "          -2.1392e-03,  5.7913e-03,  1.8097e-02,  1.8508e-02, -1.7599e-02,\n",
       "          -2.5854e-02, -1.2610e-02, -1.0680e-02,  2.2462e-02, -2.3771e-02,\n",
       "           1.5191e-02,  1.0287e-02, -7.4976e-03, -1.0291e-02, -9.9999e-03,\n",
       "          -4.0198e-03, -2.4254e-02, -2.6227e-02, -2.5196e-02, -2.4510e-02,\n",
       "          -2.7926e-02,  1.8237e-02,  1.7102e-02,  2.0178e-02,  5.7405e-03,\n",
       "           4.7856e-03, -1.8895e-02,  2.4935e-02, -2.1256e-03, -1.1832e-02,\n",
       "           1.9952e-02, -1.5687e-02,  2.4794e-02,  1.4422e-02,  2.9344e-03,\n",
       "           1.5414e-02,  5.6720e-03, -2.2694e-02,  9.7389e-03, -2.2394e-02,\n",
       "           1.3008e-02, -5.5353e-03, -4.1145e-03, -1.8951e-02, -3.7085e-04,\n",
       "           1.1929e-02, -6.5194e-03,  1.7410e-02,  1.5095e-02, -2.3740e-02,\n",
       "          -1.7886e-04, -2.4431e-02,  7.5414e-03,  6.6818e-04,  2.3992e-03,\n",
       "           1.4623e-02,  1.5755e-02, -1.6945e-02, -6.6164e-03,  2.5831e-03,\n",
       "           5.2128e-03, -8.3133e-03, -2.0260e-02, -1.3577e-02, -1.9585e-02,\n",
       "           7.0748e-03, -1.3878e-02,  1.9490e-02, -2.2293e-02, -1.1124e-02,\n",
       "          -5.9090e-03, -1.8888e-02,  5.3064e-03, -1.4450e-02, -1.2144e-02,\n",
       "          -3.2434e-03, -1.0861e-02,  1.8732e-02,  1.9239e-02, -1.9491e-02,\n",
       "          -8.8105e-03,  8.9416e-04,  7.6161e-03,  1.5486e-02,  1.8403e-02,\n",
       "          -9.6015e-03, -2.3079e-02, -5.7674e-03,  5.4905e-03, -8.4951e-03,\n",
       "           9.0003e-03,  1.2570e-02,  1.0735e-03,  1.3238e-02,  2.6189e-02,\n",
       "           1.7810e-02,  1.1062e-02,  2.7710e-03, -2.0595e-02,  3.3362e-04,\n",
       "           2.5126e-02,  6.7292e-04, -1.5604e-02, -8.4043e-03,  1.9726e-02,\n",
       "           1.7857e-02, -2.6549e-02,  8.6385e-03, -8.9237e-03, -1.7078e-02,\n",
       "           2.6991e-02,  7.7845e-03, -5.7851e-03, -1.1905e-02, -1.1554e-02,\n",
       "           1.6735e-02, -2.2267e-02, -1.2288e-02,  1.9697e-02,  1.6638e-02,\n",
       "          -1.0674e-02, -2.0759e-02, -1.2992e-02, -2.2213e-02,  1.5934e-02,\n",
       "           1.9614e-02,  4.8346e-03,  5.7084e-03,  2.7170e-02, -1.0999e-02,\n",
       "           1.7331e-02,  1.4478e-02, -2.6656e-02,  1.4404e-02, -3.9271e-03,\n",
       "           6.4340e-03, -5.7740e-03, -7.5810e-03,  1.1888e-02,  2.5850e-02,\n",
       "           2.6210e-02, -1.5445e-02,  1.8629e-02,  1.2418e-02,  9.5760e-03,\n",
       "          -1.3345e-02, -1.2357e-02,  6.7509e-03,  1.2375e-02,  2.1618e-02,\n",
       "          -1.2492e-02,  1.2742e-02,  2.7872e-02, -1.7447e-02, -1.5713e-02,\n",
       "          -1.5574e-03,  3.5863e-03, -1.4346e-02, -2.1103e-02,  2.4307e-02,\n",
       "           2.6784e-02, -2.2966e-02, -1.6533e-02, -2.7508e-03, -1.9738e-02,\n",
       "          -4.7219e-03, -2.3139e-02, -1.0159e-02,  2.8863e-03, -1.7451e-02,\n",
       "          -1.6438e-02,  1.2867e-02,  1.2170e-02, -1.5497e-03, -1.8530e-02,\n",
       "          -1.1088e-03,  2.0399e-02,  1.3791e-02, -2.7826e-02, -2.6259e-04,\n",
       "          -2.4836e-02, -1.3976e-02,  1.3518e-02, -2.1447e-04, -1.3061e-02,\n",
       "          -1.8381e-02,  1.8993e-02,  2.7592e-02,  2.1016e-02, -1.0167e-02,\n",
       "           2.6501e-02,  1.6226e-02,  6.5928e-03, -1.2267e-02,  1.5950e-02,\n",
       "          -5.1520e-03,  7.7402e-03,  2.2953e-02,  1.1114e-02,  1.1995e-02,\n",
       "           2.5860e-02,  1.5272e-02,  2.4543e-02, -2.1422e-02, -7.0204e-03,\n",
       "          -6.9204e-03,  1.1495e-03, -5.9419e-03, -3.5482e-03,  2.7595e-02,\n",
       "          -9.0525e-03,  1.0484e-02,  1.1632e-02, -2.0789e-02, -1.9190e-02,\n",
       "          -2.4498e-04, -5.7126e-03, -1.3333e-02,  1.2007e-02,  2.0940e-02,\n",
       "          -1.4838e-02,  1.3323e-02,  1.8175e-02,  1.7010e-02,  1.3654e-02,\n",
       "           2.0771e-02,  2.4039e-02, -7.1553e-03,  2.6459e-02,  1.6955e-03,\n",
       "           2.5814e-02,  2.6973e-02, -1.1615e-02, -2.2804e-02, -7.5661e-03,\n",
       "          -2.5710e-02, -2.1568e-02,  9.3312e-03, -2.2287e-02, -6.0362e-03,\n",
       "           2.4099e-02, -5.0800e-03, -7.9184e-03,  1.2328e-02, -2.0970e-02,\n",
       "           2.7231e-02, -2.2770e-03, -2.6335e-02,  1.0878e-02,  1.7879e-02,\n",
       "          -2.1842e-02, -1.5271e-02,  1.7736e-02, -1.2282e-02,  2.1675e-02,\n",
       "           2.1668e-02, -1.4380e-02,  1.3358e-02, -4.4074e-03, -2.3039e-03,\n",
       "          -1.6182e-02,  1.4911e-02, -2.4342e-02,  1.2078e-02,  2.3650e-02,\n",
       "          -1.9937e-04,  2.3526e-02, -2.6559e-03, -2.3597e-02, -1.0378e-02,\n",
       "           4.7773e-04,  1.7854e-02,  1.7636e-02, -6.0352e-03,  9.2755e-03,\n",
       "           2.1452e-02,  5.6089e-03, -9.6178e-03,  3.1558e-04, -1.5082e-02,\n",
       "          -1.4119e-02,  8.3319e-03,  7.0582e-03, -7.4155e-03, -2.5998e-02,\n",
       "          -2.7382e-02, -3.3133e-03, -2.4462e-02, -1.8609e-02, -1.4950e-02,\n",
       "           2.4833e-02, -2.1810e-02,  1.0257e-02,  1.7001e-02, -1.8757e-02,\n",
       "          -2.0602e-02, -2.6744e-02,  8.3670e-03, -2.7732e-02,  1.0080e-02,\n",
       "          -1.4061e-03, -1.5712e-02,  2.2502e-02,  3.7213e-03,  1.3961e-03,\n",
       "          -3.0354e-03,  2.4726e-02,  1.6350e-02, -2.7047e-02, -2.6435e-02,\n",
       "           4.7601e-03, -7.2728e-03, -1.8360e-02, -2.4659e-02,  2.7747e-02,\n",
       "          -1.1359e-02, -3.0167e-03, -1.9806e-02,  3.6052e-03, -1.5037e-02,\n",
       "           2.6464e-02, -1.0297e-02, -1.5247e-02, -1.6452e-03,  2.4966e-02,\n",
       "          -1.4180e-02, -2.4753e-02,  1.8877e-03, -1.2289e-02, -1.0787e-02,\n",
       "           1.1072e-02,  1.7731e-02,  1.4714e-03, -2.2640e-03,  5.3349e-03,\n",
       "          -1.2659e-02,  2.0221e-02, -1.8151e-02,  1.9434e-02,  1.4468e-02,\n",
       "          -2.0570e-02,  1.8586e-02,  3.8707e-03,  2.0421e-02, -1.4456e-04,\n",
       "           1.6042e-02,  3.4607e-03, -2.1908e-02, -2.7848e-02, -7.6629e-03,\n",
       "           2.1354e-02, -1.0020e-02, -2.3573e-03,  7.4848e-03, -2.6131e-02,\n",
       "          -2.6020e-02,  1.3121e-02,  2.5540e-02,  4.2316e-03,  5.1886e-03,\n",
       "          -2.3784e-03,  1.6071e-02, -7.8766e-03,  3.7658e-03,  4.6553e-03,\n",
       "          -1.4238e-02, -1.9603e-02,  8.0666e-03,  6.2632e-05,  3.2260e-03,\n",
       "           1.3998e-02,  2.3376e-02,  1.0554e-02, -8.2237e-03,  1.6805e-03,\n",
       "          -9.3202e-03, -8.1574e-04,  1.0261e-02,  2.5482e-02,  1.5369e-03,\n",
       "           1.8445e-02,  2.8840e-03,  1.1795e-02,  1.6344e-02,  4.8498e-03,\n",
       "          -7.0432e-03,  2.7388e-02,  1.1986e-02,  1.5418e-02,  1.2183e-02,\n",
       "          -1.9937e-02, -2.3744e-02, -1.6073e-02, -5.4348e-04,  2.2694e-02,\n",
       "           7.3344e-03, -1.1006e-02,  2.2777e-03, -1.4861e-02, -2.2426e-02,\n",
       "          -1.3304e-02, -1.3382e-02, -1.9333e-02,  3.2442e-03, -1.6442e-02,\n",
       "           2.3485e-02, -8.6130e-03, -1.3462e-02, -1.2531e-02,  2.7282e-02,\n",
       "          -1.4034e-02, -2.1369e-02, -1.5805e-02, -1.3217e-02,  2.3579e-02,\n",
       "           2.1236e-03,  2.7545e-03,  1.6189e-03,  2.1147e-02,  1.7391e-02,\n",
       "           1.2716e-02,  2.7225e-02,  1.3911e-03, -2.7532e-02, -2.2117e-02,\n",
       "          -2.4926e-02,  1.0937e-02, -1.4709e-02,  2.1297e-02,  3.2582e-03,\n",
       "           1.3113e-03, -1.2517e-02,  2.5498e-02,  2.7192e-02,  1.4323e-02,\n",
       "           1.8162e-02,  1.0151e-02,  9.6300e-04,  3.9606e-03,  2.6718e-02,\n",
       "          -9.1645e-03, -2.6284e-02,  1.9543e-02, -2.2188e-02,  1.1200e-02,\n",
       "          -2.3705e-02, -5.9599e-03,  2.0127e-02, -2.7430e-02,  1.7806e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0077, -0.0160, -0.0083,  ..., -0.0266,  0.0217, -0.0318],\n",
       "          [ 0.0263, -0.0158, -0.0157,  ..., -0.0169,  0.0227,  0.0248],\n",
       "          [ 0.0368,  0.0021, -0.0009,  ..., -0.0370, -0.0137, -0.0121],\n",
       "          ...,\n",
       "          [ 0.0187,  0.0093, -0.0198,  ..., -0.0340,  0.0135, -0.0342],\n",
       "          [-0.0012,  0.0334, -0.0148,  ..., -0.0243,  0.0383, -0.0283],\n",
       "          [-0.0077,  0.0032, -0.0141,  ...,  0.0391,  0.0239,  0.0176]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0215, -0.0347, -0.0372,  0.0245,  0.0109, -0.0311, -0.0247, -0.0161],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0060,  0.0055,  0.0044,  ..., -0.0093, -0.0086, -0.0011],\n",
       "          [ 0.0052,  0.0107, -0.0484,  ...,  0.0017, -0.0235, -0.0276],\n",
       "          [-0.0091,  0.0117,  0.0125,  ..., -0.0042, -0.0026, -0.0043],\n",
       "          ...,\n",
       "          [ 0.0191,  0.0272, -0.0010,  ...,  0.0301,  0.0181,  0.0078],\n",
       "          [-0.0112, -0.0067, -0.0094,  ..., -0.0028,  0.0069, -0.0121],\n",
       "          [-0.0079,  0.0310, -0.0094,  ...,  0.0287,  0.0176,  0.0008]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0180,  0.0040, -0.0166,  ...,  0.0224,  0.0211,  0.0037],\n",
       "          [-0.0043,  0.0092,  0.0117,  ..., -0.0145,  0.0036, -0.0061],\n",
       "          [-0.0114,  0.0061, -0.0131,  ..., -0.0104,  0.0045,  0.0030],\n",
       "          ...,\n",
       "          [ 0.0069,  0.0091,  0.0288,  ...,  0.0082, -0.0072,  0.0131],\n",
       "          [-0.0198, -0.0115,  0.0014,  ..., -0.0070, -0.0021, -0.0023],\n",
       "          [-0.0056,  0.0052,  0.0047,  ...,  0.0162,  0.0181, -0.0194]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0221,  0.0049, -0.0159,  ..., -0.0270, -0.0042,  0.0130],\n",
       "          [ 0.0167, -0.0084,  0.0063,  ..., -0.0088, -0.0166, -0.0013],\n",
       "          [-0.0075, -0.0056,  0.0064,  ...,  0.0070, -0.0052,  0.0134],\n",
       "          ...,\n",
       "          [ 0.0047, -0.0070, -0.0356,  ...,  0.0138,  0.0080, -0.0107],\n",
       "          [-0.0130,  0.0069, -0.0073,  ..., -0.0036,  0.0130,  0.0139],\n",
       "          [ 0.0018,  0.0038,  0.0126,  ...,  0.0211,  0.0236, -0.0209]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0194,  0.0279,  0.0123,  ...,  0.0006,  0.0071,  0.0026],\n",
       "          [-0.0066,  0.0041, -0.0400,  ..., -0.0060, -0.0115,  0.0073],\n",
       "          [-0.0154,  0.0211, -0.0135,  ...,  0.0094,  0.0044,  0.0265],\n",
       "          ...,\n",
       "          [ 0.0090,  0.0198,  0.0058,  ...,  0.0044,  0.0244, -0.0043],\n",
       "          [ 0.0161, -0.0193, -0.0066,  ...,  0.0055,  0.0027,  0.0373],\n",
       "          [ 0.0002,  0.0239, -0.0222,  ..., -0.0042, -0.0277,  0.0012]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0251,  0.0050,  0.0070,  ..., -0.0209, -0.0223,  0.0250],\n",
       "          [ 0.0230,  0.0196,  0.0263,  ..., -0.0223,  0.0054, -0.0132],\n",
       "          [-0.0264,  0.0160, -0.0078,  ..., -0.0081, -0.0060,  0.0152],\n",
       "          ...,\n",
       "          [-0.0173, -0.0180,  0.0022,  ...,  0.0267,  0.0018,  0.0053],\n",
       "          [ 0.0268,  0.0193,  0.0045,  ...,  0.0107, -0.0049, -0.0177],\n",
       "          [-0.0195, -0.0063, -0.0251,  ...,  0.0061,  0.0084,  0.0118]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0063,  0.0277,  0.0177, -0.0225, -0.0109, -0.0217, -0.0201,  0.0041,\n",
       "           0.0113, -0.0272,  0.0049, -0.0044,  0.0168,  0.0134,  0.0234,  0.0015,\n",
       "          -0.0058, -0.0065,  0.0215,  0.0009, -0.0009, -0.0087, -0.0078, -0.0147,\n",
       "           0.0039,  0.0103, -0.0195,  0.0010,  0.0094,  0.0265, -0.0229,  0.0098,\n",
       "           0.0278, -0.0202,  0.0246, -0.0038, -0.0095,  0.0213,  0.0151,  0.0145,\n",
       "          -0.0230, -0.0117, -0.0016,  0.0054,  0.0126,  0.0275,  0.0267,  0.0083,\n",
       "          -0.0102, -0.0125,  0.0221, -0.0162,  0.0154,  0.0081, -0.0161, -0.0194,\n",
       "           0.0272, -0.0154,  0.0148,  0.0002,  0.0046, -0.0008, -0.0059,  0.0113,\n",
       "          -0.0077,  0.0201, -0.0087, -0.0195,  0.0057,  0.0266, -0.0025,  0.0271,\n",
       "          -0.0279,  0.0181, -0.0159, -0.0212, -0.0208, -0.0240, -0.0063, -0.0109,\n",
       "          -0.0014, -0.0277, -0.0018,  0.0220,  0.0113,  0.0098, -0.0078, -0.0151,\n",
       "           0.0063,  0.0049, -0.0208, -0.0044, -0.0247, -0.0161,  0.0085, -0.0230,\n",
       "          -0.0190, -0.0043,  0.0167,  0.0245,  0.0072, -0.0147, -0.0131, -0.0061,\n",
       "           0.0192,  0.0257, -0.0033, -0.0158, -0.0194, -0.0160,  0.0126,  0.0217,\n",
       "          -0.0118,  0.0214, -0.0230, -0.0165,  0.0093,  0.0048, -0.0061,  0.0103,\n",
       "          -0.0186,  0.0005, -0.0133, -0.0190,  0.0197,  0.0100, -0.0273, -0.0057,\n",
       "          -0.0028, -0.0034,  0.0133,  0.0027,  0.0248, -0.0229, -0.0077,  0.0090,\n",
       "           0.0039, -0.0191, -0.0041, -0.0262, -0.0116, -0.0030,  0.0237, -0.0111,\n",
       "           0.0040,  0.0196, -0.0116, -0.0005,  0.0121, -0.0275, -0.0031, -0.0220,\n",
       "           0.0045, -0.0110, -0.0099,  0.0231,  0.0151, -0.0072, -0.0173,  0.0251,\n",
       "           0.0059,  0.0132, -0.0198,  0.0139, -0.0091,  0.0099, -0.0118, -0.0145,\n",
       "          -0.0270,  0.0259,  0.0269,  0.0272,  0.0175,  0.0073,  0.0261,  0.0242,\n",
       "          -0.0017,  0.0007, -0.0098,  0.0175,  0.0151,  0.0262,  0.0163,  0.0262,\n",
       "           0.0105, -0.0168,  0.0039,  0.0147, -0.0100, -0.0113, -0.0176, -0.0205,\n",
       "           0.0131, -0.0078, -0.0098,  0.0068,  0.0217, -0.0182,  0.0150,  0.0150,\n",
       "           0.0151, -0.0102, -0.0020, -0.0161,  0.0084,  0.0230,  0.0145, -0.0227,\n",
       "          -0.0022, -0.0184,  0.0010,  0.0221,  0.0053,  0.0108, -0.0221,  0.0135,\n",
       "           0.0143, -0.0068,  0.0069, -0.0082,  0.0194,  0.0022,  0.0240,  0.0083,\n",
       "          -0.0038, -0.0121,  0.0005, -0.0032,  0.0003, -0.0080, -0.0110,  0.0140,\n",
       "          -0.0126, -0.0185, -0.0254,  0.0086,  0.0113, -0.0178,  0.0058, -0.0016,\n",
       "          -0.0224,  0.0198,  0.0096,  0.0085, -0.0106,  0.0050, -0.0233,  0.0114,\n",
       "          -0.0269, -0.0081,  0.0104, -0.0202,  0.0264,  0.0089,  0.0053, -0.0009,\n",
       "          -0.0096, -0.0013,  0.0242, -0.0156, -0.0008, -0.0271,  0.0025, -0.0131,\n",
       "          -0.0224, -0.0009, -0.0227, -0.0252, -0.0092,  0.0210, -0.0226, -0.0250,\n",
       "           0.0182,  0.0030, -0.0261,  0.0014, -0.0140,  0.0196,  0.0279, -0.0085,\n",
       "          -0.0253,  0.0061, -0.0025,  0.0179,  0.0258,  0.0192,  0.0055, -0.0012,\n",
       "           0.0227, -0.0245, -0.0085, -0.0089,  0.0208, -0.0279, -0.0057, -0.0192,\n",
       "          -0.0257,  0.0196, -0.0050,  0.0123, -0.0277, -0.0097,  0.0023, -0.0191,\n",
       "          -0.0245,  0.0094, -0.0256, -0.0257,  0.0166, -0.0218, -0.0278,  0.0187,\n",
       "           0.0192,  0.0177, -0.0066, -0.0030,  0.0187, -0.0202,  0.0277,  0.0208,\n",
       "           0.0173, -0.0105,  0.0074, -0.0212,  0.0156,  0.0137, -0.0023,  0.0176,\n",
       "          -0.0207, -0.0156,  0.0184, -0.0080,  0.0022,  0.0170,  0.0157, -0.0165,\n",
       "           0.0257, -0.0244, -0.0053,  0.0041, -0.0127,  0.0180,  0.0112,  0.0250,\n",
       "          -0.0194,  0.0235, -0.0112, -0.0199,  0.0256, -0.0023, -0.0153,  0.0221,\n",
       "           0.0196, -0.0265,  0.0197, -0.0214, -0.0230,  0.0239, -0.0027, -0.0126,\n",
       "           0.0188, -0.0067, -0.0109, -0.0233, -0.0230, -0.0217, -0.0045, -0.0279,\n",
       "           0.0126,  0.0088,  0.0212,  0.0256, -0.0093, -0.0048,  0.0166, -0.0270,\n",
       "           0.0130,  0.0123,  0.0145,  0.0085,  0.0229, -0.0234, -0.0217,  0.0225,\n",
       "          -0.0232, -0.0146,  0.0019, -0.0017, -0.0197,  0.0237, -0.0171, -0.0009,\n",
       "           0.0045, -0.0172,  0.0081,  0.0145,  0.0210, -0.0141,  0.0091, -0.0090,\n",
       "          -0.0242,  0.0082, -0.0043, -0.0061, -0.0152,  0.0167, -0.0255, -0.0149,\n",
       "          -0.0150, -0.0238, -0.0242,  0.0264, -0.0208, -0.0005, -0.0236,  0.0203,\n",
       "           0.0032, -0.0207, -0.0217, -0.0018,  0.0202, -0.0048, -0.0144,  0.0059,\n",
       "           0.0160, -0.0153,  0.0032, -0.0041, -0.0015, -0.0066, -0.0231, -0.0131,\n",
       "          -0.0067, -0.0154,  0.0158, -0.0097,  0.0183, -0.0129, -0.0090, -0.0086,\n",
       "           0.0099,  0.0002, -0.0050, -0.0242,  0.0182,  0.0128,  0.0102, -0.0184,\n",
       "          -0.0217, -0.0187, -0.0182, -0.0267,  0.0088,  0.0188, -0.0174, -0.0242,\n",
       "           0.0132, -0.0267, -0.0135, -0.0057, -0.0250, -0.0117,  0.0139,  0.0181,\n",
       "           0.0048, -0.0050, -0.0219, -0.0022, -0.0121, -0.0057, -0.0219, -0.0080,\n",
       "           0.0210, -0.0114, -0.0192,  0.0125,  0.0023,  0.0098,  0.0246, -0.0011,\n",
       "           0.0112,  0.0108,  0.0222, -0.0113,  0.0115,  0.0056,  0.0045,  0.0169,\n",
       "          -0.0223, -0.0230,  0.0094,  0.0141,  0.0251, -0.0194,  0.0027, -0.0002,\n",
       "           0.0031, -0.0226, -0.0272,  0.0036,  0.0038,  0.0149, -0.0164, -0.0179,\n",
       "          -0.0206,  0.0181,  0.0089,  0.0036,  0.0048,  0.0218, -0.0220, -0.0014,\n",
       "           0.0209, -0.0068, -0.0072,  0.0213,  0.0034,  0.0037, -0.0254,  0.0165,\n",
       "           0.0234,  0.0086,  0.0104, -0.0232,  0.0150, -0.0069,  0.0179, -0.0257,\n",
       "           0.0211,  0.0118,  0.0151,  0.0047,  0.0085,  0.0071,  0.0225, -0.0082,\n",
       "           0.0251,  0.0051,  0.0241, -0.0165, -0.0071, -0.0200, -0.0117,  0.0112,\n",
       "          -0.0227,  0.0108,  0.0250,  0.0112, -0.0241, -0.0276, -0.0021,  0.0092,\n",
       "          -0.0249,  0.0134,  0.0078, -0.0251,  0.0184,  0.0141, -0.0214, -0.0107,\n",
       "          -0.0207, -0.0009,  0.0005, -0.0033,  0.0076, -0.0156,  0.0092,  0.0011,\n",
       "          -0.0158, -0.0018,  0.0175,  0.0175,  0.0162,  0.0191,  0.0048, -0.0206,\n",
       "          -0.0267,  0.0111, -0.0274, -0.0146, -0.0146, -0.0146, -0.0214, -0.0100,\n",
       "           0.0249, -0.0106,  0.0091,  0.0207, -0.0243,  0.0080, -0.0046, -0.0033,\n",
       "           0.0201,  0.0094, -0.0112,  0.0135, -0.0123, -0.0131,  0.0243, -0.0135,\n",
       "          -0.0189, -0.0094, -0.0007,  0.0173,  0.0041,  0.0163,  0.0067,  0.0117,\n",
       "          -0.0040, -0.0007, -0.0165, -0.0065, -0.0140, -0.0109, -0.0219, -0.0033,\n",
       "           0.0045, -0.0067,  0.0099, -0.0216,  0.0135,  0.0158,  0.0153, -0.0098,\n",
       "           0.0043, -0.0127,  0.0141,  0.0245, -0.0153,  0.0219,  0.0139, -0.0157,\n",
       "          -0.0138, -0.0222,  0.0124, -0.0057,  0.0061,  0.0151,  0.0008,  0.0084],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0156,  0.0379,  0.0252,  ..., -0.0099,  0.0221,  0.0036],\n",
       "          [-0.0122,  0.0293,  0.0097,  ...,  0.0392,  0.0155, -0.0314],\n",
       "          [ 0.0307,  0.0358,  0.0121,  ...,  0.0287,  0.0103, -0.0167],\n",
       "          ...,\n",
       "          [-0.0105, -0.0050, -0.0222,  ..., -0.0226, -0.0261,  0.0349],\n",
       "          [-0.0168, -0.0360, -0.0030,  ..., -0.0110, -0.0378,  0.0349],\n",
       "          [-0.0334, -0.0340, -0.0388,  ..., -0.0111, -0.0145,  0.0117]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0155, -0.0175,  0.0244,  0.0057, -0.0306, -0.0097, -0.0104,  0.0266],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0170,  0.0024, -0.0099,  ...,  0.0213, -0.0161,  0.0209],\n",
       "          [ 0.0217, -0.0050, -0.0043,  ..., -0.0298, -0.0127, -0.0002],\n",
       "          [-0.0028, -0.0088,  0.0265,  ..., -0.0244, -0.0460,  0.0026],\n",
       "          ...,\n",
       "          [-0.0011, -0.0137, -0.0171,  ...,  0.0438,  0.0119, -0.0387],\n",
       "          [-0.0118, -0.0123, -0.0044,  ..., -0.0210, -0.0059, -0.0051],\n",
       "          [-0.0302, -0.0306,  0.0016,  ...,  0.0296,  0.0216, -0.0011]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0289, -0.0349, -0.0063,  ...,  0.0053,  0.0072, -0.0117],\n",
       "          [ 0.0248, -0.0077,  0.0191,  ..., -0.0096,  0.0169, -0.0207],\n",
       "          [ 0.0007, -0.0184,  0.0069,  ..., -0.0003,  0.0082, -0.0161],\n",
       "          ...,\n",
       "          [-0.0050,  0.0065, -0.0082,  ..., -0.0003,  0.0204,  0.0060],\n",
       "          [-0.0017, -0.0017, -0.0152,  ..., -0.0025, -0.0074,  0.0179],\n",
       "          [ 0.0177, -0.0170, -0.0177,  ...,  0.0057,  0.0270,  0.0352]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0273, -0.0073, -0.0211,  ..., -0.0089,  0.0030, -0.0143],\n",
       "          [ 0.0153,  0.0165,  0.0126,  ..., -0.0148, -0.0028,  0.0052],\n",
       "          [ 0.0206,  0.0127,  0.0070,  ...,  0.0159, -0.0160,  0.0111],\n",
       "          ...,\n",
       "          [-0.0069,  0.0043, -0.0172,  ..., -0.0172,  0.0124,  0.0126],\n",
       "          [-0.0112, -0.0452,  0.0302,  ..., -0.0265,  0.0032, -0.0248],\n",
       "          [-0.0081, -0.0071,  0.0318,  ...,  0.0135,  0.0004, -0.0218]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0032,  0.0058,  0.0178,  ...,  0.0245,  0.0021,  0.0031],\n",
       "          [ 0.0275,  0.0388,  0.0046,  ..., -0.0166,  0.0090,  0.0411],\n",
       "          [-0.0231,  0.0204, -0.0172,  ..., -0.0075,  0.0091, -0.0160],\n",
       "          ...,\n",
       "          [-0.0115, -0.0188, -0.0029,  ...,  0.0218, -0.0178, -0.0134],\n",
       "          [ 0.0107, -0.0193, -0.0060,  ...,  0.0191, -0.0006,  0.0204],\n",
       "          [-0.0073, -0.0055, -0.0160,  ..., -0.0330, -0.0088,  0.0026]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0124, -0.0006,  0.0236,  ...,  0.0031,  0.0187, -0.0163],\n",
       "          [ 0.0201, -0.0094, -0.0023,  ...,  0.0176,  0.0131,  0.0222],\n",
       "          [-0.0187, -0.0278,  0.0178,  ..., -0.0152,  0.0183, -0.0004],\n",
       "          ...,\n",
       "          [-0.0212, -0.0241, -0.0185,  ..., -0.0226, -0.0237,  0.0158],\n",
       "          [-0.0272, -0.0089,  0.0110,  ...,  0.0081,  0.0246,  0.0014],\n",
       "          [ 0.0171, -0.0094,  0.0075,  ...,  0.0154,  0.0058,  0.0122]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 1.3863e-02,  1.9938e-02, -1.2128e-02,  1.1862e-03, -2.2041e-02,\n",
       "          -2.7787e-02,  3.8038e-03,  2.9151e-04,  2.2325e-02,  1.5241e-02,\n",
       "           1.5836e-03, -6.5760e-03, -2.1723e-03, -6.3384e-03, -7.9373e-03,\n",
       "          -4.6432e-03,  4.4209e-03, -2.2719e-02, -9.4843e-03, -1.9457e-03,\n",
       "           1.2505e-02, -1.0307e-02, -2.6857e-02,  5.0668e-03, -2.4066e-04,\n",
       "           4.4347e-03,  1.8124e-02,  1.1687e-02,  1.0909e-02,  1.4600e-02,\n",
       "           1.0231e-02,  2.3316e-02, -1.1214e-02,  2.3732e-02, -2.7281e-02,\n",
       "           1.6071e-03,  1.0818e-02, -2.6438e-02, -1.9450e-02,  2.0106e-02,\n",
       "          -1.8116e-02, -1.2654e-02,  2.0518e-02,  1.1077e-02, -1.5327e-02,\n",
       "           1.4653e-03,  1.7185e-02,  7.2671e-03,  2.1833e-02, -2.7312e-02,\n",
       "          -6.0075e-03,  1.7814e-02, -3.5653e-03, -2.5886e-03, -3.9648e-03,\n",
       "           7.6877e-03,  6.7248e-03,  2.6443e-02, -1.1936e-02,  2.2606e-02,\n",
       "          -6.2025e-03, -1.4084e-02, -1.2545e-02, -1.6806e-02,  2.4841e-04,\n",
       "          -6.3072e-03,  1.0461e-02,  9.5518e-03, -2.6659e-02,  2.0694e-02,\n",
       "          -1.4461e-02,  2.7360e-04, -6.8937e-03,  4.4503e-03, -4.0243e-03,\n",
       "          -1.4541e-02, -6.4014e-03,  1.7720e-02,  9.7810e-03, -2.0196e-03,\n",
       "           2.7220e-02,  2.4999e-02, -2.7242e-02, -1.9403e-02, -3.3485e-03,\n",
       "          -6.0320e-03, -5.8654e-03,  2.4095e-02,  2.3427e-02,  1.6484e-02,\n",
       "           2.5379e-02, -1.0025e-02,  1.5251e-02, -1.5574e-02, -2.0067e-02,\n",
       "           1.5920e-02,  1.4065e-02,  1.9891e-02,  8.8716e-03, -2.6545e-02,\n",
       "           1.2630e-02,  1.5512e-02, -7.6475e-03,  1.9568e-02, -9.8066e-03,\n",
       "           2.0897e-02, -1.4954e-02, -3.7078e-03,  3.9592e-03,  2.9814e-03,\n",
       "          -2.2875e-02,  2.1761e-02,  1.2564e-02, -6.3551e-03, -8.4104e-04,\n",
       "          -1.6003e-02,  1.4077e-03, -2.5692e-02, -9.2397e-03, -1.5635e-02,\n",
       "          -2.2799e-02,  1.0549e-02, -7.8135e-03,  2.1045e-02, -7.1985e-03,\n",
       "          -1.4538e-02,  1.5419e-02, -1.8124e-02, -7.2455e-03,  2.6564e-02,\n",
       "           1.4584e-02,  6.5716e-03,  2.1945e-02,  2.3813e-02, -8.5379e-03,\n",
       "           7.1693e-03,  1.0273e-02, -2.7337e-02, -1.9309e-02,  7.0278e-03,\n",
       "          -2.2576e-02, -1.6122e-02, -1.4295e-02,  3.7135e-03,  2.3511e-02,\n",
       "           1.2203e-03,  2.4338e-02, -5.0110e-03,  2.0442e-02,  2.1955e-02,\n",
       "          -1.9689e-02,  9.5004e-03,  2.7658e-02, -1.7848e-02, -8.1862e-03,\n",
       "           2.9483e-03,  1.5413e-02, -2.3759e-02,  9.6712e-03, -1.4216e-02,\n",
       "          -2.3923e-02, -2.1448e-02, -9.8647e-03, -6.4711e-03, -1.0849e-03,\n",
       "           9.7519e-03, -2.6227e-02,  2.4961e-02,  1.9870e-02,  2.5694e-02,\n",
       "           2.6577e-02, -2.6085e-02,  4.0139e-03,  1.7342e-03, -1.5501e-02,\n",
       "          -7.5409e-03, -1.1196e-02, -8.6721e-03,  4.1409e-03, -2.4042e-02,\n",
       "           1.0690e-02,  8.0909e-03, -2.1722e-02,  2.6510e-02,  5.7618e-03,\n",
       "           7.5971e-03, -1.2757e-02,  9.6882e-03,  1.5322e-02,  4.5318e-03,\n",
       "          -1.9280e-03,  2.7616e-02,  3.6074e-03,  1.5536e-02,  8.3339e-03,\n",
       "          -8.6290e-03,  5.1631e-03,  1.3575e-02,  5.6046e-04,  4.6239e-03,\n",
       "           2.6149e-02, -6.4629e-03,  7.8312e-03,  2.1788e-03,  2.7608e-02,\n",
       "          -2.5819e-02,  1.2239e-02,  2.1966e-02,  2.4691e-02,  1.7315e-02,\n",
       "           2.0609e-02, -1.0718e-02,  1.9347e-02, -2.1021e-02,  1.9035e-02,\n",
       "          -1.0109e-02,  8.9896e-03, -1.4377e-02, -2.3778e-02,  1.7585e-02,\n",
       "           1.1623e-02, -1.8580e-02,  1.9307e-02, -1.1213e-02,  2.0681e-02,\n",
       "           2.7321e-04,  1.9543e-02,  2.2520e-02, -2.6178e-02, -2.2415e-03,\n",
       "          -1.7097e-02, -1.7427e-02,  8.0696e-03, -1.3773e-02, -1.7173e-02,\n",
       "           7.7228e-03,  7.4252e-03, -1.4699e-02, -1.8463e-02, -2.5774e-02,\n",
       "          -9.5257e-03,  1.5453e-02,  2.6916e-02,  1.8479e-02, -2.4036e-02,\n",
       "          -7.8139e-03,  9.3949e-03,  1.3729e-03, -2.8142e-03, -5.2815e-03,\n",
       "           1.6764e-02,  1.8814e-02, -1.6072e-02,  1.0465e-02,  1.9936e-03,\n",
       "           2.0616e-02,  2.3695e-02, -8.1615e-03,  1.7542e-02, -4.0925e-03,\n",
       "           2.1903e-02,  5.3524e-04,  2.4059e-02, -2.6761e-02,  1.9687e-02,\n",
       "           2.3816e-02,  1.2370e-02,  2.4453e-02,  1.7711e-02,  1.3239e-03,\n",
       "          -1.4028e-03, -4.1889e-03, -1.5592e-02, -2.2202e-02, -2.6508e-02,\n",
       "          -2.3280e-02, -9.4981e-03, -1.6044e-02, -1.2915e-02,  4.7110e-03,\n",
       "          -1.3114e-02, -1.6126e-02,  6.1187e-03,  2.3267e-02, -1.4464e-02,\n",
       "           4.8802e-03,  1.5261e-02,  1.5205e-02,  6.7376e-05, -8.0710e-03,\n",
       "          -7.0354e-03,  1.6939e-02, -2.7664e-02,  9.2310e-03,  1.9898e-02,\n",
       "           1.2079e-02,  2.6491e-02, -1.0659e-02, -2.2586e-02,  1.7675e-02,\n",
       "           2.6145e-02,  2.7396e-03,  2.5478e-02,  1.1822e-02,  2.0441e-02,\n",
       "          -1.7974e-02,  1.4844e-02, -4.7304e-03,  1.6169e-02, -1.4574e-02,\n",
       "           1.9217e-03,  1.1196e-02,  5.4396e-03, -2.2758e-02,  1.7840e-02,\n",
       "           5.3775e-03, -5.2852e-03,  1.4964e-02,  2.0641e-02,  2.5432e-02,\n",
       "           3.6824e-03, -1.6951e-02,  2.4030e-02,  1.8252e-03,  1.2698e-02,\n",
       "           5.9652e-03, -5.0579e-03,  2.3397e-03, -3.9589e-03, -1.1985e-02,\n",
       "           2.5240e-02,  8.2748e-03,  2.3507e-02,  8.5629e-03,  1.2220e-02,\n",
       "          -3.0120e-03,  2.7439e-02, -2.0127e-02, -4.0538e-03, -1.5554e-02,\n",
       "          -8.7727e-03, -2.0358e-02,  1.6054e-02,  1.2580e-02,  3.3629e-03,\n",
       "          -8.5331e-03, -7.5425e-03,  9.3132e-03,  1.4569e-02,  8.6771e-03,\n",
       "          -1.4895e-02, -2.7739e-02,  1.5244e-03,  1.5589e-02,  4.3487e-03,\n",
       "          -9.6086e-03, -1.9453e-02, -1.6725e-02,  1.0954e-02, -4.2237e-03,\n",
       "          -4.0135e-03, -2.4986e-02, -2.7192e-02,  8.5848e-03, -1.3459e-02,\n",
       "          -2.7819e-02, -2.3911e-03, -1.8370e-03, -1.9354e-02,  1.0123e-02,\n",
       "          -7.5657e-03,  1.4924e-02,  2.2232e-02,  2.7080e-02, -1.8891e-02,\n",
       "           1.1251e-02, -1.6907e-02, -1.3446e-02, -8.1014e-03,  5.3036e-03,\n",
       "           2.7728e-02, -1.9689e-02,  1.8745e-02, -9.5104e-03,  8.8732e-03,\n",
       "          -2.1385e-02,  9.3619e-04,  1.0554e-02, -2.5822e-02, -1.7755e-02,\n",
       "          -4.7858e-03,  1.7536e-02, -1.2878e-02, -2.1594e-02, -2.4761e-03,\n",
       "          -8.8032e-03, -2.3145e-02,  2.2612e-03,  1.5536e-02,  1.9675e-02,\n",
       "          -2.5121e-02,  2.3082e-02,  1.3545e-02,  9.5736e-03,  1.8279e-02,\n",
       "          -1.5100e-02,  2.6717e-02,  3.8377e-03,  2.0062e-02,  4.9162e-04,\n",
       "           1.5103e-02, -2.3827e-02, -1.3832e-02, -1.0100e-03,  1.4717e-02,\n",
       "           1.2073e-02,  8.6855e-03,  1.3806e-04, -9.5741e-03, -1.8523e-02,\n",
       "          -7.7748e-03, -2.2794e-02, -1.7206e-02, -1.3076e-02, -9.5414e-03,\n",
       "           1.1401e-02, -1.1441e-02,  1.0504e-02,  2.9502e-03, -2.0526e-02,\n",
       "           1.2421e-02, -2.6030e-02, -2.8855e-03, -5.7605e-03,  1.7340e-03,\n",
       "          -5.9439e-03,  8.7834e-03, -1.9506e-02,  2.4264e-02,  1.8057e-02,\n",
       "           1.7765e-02, -1.4735e-02,  1.9056e-03, -8.4510e-03,  2.7693e-02,\n",
       "           2.4804e-02, -2.3504e-02,  2.0489e-02,  3.7996e-03,  2.7893e-02,\n",
       "           6.9418e-03, -1.9684e-03, -1.0448e-02, -1.7666e-02,  2.7405e-02,\n",
       "          -2.3557e-03,  2.5569e-02, -1.6871e-02, -6.4358e-03,  1.3881e-02,\n",
       "          -1.3828e-03, -2.1022e-02, -1.7964e-02,  1.3952e-02, -2.0282e-02,\n",
       "          -1.2137e-03, -2.7768e-02,  4.6190e-03,  1.8985e-02, -1.7892e-02,\n",
       "          -1.3801e-04, -5.5410e-04, -2.0766e-02, -5.0354e-03,  2.6444e-02,\n",
       "          -9.0452e-04,  1.0630e-02,  1.4624e-02, -1.3152e-02, -2.3010e-02,\n",
       "           2.7382e-02,  1.7819e-02, -4.3513e-05,  2.1513e-02,  7.3463e-04,\n",
       "           8.0195e-03, -2.5104e-02,  9.3725e-03,  1.9072e-02,  1.6152e-02,\n",
       "          -1.0784e-02,  8.1738e-03,  2.3453e-02,  1.2071e-02, -1.6395e-02,\n",
       "           1.6058e-02, -1.0207e-02,  5.8976e-03, -1.1150e-02,  1.5996e-02,\n",
       "           3.3905e-03, -1.2607e-02, -1.2749e-02, -2.5789e-03,  1.1206e-03,\n",
       "           2.1278e-02, -7.7199e-03,  2.0062e-03, -1.5088e-02, -2.7618e-02,\n",
       "           1.3295e-02,  1.3885e-02,  8.4966e-03,  3.3109e-03,  2.9763e-03,\n",
       "          -1.3794e-02, -3.6984e-03, -1.6284e-02, -2.0073e-02,  5.5850e-03,\n",
       "           1.1156e-02, -9.4879e-03, -1.2649e-02,  5.9360e-03, -1.3021e-02,\n",
       "           1.8359e-02,  1.0801e-02, -2.1011e-02,  1.5860e-02,  1.3192e-02,\n",
       "          -1.3027e-02, -2.6117e-02,  1.3085e-02, -1.1684e-03, -6.1873e-03,\n",
       "           2.6157e-02, -2.6547e-02,  2.0336e-02,  2.2065e-02,  2.0283e-02,\n",
       "          -3.6936e-03, -2.5559e-02,  1.4506e-02, -5.3884e-03, -2.0118e-02,\n",
       "           1.7571e-02,  2.3778e-02,  1.4932e-02,  6.7714e-03,  1.3379e-02,\n",
       "          -4.7400e-03,  9.5575e-03, -6.9666e-03, -2.2455e-02,  1.5128e-02,\n",
       "          -1.7742e-02, -2.7870e-02,  6.3678e-03,  1.7994e-02, -6.9744e-03,\n",
       "          -8.9747e-03, -2.6117e-02,  1.3593e-02, -2.3587e-02, -2.1175e-02,\n",
       "           9.6338e-03, -1.2583e-02, -2.2837e-02, -1.9250e-02,  2.3069e-02,\n",
       "          -1.4469e-02,  5.5208e-03,  1.6752e-02, -1.5542e-02, -2.3399e-02,\n",
       "          -2.4287e-02,  8.6493e-03, -2.4295e-03, -2.4850e-02,  1.8014e-02,\n",
       "           1.9364e-02,  1.0394e-02, -9.9217e-03, -2.3664e-02,  2.1041e-02,\n",
       "           2.3179e-02,  1.9665e-02,  8.6458e-03, -6.9038e-03, -4.5576e-03,\n",
       "          -2.0369e-02,  2.8288e-03,  1.2437e-02,  1.5488e-02, -1.6183e-02,\n",
       "           2.7011e-02,  9.5796e-03, -1.5144e-02,  2.1665e-02, -1.1930e-02,\n",
       "          -4.6220e-03, -1.6135e-03,  1.1080e-02, -2.0969e-02,  2.0089e-02,\n",
       "           1.1219e-02, -2.1065e-02,  5.0702e-03, -2.2007e-02,  6.5734e-03,\n",
       "           1.4288e-02,  1.9528e-02, -2.3894e-02, -7.9975e-03,  1.0864e-02,\n",
       "          -1.1104e-02,  5.8228e-03,  2.0393e-02,  9.1024e-03, -5.9409e-03,\n",
       "           2.3961e-02, -2.0855e-02, -8.8576e-03, -2.0104e-02,  1.6620e-02,\n",
       "           7.1137e-03, -1.5630e-02,  2.0223e-02,  1.9742e-03, -2.8332e-03,\n",
       "          -2.1357e-02, -2.1140e-02,  4.6796e-03,  5.3004e-03,  3.8871e-03,\n",
       "          -1.3687e-03,  2.3821e-02, -2.5024e-02,  1.2002e-02,  8.4342e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0006,  0.0066,  0.0191,  ..., -0.0198, -0.0146, -0.0073],\n",
       "          [-0.0193, -0.0056, -0.0061,  ...,  0.0217, -0.0370, -0.0368],\n",
       "          [ 0.0034, -0.0359, -0.0013,  ..., -0.0169,  0.0258,  0.0332],\n",
       "          ...,\n",
       "          [ 0.0025,  0.0099, -0.0043,  ...,  0.0271, -0.0150,  0.0148],\n",
       "          [ 0.0189, -0.0017,  0.0146,  ...,  0.0285, -0.0265, -0.0126],\n",
       "          [ 0.0295, -0.0100, -0.0316,  ..., -0.0116, -0.0342,  0.0091]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0031, -0.0011,  0.0039,  0.0076,  0.0007,  0.0090,  0.0257, -0.0250],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0042, -0.0194,  0.0013,  ..., -0.0029,  0.0016, -0.0118],\n",
       "          [-0.0085, -0.0226, -0.0118,  ..., -0.0044,  0.0153,  0.0006],\n",
       "          [-0.0105, -0.0298, -0.0034,  ..., -0.0059,  0.0013,  0.0085],\n",
       "          ...,\n",
       "          [-0.0293,  0.0167, -0.0280,  ...,  0.0334,  0.0100,  0.0027],\n",
       "          [ 0.0126, -0.0200, -0.0302,  ..., -0.0293,  0.0148,  0.0026],\n",
       "          [-0.0067, -0.0179,  0.0170,  ...,  0.0051,  0.0137, -0.0057]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0040, -0.0022, -0.0008,  ..., -0.0301, -0.0080,  0.0086],\n",
       "          [ 0.0017, -0.0113, -0.0289,  ..., -0.0126,  0.0023,  0.0004],\n",
       "          [-0.0048, -0.0095,  0.0001,  ...,  0.0088,  0.0139,  0.0128],\n",
       "          ...,\n",
       "          [ 0.0028, -0.0034, -0.0266,  ...,  0.0316, -0.0216,  0.0263],\n",
       "          [-0.0310,  0.0125,  0.0296,  ...,  0.0076,  0.0083,  0.0347],\n",
       "          [ 0.0148, -0.0086,  0.0089,  ..., -0.0032,  0.0204, -0.0073]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0002, -0.0066, -0.0136,  ...,  0.0041,  0.0161, -0.0267],\n",
       "          [ 0.0145, -0.0031, -0.0172,  ...,  0.0105,  0.0065, -0.0010],\n",
       "          [-0.0110,  0.0333, -0.0016,  ..., -0.0215, -0.0229, -0.0301],\n",
       "          ...,\n",
       "          [-0.0045,  0.0059,  0.0179,  ...,  0.0312, -0.0002,  0.0091],\n",
       "          [-0.0109, -0.0222, -0.0012,  ...,  0.0116,  0.0055,  0.0022],\n",
       "          [-0.0056,  0.0082,  0.0042,  ...,  0.0194,  0.0093,  0.0084]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0157,  0.0051,  0.0185,  ..., -0.0071, -0.0113,  0.0304],\n",
       "          [-0.0075, -0.0059,  0.0062,  ...,  0.0044,  0.0072, -0.0068],\n",
       "          [ 0.0157, -0.0010, -0.0177,  ...,  0.0082,  0.0093, -0.0097],\n",
       "          ...,\n",
       "          [-0.0177,  0.0063, -0.0272,  ...,  0.0132,  0.0020, -0.0081],\n",
       "          [ 0.0049, -0.0221,  0.0085,  ...,  0.0001,  0.0054,  0.0130],\n",
       "          [-0.0316, -0.0179,  0.0119,  ..., -0.0010,  0.0093,  0.0180]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0165,  0.0216,  0.0223,  ...,  0.0252, -0.0016,  0.0010],\n",
       "          [ 0.0259,  0.0209,  0.0190,  ..., -0.0229,  0.0276, -0.0056],\n",
       "          [ 0.0108, -0.0175, -0.0072,  ..., -0.0237, -0.0228,  0.0061],\n",
       "          ...,\n",
       "          [-0.0040, -0.0153,  0.0031,  ..., -0.0194, -0.0096,  0.0263],\n",
       "          [-0.0015, -0.0268,  0.0146,  ..., -0.0257,  0.0145,  0.0178],\n",
       "          [ 0.0244, -0.0059,  0.0102,  ...,  0.0275, -0.0180, -0.0265]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 2.7082e-02,  2.2995e-02,  1.6102e-02,  7.2833e-03,  1.3443e-02,\n",
       "           8.6976e-03,  2.3699e-03,  1.2188e-02, -6.6786e-03, -2.5321e-02,\n",
       "           4.7956e-03, -1.5607e-02, -2.6612e-02, -2.3907e-02,  4.0977e-03,\n",
       "          -1.9229e-02, -1.9860e-02, -1.5596e-02,  1.7473e-02,  1.5380e-02,\n",
       "          -2.6863e-02, -1.2908e-02,  6.4209e-03,  1.8241e-02,  2.5648e-03,\n",
       "          -2.3814e-02, -6.8900e-03, -9.8788e-03, -6.2228e-03,  1.2743e-02,\n",
       "           8.8514e-03, -2.2976e-03,  4.7632e-03, -1.4398e-02, -2.3514e-02,\n",
       "           6.7427e-03,  1.1589e-02, -2.2501e-02,  1.5434e-02,  9.4065e-03,\n",
       "           5.3846e-03, -3.8934e-03, -1.1173e-02,  8.3249e-04,  1.4436e-02,\n",
       "          -1.1898e-02,  2.2238e-02,  1.5302e-02,  1.1935e-02,  2.0398e-02,\n",
       "           2.2074e-02, -1.4737e-02, -4.5905e-03,  1.4859e-02,  9.7412e-03,\n",
       "          -1.4385e-02,  2.9275e-03,  1.8215e-02,  4.7266e-03, -2.0370e-02,\n",
       "           1.1627e-02,  2.2874e-02,  1.7548e-02, -1.5192e-02, -1.9044e-02,\n",
       "          -1.4657e-02,  2.1917e-02, -2.5880e-03, -7.8784e-03,  9.7901e-03,\n",
       "           2.1444e-02,  2.7639e-02, -1.6093e-02, -1.1566e-02, -1.4976e-02,\n",
       "           2.0902e-02, -2.1038e-02, -1.4696e-02,  1.7896e-02, -2.2728e-02,\n",
       "          -7.1547e-03, -1.3170e-02,  1.3511e-02, -9.4884e-03, -1.4888e-02,\n",
       "           2.2893e-02,  5.7672e-03, -9.0683e-03,  5.7784e-03,  3.1547e-03,\n",
       "           2.1197e-03,  2.3079e-02,  2.1356e-03,  5.8441e-03,  1.8483e-02,\n",
       "           2.4289e-02,  5.2746e-03,  4.1310e-03,  3.7438e-03, -1.9640e-03,\n",
       "           1.8080e-02,  1.2933e-02, -6.3637e-03, -1.3445e-03,  7.6711e-03,\n",
       "          -1.1074e-02,  1.1455e-05, -1.1912e-02,  1.0115e-02, -1.2484e-02,\n",
       "           2.5200e-02, -8.3643e-03,  6.9274e-03,  1.9292e-03, -6.3407e-03,\n",
       "          -1.0263e-02, -1.2815e-02,  2.3434e-02, -1.6429e-02,  1.0338e-02,\n",
       "          -1.3988e-02, -1.0493e-02,  1.7440e-02, -2.1322e-02,  1.1597e-02,\n",
       "          -1.1634e-02, -1.3070e-02, -1.7144e-02,  1.9768e-02, -1.3717e-02,\n",
       "           2.3992e-02,  1.0283e-02, -1.0108e-02,  2.0591e-02, -1.6544e-02,\n",
       "          -1.4607e-02,  2.1989e-02,  2.2909e-02,  1.6115e-02,  2.3325e-02,\n",
       "           2.5088e-03, -6.4227e-03, -1.9778e-02, -1.6170e-02, -2.4352e-02,\n",
       "           2.6555e-03, -8.9303e-03, -2.1733e-03, -2.6568e-02,  1.9777e-02,\n",
       "          -1.6620e-02,  9.0330e-03, -2.2589e-02,  7.8698e-03,  9.1576e-03,\n",
       "           2.5141e-02,  2.6388e-02, -2.8073e-03,  1.8928e-02, -2.7225e-02,\n",
       "           1.3957e-02,  7.1751e-03, -8.0710e-03, -1.2748e-02,  1.9622e-03,\n",
       "           2.3040e-02, -2.8306e-03,  1.7134e-02,  2.7753e-02, -1.1354e-02,\n",
       "          -3.0581e-03,  1.5554e-02, -6.1973e-03,  2.1272e-02,  1.7171e-03,\n",
       "           1.3095e-02,  7.7851e-03, -1.4708e-02,  2.5335e-02, -1.5798e-02,\n",
       "           7.0144e-04,  7.3270e-03,  6.6690e-03,  1.0784e-03,  2.6643e-02,\n",
       "          -2.5933e-02, -2.1229e-02, -2.0570e-02, -7.7594e-03, -8.1944e-03,\n",
       "           6.2262e-03, -5.0990e-03, -8.0662e-03, -1.4630e-02,  6.3367e-03,\n",
       "           2.7367e-02, -1.9136e-03,  2.0528e-02,  7.0287e-03,  1.3803e-02,\n",
       "           9.8130e-03,  1.7976e-02,  1.5909e-02, -9.9562e-03,  1.3459e-02,\n",
       "           8.7503e-03, -1.9853e-02, -2.1091e-02,  2.4652e-02,  2.2438e-02,\n",
       "           9.5957e-03,  2.4364e-02,  4.3786e-03, -5.7593e-04, -2.2824e-02,\n",
       "           2.7238e-02,  1.8774e-02,  1.8666e-02,  6.3404e-03, -2.3976e-02,\n",
       "          -4.3125e-03, -2.6692e-02, -8.3536e-03, -8.1128e-03, -1.5560e-02,\n",
       "           3.5181e-03,  1.4788e-02, -2.3306e-02,  1.1385e-03,  1.9061e-02,\n",
       "          -4.2004e-03,  2.1537e-02,  1.4357e-02,  2.4538e-02,  2.2909e-02,\n",
       "           7.8202e-03,  2.6559e-02,  9.8808e-03,  2.2856e-02,  1.0329e-02,\n",
       "           2.0629e-02, -2.3013e-02, -7.7914e-03,  1.3202e-02, -1.6089e-02,\n",
       "           1.3054e-02, -2.3440e-02,  1.3895e-02, -4.7517e-03,  8.2672e-03,\n",
       "           8.1952e-03,  2.4608e-02,  2.5459e-02, -1.3931e-03,  3.6235e-03,\n",
       "          -1.4644e-02,  2.7813e-02,  2.3911e-02,  1.8647e-02, -8.0084e-03,\n",
       "           4.5045e-03,  1.8146e-03,  1.6731e-02,  1.8085e-02,  2.5496e-02,\n",
       "          -2.1632e-02,  1.9505e-02, -1.6886e-02,  1.0975e-03,  2.4049e-02,\n",
       "          -1.2104e-02, -9.1056e-03,  1.8145e-02,  1.2223e-02,  8.5839e-03,\n",
       "          -1.8819e-03,  2.6330e-02, -2.3054e-03,  2.0954e-02, -9.7121e-03,\n",
       "           3.9428e-03, -9.0644e-03,  1.6863e-02,  1.5422e-02, -2.1123e-03,\n",
       "          -7.6876e-03,  2.3040e-02, -1.1472e-02, -8.6578e-04, -1.0208e-02,\n",
       "           2.2789e-02,  8.4685e-03,  2.5491e-02, -2.7051e-03,  1.6099e-02,\n",
       "           2.5792e-02,  2.5270e-02, -2.3405e-02,  2.1715e-02,  2.5613e-03,\n",
       "           6.5039e-03, -2.5791e-02,  1.4323e-02, -1.0978e-04, -6.9025e-03,\n",
       "           2.1233e-02, -2.4372e-02, -2.3455e-02, -1.0546e-02, -1.1799e-02,\n",
       "          -1.7556e-02, -7.1265e-04,  2.2319e-02,  2.6907e-02,  1.4527e-02,\n",
       "           1.1980e-02, -2.5078e-02, -2.3175e-02, -1.1607e-02,  2.6542e-02,\n",
       "           1.1188e-03, -8.8468e-03, -1.4348e-02, -7.3834e-03, -5.7565e-04,\n",
       "          -4.2586e-03, -3.1369e-03,  2.5874e-03,  1.2437e-03, -1.3822e-02,\n",
       "          -2.5609e-02,  9.7916e-03, -2.7806e-02,  4.7684e-03, -2.2840e-02,\n",
       "           2.0999e-02,  8.5882e-03,  1.0416e-03, -1.9881e-02, -4.4333e-03,\n",
       "          -2.6134e-02,  2.1096e-02,  1.4452e-02, -8.6682e-03, -1.4827e-02,\n",
       "          -5.2542e-03,  1.7313e-02, -1.4943e-02, -5.4860e-03, -1.6939e-02,\n",
       "          -1.4464e-02,  1.7816e-02,  1.7011e-02,  5.4394e-03, -7.0144e-03,\n",
       "          -8.9402e-03, -1.1082e-03,  1.5018e-02, -1.5524e-02, -5.0665e-03,\n",
       "           2.1796e-02, -2.5538e-02, -1.3043e-02,  2.1455e-02,  1.0045e-02,\n",
       "           2.4633e-02, -2.5746e-02, -8.5188e-03, -1.2196e-03, -1.4565e-02,\n",
       "          -1.1195e-02, -1.6607e-02,  2.4834e-02,  2.0332e-02,  1.4559e-02,\n",
       "           9.6642e-03,  1.8624e-02,  6.1917e-03, -2.3439e-02, -2.7997e-03,\n",
       "          -2.9108e-03, -2.1489e-02, -1.0985e-02, -6.1659e-03, -2.4169e-02,\n",
       "          -1.1588e-02,  6.2445e-03,  2.6500e-02, -2.6596e-02,  3.0624e-03,\n",
       "          -1.4051e-02, -2.7622e-02,  2.1938e-02,  9.2894e-03, -6.2441e-03,\n",
       "           1.0395e-02, -1.5736e-02, -2.1650e-02, -7.2639e-03, -9.7994e-04,\n",
       "           1.5745e-02, -1.6418e-03, -1.1647e-02, -7.7245e-03, -2.6007e-02,\n",
       "          -1.0664e-02,  1.2788e-02, -2.1372e-02,  2.5655e-02, -1.1081e-02,\n",
       "           2.4213e-02,  1.0489e-02,  2.7036e-02, -4.5974e-03,  4.8171e-03,\n",
       "           2.7253e-02, -1.3509e-02,  7.4961e-04,  8.6708e-03, -2.3516e-02,\n",
       "          -7.0156e-03, -2.5852e-02,  2.2868e-02,  4.5532e-05, -2.7270e-02,\n",
       "          -3.7489e-03,  1.1753e-02, -4.6167e-03, -1.3129e-02, -1.8119e-02,\n",
       "           1.1077e-02,  1.7579e-03,  9.1734e-03,  8.6030e-03,  2.7934e-02,\n",
       "          -3.2327e-03, -5.6303e-03,  1.9809e-02,  5.7893e-03,  8.5223e-04,\n",
       "           1.4272e-02,  1.7122e-02,  1.3928e-02, -2.3992e-02, -1.1125e-02,\n",
       "          -2.7734e-02, -8.8133e-04,  2.1582e-02, -2.1756e-03,  4.1143e-03,\n",
       "           1.7115e-02,  1.4749e-02, -9.1132e-03,  2.4065e-02, -6.8846e-03,\n",
       "          -2.0567e-02, -1.9751e-02, -1.1766e-02,  6.5108e-03, -2.1276e-02,\n",
       "          -1.1448e-02,  2.0854e-02,  1.9747e-02, -8.2137e-03,  1.7249e-02,\n",
       "           8.9138e-03, -2.3803e-02,  2.3856e-02,  1.4361e-02, -8.5272e-03,\n",
       "           1.1178e-02,  1.7347e-02, -2.4552e-03, -2.1715e-02,  6.1978e-03,\n",
       "           1.7530e-03, -2.5665e-02,  1.6558e-02,  1.7321e-03, -1.2637e-02,\n",
       "           2.8167e-03, -1.6887e-05,  2.5231e-02,  2.6056e-02, -1.4580e-02,\n",
       "          -6.4524e-03, -5.3319e-03, -4.3724e-03, -4.6034e-03,  1.8113e-03,\n",
       "           2.0322e-03, -1.3015e-03,  2.7428e-02,  1.8427e-02,  2.0293e-02,\n",
       "           2.7692e-02,  9.6015e-03,  1.9327e-02, -2.5708e-02,  2.3810e-02,\n",
       "           8.9898e-03,  1.7588e-02, -9.0195e-03,  8.0626e-03, -8.0198e-03,\n",
       "          -2.3566e-02, -2.9861e-03,  1.6008e-02,  1.3987e-02, -2.2702e-02,\n",
       "           4.3116e-04, -1.3678e-02, -5.3872e-05,  1.1883e-02, -2.1314e-02,\n",
       "          -1.1980e-02, -1.1111e-02, -9.4793e-03, -3.3889e-03, -9.7519e-03,\n",
       "          -5.0888e-03, -1.0137e-02, -2.6636e-02, -1.7828e-02, -5.1582e-03,\n",
       "           1.3593e-02,  1.1779e-02, -1.7565e-02, -2.4426e-02,  1.8726e-02,\n",
       "          -1.2616e-02, -1.3026e-02, -1.1990e-03,  2.4579e-03,  1.4447e-02,\n",
       "          -1.4806e-02, -1.4561e-02,  1.6768e-02,  2.6384e-02,  2.3382e-02,\n",
       "          -8.5118e-03, -3.4968e-03, -1.0442e-02, -7.7407e-04,  4.8189e-04,\n",
       "          -1.1033e-02, -1.9165e-02, -2.7944e-02,  1.0901e-02,  2.5539e-02,\n",
       "          -1.6905e-02, -1.8395e-02, -1.3134e-03, -2.5743e-02,  2.7273e-02,\n",
       "          -2.0321e-02, -8.1187e-03,  7.2082e-04, -2.7903e-02, -1.7587e-02,\n",
       "           1.7549e-02,  2.5481e-02, -1.0771e-02, -2.9329e-03, -2.5732e-02,\n",
       "          -2.0535e-02, -9.7841e-03,  8.5522e-03, -3.2941e-03, -2.2259e-02,\n",
       "          -1.6758e-02,  1.6436e-02, -2.6419e-02, -4.4533e-04,  4.0512e-03,\n",
       "          -2.6360e-02,  1.0970e-02,  8.3813e-04, -1.6612e-02,  2.2212e-02,\n",
       "           1.9960e-02,  4.1176e-03,  1.1767e-02, -1.4221e-02, -7.4543e-03,\n",
       "           2.4377e-04,  1.9790e-03,  2.7577e-02,  1.4428e-02, -1.4695e-04,\n",
       "           2.4504e-02, -1.8849e-02, -2.7783e-02, -1.8283e-02,  1.8420e-02,\n",
       "           1.0849e-02, -9.7709e-03,  8.3411e-03,  1.5401e-02,  1.6335e-02,\n",
       "           8.0492e-03,  2.4851e-02, -2.2927e-02,  1.0661e-03, -4.7912e-03,\n",
       "          -7.4509e-03, -2.4551e-03,  1.3631e-02, -2.7469e-02, -5.7520e-03,\n",
       "          -2.5549e-02, -1.5625e-02, -6.8355e-03,  1.1585e-02,  7.8035e-03,\n",
       "           6.0430e-03,  1.4819e-02,  1.0716e-02, -2.4511e-02, -2.5035e-02,\n",
       "          -1.0815e-02,  1.5356e-02,  2.3797e-02, -1.4371e-02, -3.5438e-03,\n",
       "           1.6784e-02, -2.0097e-02,  1.3696e-03, -2.1873e-02,  1.7740e-02,\n",
       "           9.0098e-03, -5.1123e-03, -1.8439e-02, -2.5285e-02,  1.8609e-02,\n",
       "           2.6888e-02,  9.5683e-03,  1.5286e-02,  1.2872e-02, -9.1999e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0124, -0.0281,  0.0062,  ..., -0.0213,  0.0302, -0.0080],\n",
       "          [ 0.0172, -0.0347, -0.0125,  ...,  0.0129, -0.0066, -0.0223],\n",
       "          [ 0.0216, -0.0330, -0.0196,  ..., -0.0350, -0.0284, -0.0374],\n",
       "          ...,\n",
       "          [ 0.0165, -0.0386, -0.0027,  ..., -0.0161, -0.0134, -0.0060],\n",
       "          [-0.0088, -0.0234,  0.0222,  ..., -0.0261, -0.0307, -0.0384],\n",
       "          [-0.0227, -0.0100, -0.0265,  ..., -0.0110,  0.0319, -0.0211]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0293, -0.0354,  0.0309, -0.0389, -0.0075, -0.0360, -0.0008,  0.0373],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.5491e-02,  8.9023e-03,  1.2157e-02,  ..., -3.0452e-03,\n",
       "           -3.6898e-02,  4.3973e-02],\n",
       "          [-1.8586e-02, -2.2992e-02, -6.6507e-03,  ..., -1.3494e-02,\n",
       "            2.4872e-02, -9.9152e-03],\n",
       "          [-3.7472e-05, -4.7191e-03,  3.2815e-03,  ..., -5.7095e-03,\n",
       "            3.2480e-02, -7.7193e-03],\n",
       "          ...,\n",
       "          [ 2.1913e-02,  4.7504e-02, -2.6430e-02,  ...,  2.9309e-02,\n",
       "           -1.0987e-03, -1.1572e-02],\n",
       "          [ 2.1209e-03, -1.0377e-02,  3.0447e-02,  ..., -4.5570e-03,\n",
       "           -1.5828e-02,  1.5598e-02],\n",
       "          [ 3.2332e-02,  1.0356e-02,  1.0661e-02,  ...,  7.2779e-03,\n",
       "           -2.4231e-02, -1.4003e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0116,  0.0023,  0.0321,  ...,  0.0035,  0.0135, -0.0055],\n",
       "          [ 0.0131,  0.0175, -0.0003,  ...,  0.0062,  0.0069,  0.0102],\n",
       "          [ 0.0226,  0.0178, -0.0020,  ...,  0.0109, -0.0110,  0.0014],\n",
       "          ...,\n",
       "          [-0.0064, -0.0266,  0.0056,  ..., -0.0071, -0.0134, -0.0028],\n",
       "          [-0.0020, -0.0067, -0.0088,  ...,  0.0185, -0.0108,  0.0090],\n",
       "          [-0.0034, -0.0021, -0.0123,  ...,  0.0018,  0.0165,  0.0138]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 1.1309e-02, -2.9498e-02, -1.9848e-02,  ...,  2.5566e-02,\n",
       "            2.5016e-02,  1.0154e-02],\n",
       "          [-1.1163e-02,  2.5394e-02, -2.5796e-02,  ..., -4.7030e-04,\n",
       "            2.4209e-02,  1.6053e-02],\n",
       "          [-6.3772e-03,  2.4811e-02, -1.6827e-02,  ..., -5.1743e-03,\n",
       "           -4.5741e-03,  3.7986e-02],\n",
       "          ...,\n",
       "          [ 4.4918e-03, -1.5936e-02, -4.1874e-03,  ..., -2.4732e-02,\n",
       "            3.1066e-03, -1.6735e-02],\n",
       "          [ 4.5712e-03, -2.9071e-03, -1.0272e-02,  ...,  2.6754e-03,\n",
       "            3.4559e-02, -5.8562e-03],\n",
       "          [-3.9556e-05, -4.6051e-02, -6.3497e-04,  ..., -2.4252e-02,\n",
       "           -4.5666e-03, -1.3614e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0065,  0.0013,  0.0025,  ..., -0.0032,  0.0033,  0.0223],\n",
       "          [ 0.0318,  0.0122, -0.0043,  ..., -0.0097, -0.0030, -0.0079],\n",
       "          [-0.0189, -0.0149, -0.0169,  ..., -0.0149,  0.0040, -0.0210],\n",
       "          ...,\n",
       "          [ 0.0209, -0.0196, -0.0048,  ..., -0.0099, -0.0233, -0.0054],\n",
       "          [ 0.0182, -0.0073, -0.0254,  ...,  0.0122,  0.0119, -0.0170],\n",
       "          [-0.0224,  0.0079,  0.0133,  ...,  0.0019,  0.0236, -0.0106]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0069,  0.0162,  0.0124,  ..., -0.0047, -0.0235, -0.0199],\n",
       "          [-0.0131,  0.0100, -0.0016,  ..., -0.0180,  0.0048,  0.0116],\n",
       "          [-0.0036, -0.0221, -0.0096,  ...,  0.0075,  0.0018,  0.0231],\n",
       "          ...,\n",
       "          [-0.0040, -0.0134, -0.0220,  ..., -0.0154, -0.0275,  0.0184],\n",
       "          [-0.0003, -0.0071, -0.0253,  ...,  0.0171, -0.0022,  0.0210],\n",
       "          [ 0.0075, -0.0077,  0.0120,  ..., -0.0167, -0.0215,  0.0231]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-9.8652e-03, -3.0790e-03,  3.4194e-03, -8.4877e-04, -2.8507e-03,\n",
       "           6.2482e-03, -1.5798e-02, -2.7322e-02,  5.9347e-03, -1.1198e-02,\n",
       "           2.0669e-02, -1.0649e-03,  9.2283e-03,  1.1349e-02, -1.0388e-02,\n",
       "          -2.0993e-02,  4.6550e-04,  1.3476e-02, -1.9584e-02,  2.2204e-02,\n",
       "          -7.5918e-03,  1.6404e-02, -1.9439e-02,  1.7933e-02, -5.9677e-03,\n",
       "          -1.8255e-02,  6.9045e-04, -2.6402e-02, -1.3893e-02,  2.4401e-02,\n",
       "           2.2454e-02,  3.4063e-03, -1.5686e-02, -1.1870e-02,  1.0847e-02,\n",
       "           1.7799e-02, -6.8042e-03,  9.4941e-03,  2.4478e-02, -8.7193e-03,\n",
       "          -3.1245e-03,  2.5232e-04,  1.2121e-02, -5.1471e-03,  1.4673e-02,\n",
       "          -1.0862e-03, -2.7477e-02, -4.0063e-03, -1.9582e-02,  2.5850e-02,\n",
       "          -2.5860e-02, -8.6668e-03,  1.5017e-02, -1.6228e-02, -2.4686e-02,\n",
       "           4.5249e-03,  1.5141e-02, -8.7971e-03,  1.4269e-02,  6.1750e-04,\n",
       "           6.5232e-03,  3.8638e-05, -2.2931e-02,  1.4142e-02,  9.8371e-03,\n",
       "          -1.9658e-02,  6.3317e-04,  2.4995e-02,  2.0135e-02, -2.3360e-03,\n",
       "           1.1767e-02, -2.3547e-02,  2.0536e-03,  1.9933e-02, -2.2034e-02,\n",
       "          -1.4277e-02,  7.0160e-03,  5.1953e-03, -1.3235e-03,  2.4525e-02,\n",
       "           2.3603e-02, -2.1857e-02, -4.6818e-03,  1.4764e-02, -1.8641e-02,\n",
       "          -1.2906e-02,  1.5574e-02, -7.2932e-03,  5.8139e-03, -1.0194e-02,\n",
       "           1.2014e-02,  4.7668e-03,  1.2461e-03, -9.8025e-03,  2.2969e-02,\n",
       "           2.6762e-02,  1.2641e-02, -1.6146e-02, -2.4958e-03,  1.3947e-02,\n",
       "          -1.6457e-02, -2.5318e-02,  1.7793e-02, -1.2337e-02, -2.4957e-02,\n",
       "           2.2364e-02,  1.1661e-02,  2.0811e-02, -3.9703e-03, -1.2230e-02,\n",
       "           1.2743e-02, -2.1563e-02,  2.2433e-02, -2.2775e-03, -2.1133e-02,\n",
       "           1.3072e-02,  9.3844e-03,  2.2108e-02,  1.3512e-02,  1.6356e-02,\n",
       "           1.1646e-02, -1.6976e-02,  2.6366e-02,  6.1364e-03, -2.7761e-02,\n",
       "           1.4118e-02, -1.9547e-02, -2.7036e-02,  1.1964e-03, -1.1387e-03,\n",
       "           1.6467e-02,  2.2337e-02, -2.6055e-02,  1.2143e-02, -1.1420e-02,\n",
       "           1.0678e-02,  2.1404e-02,  1.2077e-02,  2.6932e-02, -8.6872e-03,\n",
       "           3.9561e-03, -1.0479e-02, -9.0972e-03, -1.3591e-03,  2.4930e-02,\n",
       "           2.1860e-02, -8.4887e-04,  5.8820e-03, -1.9786e-02, -1.6339e-02,\n",
       "          -1.7045e-03,  2.6008e-02,  9.2747e-03,  3.6555e-03, -2.6376e-02,\n",
       "           1.5328e-02, -1.6651e-02,  1.4921e-02,  2.3669e-03, -2.6053e-02,\n",
       "          -6.8633e-03, -1.7612e-02, -1.4031e-02,  2.7920e-02,  2.0005e-03,\n",
       "          -6.5460e-03,  2.6573e-02,  9.7752e-03, -1.1774e-02, -7.6466e-05,\n",
       "           2.1122e-02,  1.1760e-02,  4.3840e-03,  4.7099e-03, -2.6099e-02,\n",
       "          -2.4514e-03,  1.0195e-02,  2.1300e-02, -4.7517e-04, -1.4844e-02,\n",
       "           1.6504e-02,  2.6389e-02,  4.8687e-03,  1.5880e-02, -1.9133e-02,\n",
       "           2.3766e-02,  2.3611e-03, -1.6831e-03, -1.6690e-02, -1.8196e-02,\n",
       "          -1.6132e-02,  2.1674e-02,  2.0278e-02,  2.6472e-02,  7.3808e-03,\n",
       "           1.9280e-02,  8.4144e-03, -2.6577e-02,  1.7150e-02,  1.1156e-02,\n",
       "           5.9440e-03,  1.5705e-02,  2.3801e-02, -2.1615e-02,  2.2780e-02,\n",
       "          -1.7955e-02, -2.4109e-02,  1.1906e-02, -8.7217e-03,  7.8641e-03,\n",
       "          -2.2931e-02,  2.6574e-02,  1.2260e-02, -1.4891e-02, -2.4900e-02,\n",
       "           1.8378e-02,  2.0508e-02, -6.8288e-03, -2.5704e-02,  1.5118e-03,\n",
       "           1.0838e-02, -1.7798e-02,  2.2628e-02,  2.0423e-03, -2.0844e-02,\n",
       "           1.8562e-02, -7.3904e-06, -2.2701e-02, -3.9215e-03, -9.8248e-03,\n",
       "          -1.9257e-02,  2.2084e-02, -2.4700e-02,  2.6016e-02, -6.4369e-03,\n",
       "          -2.7286e-02, -2.2835e-02, -2.4215e-02,  8.9337e-03, -6.7767e-03,\n",
       "          -3.2115e-03, -1.1389e-02,  6.4938e-03, -2.7759e-02,  2.4699e-02,\n",
       "          -7.7301e-03,  4.4137e-03, -1.6618e-02, -2.3486e-02,  1.6817e-02,\n",
       "           2.1993e-02, -2.0502e-03, -2.4654e-03, -9.2309e-03, -6.0352e-03,\n",
       "          -2.4077e-02, -1.0813e-02, -1.1061e-02, -2.0079e-02,  6.5912e-03,\n",
       "          -1.8046e-02,  4.6183e-03,  1.8481e-02, -1.2283e-02, -9.2598e-03,\n",
       "           1.6314e-02,  1.6872e-02,  2.5595e-02, -1.4059e-02, -3.1537e-04,\n",
       "          -1.6064e-02,  1.6923e-02,  1.4662e-02, -2.7285e-02,  1.6961e-02,\n",
       "           1.8839e-02,  1.1018e-02,  1.3740e-02, -2.6185e-02, -6.2602e-03,\n",
       "           4.1653e-03,  1.1669e-02,  8.8529e-03, -2.6924e-02, -9.2144e-03,\n",
       "           7.3283e-03, -2.7093e-02, -2.2212e-02, -2.7763e-02, -1.3883e-03,\n",
       "           1.5198e-02, -6.1980e-03,  1.6445e-02, -1.9159e-02, -2.4692e-02,\n",
       "           7.9729e-03, -1.4678e-02, -2.3678e-03,  2.5128e-02, -3.4691e-04,\n",
       "          -2.0072e-02, -1.5543e-02,  2.3817e-02, -2.3395e-02,  1.3565e-02,\n",
       "          -6.6614e-03,  3.0218e-04,  2.7980e-03, -2.7122e-02,  1.8917e-02,\n",
       "           1.9785e-02,  1.3666e-02, -2.6543e-02,  2.1589e-02,  2.0966e-02,\n",
       "          -2.7280e-02,  2.3533e-02,  8.1441e-05,  1.6475e-02,  2.6991e-02,\n",
       "          -7.0821e-03,  7.8805e-03, -2.1117e-02, -3.9100e-04, -1.2358e-03,\n",
       "           2.1741e-03, -3.3812e-03,  2.7510e-02,  2.4347e-03,  1.3636e-02,\n",
       "           1.6469e-02, -1.7492e-02,  1.7077e-02,  2.7376e-02,  1.6074e-02,\n",
       "          -8.4017e-03, -2.1077e-02,  1.3228e-02,  1.5432e-03,  1.8244e-02,\n",
       "          -1.8690e-03,  9.9098e-03,  6.0771e-03, -1.3809e-02, -9.7726e-03,\n",
       "          -5.3604e-03,  1.3174e-02, -1.9463e-02, -9.9496e-03, -1.7106e-02,\n",
       "           1.1074e-02,  5.9396e-04, -2.7730e-02,  5.2919e-03,  7.8359e-03,\n",
       "          -2.7517e-02, -6.1716e-03, -1.7540e-02, -2.3683e-02,  2.6654e-04,\n",
       "           2.2728e-03,  1.7104e-02,  2.3103e-02, -1.3284e-02,  2.3796e-02,\n",
       "           1.6721e-02, -1.9298e-02, -2.9162e-03, -2.5988e-02, -1.2757e-02,\n",
       "          -1.1869e-02, -1.1972e-02,  3.8533e-03,  5.7743e-03,  1.3849e-02,\n",
       "          -1.4090e-02,  1.0174e-03,  8.0678e-03, -2.3861e-02, -4.4381e-03,\n",
       "          -1.9449e-02,  2.6381e-02,  1.9793e-02,  3.3226e-03,  5.0358e-03,\n",
       "          -1.6662e-02,  2.0509e-02,  2.2582e-02,  2.7110e-02, -2.6608e-02,\n",
       "           4.8077e-03, -4.0540e-04, -2.5697e-02,  8.1508e-03, -1.8611e-02,\n",
       "           2.2974e-03, -1.8037e-02,  1.8964e-02,  1.6525e-02, -9.5962e-03,\n",
       "           6.6172e-03, -8.3437e-03, -1.4151e-02,  9.7858e-03, -1.8654e-02,\n",
       "           2.2987e-02,  7.0776e-03,  2.7730e-02,  2.3145e-02, -2.0859e-02,\n",
       "          -2.7269e-02,  9.1747e-03,  6.0074e-03,  1.5404e-02, -6.4372e-03,\n",
       "          -1.3800e-02,  4.6590e-03, -5.6554e-03,  1.6603e-02,  2.2732e-02,\n",
       "           1.9179e-02,  3.2815e-03,  9.8813e-03, -5.9212e-03,  2.9766e-03,\n",
       "          -9.9503e-03,  4.7134e-03, -6.7540e-04,  1.0799e-03,  1.6033e-02,\n",
       "          -1.3902e-02,  2.2513e-02,  4.0821e-03, -9.5184e-03,  1.8253e-03,\n",
       "           1.6949e-02, -7.2904e-03,  1.9413e-02, -7.9697e-03,  1.0412e-02,\n",
       "           7.4105e-03, -4.3116e-04,  1.5223e-02,  2.0438e-02, -1.8982e-02,\n",
       "           1.6141e-02,  2.5527e-02,  1.3006e-02,  2.6486e-02,  1.7621e-03,\n",
       "          -5.4281e-04,  2.3243e-02, -2.1509e-02, -2.5481e-02, -8.6367e-03,\n",
       "          -1.2882e-02, -6.4729e-03,  2.6156e-02,  2.3904e-02,  6.7545e-03,\n",
       "           8.6094e-03,  9.4465e-03, -1.5617e-02, -4.9724e-03, -7.1141e-03,\n",
       "          -1.3276e-02, -2.7025e-02,  1.5778e-02, -2.1025e-02,  2.6267e-02,\n",
       "           3.4373e-03, -3.3697e-03,  2.4026e-02,  2.1326e-02, -1.2693e-03,\n",
       "          -1.0344e-02, -2.5323e-02,  5.4580e-03,  1.2061e-02, -2.4576e-02,\n",
       "           6.2058e-03,  2.2670e-02,  2.0647e-02,  1.8433e-02, -3.2917e-03,\n",
       "           2.5133e-03,  1.0964e-02,  1.0368e-02,  1.3182e-02, -1.0559e-02,\n",
       "          -2.1803e-02,  2.6871e-02,  7.6127e-03,  1.9096e-02,  1.0996e-02,\n",
       "          -1.3633e-02,  4.9601e-04,  1.9354e-02, -7.3731e-03,  3.0373e-03,\n",
       "           1.5842e-02, -1.8273e-02,  2.3598e-02, -8.7366e-03,  2.2718e-02,\n",
       "           4.9761e-03, -8.8465e-03, -1.8102e-02,  1.8640e-02, -1.0959e-02,\n",
       "          -2.2829e-02,  1.1868e-02, -1.8944e-02,  7.2391e-03,  1.2328e-02,\n",
       "           1.3134e-02,  2.6332e-02,  1.3441e-03,  1.8517e-02,  2.0895e-02,\n",
       "           1.7797e-02,  2.4805e-02,  6.1265e-03, -1.4229e-02,  3.8717e-03,\n",
       "          -4.9794e-03,  2.5605e-02,  1.4559e-02, -1.2392e-02, -2.3353e-02,\n",
       "          -6.0633e-03, -3.2696e-03,  1.1299e-02, -5.9478e-04, -2.3142e-02,\n",
       "           4.7758e-03,  2.6183e-02, -1.7050e-02,  4.1821e-03, -1.6404e-02,\n",
       "          -1.5871e-02,  9.9632e-03, -2.0452e-02,  2.3404e-03, -2.4101e-02,\n",
       "           2.0811e-02,  1.2865e-02,  1.6459e-02, -2.7191e-02,  8.2358e-03,\n",
       "           2.4977e-02,  6.6579e-03, -4.6514e-03, -2.4454e-02, -2.4170e-02,\n",
       "           2.4799e-02, -2.6698e-02, -1.0587e-03,  1.7589e-02, -2.5942e-02,\n",
       "           9.3293e-03,  1.1475e-03, -8.7885e-03,  1.4355e-02,  2.4842e-02,\n",
       "           1.3826e-02, -2.7072e-04,  1.4858e-02,  1.7248e-02,  1.6459e-03,\n",
       "          -2.0617e-02,  1.3355e-02, -8.8473e-03, -1.1119e-02,  1.6986e-02,\n",
       "           1.2036e-02,  1.4809e-02, -4.8797e-03, -1.0694e-02, -4.0519e-03,\n",
       "          -7.0735e-03,  4.1640e-03,  2.1722e-02,  2.2732e-02, -1.7072e-02,\n",
       "          -1.6747e-03, -7.9653e-03, -7.2665e-03, -2.4507e-02, -2.4009e-02,\n",
       "          -1.0112e-02, -5.4160e-03,  1.9073e-03, -1.8071e-02, -4.9403e-03,\n",
       "           1.3064e-02,  3.0299e-03, -1.0878e-02,  1.1673e-02,  1.1217e-02,\n",
       "          -7.3347e-03, -1.2642e-02,  1.1991e-02,  2.6551e-02,  1.4640e-02,\n",
       "           2.0221e-02,  4.6673e-03,  1.4104e-03, -2.6272e-02,  2.1144e-02,\n",
       "          -4.8519e-03,  1.6476e-02,  9.4176e-03,  9.4923e-03,  2.4384e-02,\n",
       "           2.4022e-02,  2.7311e-02,  2.2858e-02, -1.2211e-02,  2.5019e-02,\n",
       "           8.8230e-03,  8.3500e-03, -1.4067e-02, -1.2215e-02, -2.7461e-02,\n",
       "          -7.3726e-03, -1.5211e-02,  2.0739e-02, -1.9906e-02, -1.3441e-02,\n",
       "           6.8106e-03,  1.5340e-02,  1.3514e-02,  2.0761e-02,  2.4356e-02,\n",
       "          -6.6269e-03, -2.1753e-02,  2.7668e-02,  1.5813e-02,  1.3724e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0364,  0.0316, -0.0208,  ...,  0.0200, -0.0327, -0.0096],\n",
       "          [ 0.0185,  0.0335, -0.0108,  ..., -0.0044, -0.0158, -0.0376],\n",
       "          [ 0.0247, -0.0283,  0.0392,  ..., -0.0091,  0.0243, -0.0276],\n",
       "          ...,\n",
       "          [ 0.0135,  0.0075, -0.0349,  ..., -0.0079,  0.0255,  0.0034],\n",
       "          [ 0.0351,  0.0331,  0.0212,  ...,  0.0078, -0.0217,  0.0393],\n",
       "          [ 0.0152,  0.0014, -0.0208,  ...,  0.0267,  0.0004,  0.0238]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-4.9470e-03,  1.9532e-02, -6.0739e-03, -3.5473e-02, -4.2550e-03,\n",
       "          -4.1794e-03,  3.3876e-02, -3.0841e-05], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0092,  0.0114,  0.0011,  ..., -0.0215,  0.0015,  0.0138],\n",
       "          [-0.0129,  0.0068, -0.0145,  ..., -0.0214, -0.0053, -0.0018],\n",
       "          [ 0.0013,  0.0011, -0.0156,  ...,  0.0256, -0.0018,  0.0093],\n",
       "          ...,\n",
       "          [-0.0021, -0.0101, -0.0146,  ...,  0.0212, -0.0080,  0.0101],\n",
       "          [-0.0022, -0.0279,  0.0132,  ...,  0.0064, -0.0137, -0.0142],\n",
       "          [-0.0395,  0.0113, -0.0114,  ...,  0.0040, -0.0036, -0.0079]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0037, -0.0146,  0.0177,  ..., -0.0194, -0.0028,  0.0015],\n",
       "          [-0.0047,  0.0136,  0.0192,  ..., -0.0254,  0.0042, -0.0162],\n",
       "          [ 0.0079,  0.0187, -0.0100,  ...,  0.0049, -0.0110, -0.0008],\n",
       "          ...,\n",
       "          [ 0.0282, -0.0357, -0.0164,  ...,  0.0046, -0.0251,  0.0014],\n",
       "          [-0.0089,  0.0221,  0.0285,  ..., -0.0091,  0.0060,  0.0045],\n",
       "          [ 0.0009, -0.0155,  0.0031,  ..., -0.0339,  0.0241,  0.0099]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0323, -0.0190, -0.0111,  ...,  0.0246, -0.0128,  0.0099],\n",
       "          [ 0.0076, -0.0058,  0.0052,  ..., -0.0007, -0.0062,  0.0180],\n",
       "          [-0.0019, -0.0126,  0.0003,  ..., -0.0014,  0.0224, -0.0173],\n",
       "          ...,\n",
       "          [ 0.0024, -0.0225, -0.0282,  ...,  0.0163,  0.0293,  0.0148],\n",
       "          [-0.0006,  0.0163, -0.0045,  ..., -0.0002,  0.0007,  0.0387],\n",
       "          [-0.0129, -0.0184,  0.0035,  ..., -0.0044,  0.0076, -0.0043]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0011, -0.0042, -0.0072,  ..., -0.0151, -0.0081, -0.0341],\n",
       "          [ 0.0236,  0.0043,  0.0107,  ...,  0.0214,  0.0247, -0.0058],\n",
       "          [ 0.0002,  0.0367, -0.0428,  ..., -0.0056,  0.0119,  0.0135],\n",
       "          ...,\n",
       "          [-0.0082,  0.0236,  0.0025,  ...,  0.0264, -0.0034,  0.0086],\n",
       "          [ 0.0188,  0.0073, -0.0236,  ..., -0.0172, -0.0099,  0.0034],\n",
       "          [ 0.0227,  0.0025,  0.0148,  ...,  0.0093,  0.0233,  0.0063]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0013, -0.0156,  0.0050,  ..., -0.0092,  0.0127,  0.0206],\n",
       "          [ 0.0109, -0.0078, -0.0268,  ...,  0.0049,  0.0218, -0.0133],\n",
       "          [ 0.0083,  0.0272, -0.0083,  ...,  0.0229, -0.0115,  0.0010],\n",
       "          ...,\n",
       "          [-0.0170,  0.0158,  0.0202,  ..., -0.0239,  0.0083, -0.0248],\n",
       "          [-0.0043,  0.0181,  0.0053,  ..., -0.0012, -0.0254,  0.0260],\n",
       "          [ 0.0263, -0.0027, -0.0180,  ...,  0.0125, -0.0253,  0.0035]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.9101e-02, -1.8435e-02, -2.5793e-02,  3.1849e-03,  2.8303e-03,\n",
       "          -1.0358e-02,  2.1826e-02, -2.2909e-02, -2.6983e-02, -1.8973e-02,\n",
       "          -1.7272e-02, -2.0096e-02,  1.8362e-02,  2.4719e-02,  1.9543e-02,\n",
       "           1.8194e-02, -2.5437e-02,  1.8773e-02, -1.9047e-02,  2.6595e-02,\n",
       "          -4.6464e-03, -5.2225e-03,  2.6365e-02, -2.2337e-02, -5.6618e-03,\n",
       "           2.7662e-02, -2.5885e-02, -1.5804e-02,  4.1930e-03,  4.4541e-03,\n",
       "           2.2634e-02, -2.5311e-02,  2.6500e-02,  1.5405e-02, -6.4147e-03,\n",
       "           2.4287e-02, -4.5574e-03,  2.6801e-02,  5.0013e-05,  1.6009e-02,\n",
       "          -2.0665e-02, -1.5055e-02, -2.2650e-02,  1.8909e-02, -9.1069e-03,\n",
       "          -2.2495e-02,  8.8146e-03,  2.0122e-02,  1.7069e-02, -1.2993e-02,\n",
       "           4.2230e-03,  7.2739e-03, -1.7027e-02,  1.9651e-02, -7.0351e-03,\n",
       "           9.7662e-03,  2.0132e-02, -1.7125e-02,  1.4316e-02, -1.8011e-02,\n",
       "           1.2170e-02, -1.0387e-02,  2.0022e-02, -2.3687e-02,  5.4227e-03,\n",
       "           6.1572e-04, -1.2847e-02, -2.0460e-02, -2.1568e-02, -2.7208e-02,\n",
       "          -1.3113e-02,  2.0492e-02, -6.9172e-03,  2.4106e-02,  2.3318e-02,\n",
       "          -2.1647e-02, -6.0964e-03, -8.0585e-03,  2.5901e-02, -5.9091e-03,\n",
       "           1.5818e-02, -6.1896e-03, -1.3992e-04, -2.1543e-02, -1.0585e-02,\n",
       "          -1.7542e-02, -1.0124e-02,  4.7748e-03, -4.5115e-03, -2.3479e-02,\n",
       "           1.8470e-02, -1.9152e-02,  1.3349e-02,  2.4810e-02, -1.7903e-02,\n",
       "          -2.5233e-02,  1.4493e-02,  2.1333e-03,  3.6978e-03, -1.7564e-02,\n",
       "          -1.2967e-02, -1.1912e-03,  2.6197e-02,  6.5271e-03, -3.4555e-03,\n",
       "           2.4038e-03, -1.2596e-02, -1.2979e-02, -2.6205e-02,  6.1326e-03,\n",
       "           5.4873e-03, -2.0859e-02,  9.0533e-03,  6.1114e-04,  8.2604e-03,\n",
       "           2.7344e-02, -4.0881e-03, -2.7029e-02,  1.3559e-02,  2.3614e-02,\n",
       "           1.6491e-02, -1.9003e-02, -9.9267e-03,  4.1332e-03,  2.0981e-02,\n",
       "          -2.6710e-02, -1.4555e-03,  5.1331e-03, -6.8609e-03, -1.9594e-02,\n",
       "           1.6811e-02, -2.0950e-03, -2.1410e-02,  2.3900e-02,  2.1973e-02,\n",
       "          -2.0629e-02,  2.4138e-02,  1.3374e-02, -3.0743e-03,  1.1512e-02,\n",
       "          -2.4537e-02, -1.7114e-02,  1.9317e-02, -6.3921e-03,  1.9121e-02,\n",
       "           5.1125e-03,  4.1443e-03,  1.3123e-02,  5.4997e-03,  1.7066e-02,\n",
       "          -2.3030e-02, -3.2436e-03, -5.9552e-03,  1.0881e-02,  1.2059e-02,\n",
       "           2.0648e-02, -2.4501e-02,  7.8042e-03, -9.3569e-04, -1.9093e-02,\n",
       "           1.7949e-02,  1.0784e-03, -2.1892e-02,  1.9896e-02,  5.1024e-03,\n",
       "           1.0088e-02, -5.4857e-03, -2.2041e-02, -1.6359e-02,  9.4453e-03,\n",
       "           2.1924e-02,  8.9519e-03,  2.3883e-02, -1.0536e-02,  1.1903e-02,\n",
       "           2.5068e-03, -1.4384e-02,  2.6401e-02, -1.7989e-02,  1.4444e-02,\n",
       "          -2.7813e-02, -9.6127e-04, -9.0686e-03, -7.9256e-04,  9.1021e-03,\n",
       "          -1.5541e-02,  2.7695e-02, -1.9326e-02, -2.2631e-02,  9.4597e-03,\n",
       "           2.4944e-02,  3.4132e-03,  2.0358e-02, -4.0575e-03,  8.9835e-03,\n",
       "           1.4599e-02, -2.5848e-02, -1.1889e-02,  2.2620e-02,  1.6197e-02,\n",
       "          -2.7854e-02, -7.8705e-03,  7.9150e-03, -1.3914e-02,  7.6626e-04,\n",
       "          -1.2168e-02, -1.1053e-02, -2.6658e-02, -2.3249e-02, -1.7787e-02,\n",
       "          -3.6033e-03, -2.1268e-02,  8.2728e-03, -1.7166e-02, -1.9578e-02,\n",
       "          -4.9278e-03, -7.5161e-03, -1.3019e-02, -1.2618e-02,  3.9407e-03,\n",
       "           6.3568e-03,  9.1517e-03,  3.3808e-03, -6.4473e-03,  2.2233e-02,\n",
       "          -5.3144e-03, -6.1702e-03,  2.7350e-02,  4.7756e-03, -2.6112e-02,\n",
       "          -7.5348e-03, -2.2239e-02, -2.1496e-02, -5.3532e-03, -3.1049e-03,\n",
       "          -1.4895e-02,  2.2663e-02, -1.9284e-02,  1.7251e-02, -1.8371e-02,\n",
       "          -2.0245e-03,  1.7089e-02, -2.1431e-02, -5.5362e-03,  1.1844e-02,\n",
       "          -1.0256e-03,  2.7680e-02, -1.0658e-02, -3.0202e-03, -7.0979e-03,\n",
       "           7.8127e-03,  1.6894e-02, -2.0797e-02,  3.5767e-03,  7.3709e-03,\n",
       "           2.6099e-02, -1.0377e-02,  2.2559e-02,  1.8111e-02, -1.8348e-02,\n",
       "           5.6599e-03, -1.3956e-02,  1.0059e-03,  7.8866e-03, -1.0498e-02,\n",
       "           1.2159e-02,  2.6431e-02,  2.4274e-02,  6.8476e-03, -2.6959e-02,\n",
       "           9.7577e-04,  2.4542e-03, -5.0605e-03,  2.2765e-02, -1.4023e-02,\n",
       "           6.2821e-03,  1.8721e-02,  2.4838e-02,  2.4669e-02, -1.7540e-03,\n",
       "          -1.1978e-02, -1.8725e-02,  6.2259e-03, -1.7722e-02,  1.1064e-02,\n",
       "           2.9306e-03, -2.5331e-02,  2.3969e-02, -9.3614e-03, -8.5659e-03,\n",
       "           1.9615e-02, -1.3111e-02,  4.0356e-03, -1.7175e-02,  2.5211e-02,\n",
       "          -2.6174e-02, -1.0337e-02,  2.4832e-02,  9.8528e-03,  1.3012e-02,\n",
       "          -1.5856e-02,  2.5613e-02,  3.7534e-03,  2.3541e-02,  1.0835e-02,\n",
       "          -1.7272e-02, -1.3047e-02,  1.5711e-03, -3.9714e-03,  6.5177e-03,\n",
       "          -1.7217e-03,  1.6185e-02, -1.7941e-02, -1.6065e-02,  2.2570e-02,\n",
       "          -2.6823e-02, -1.6495e-02, -1.3518e-02,  6.6293e-05,  1.2095e-02,\n",
       "           1.3120e-02, -9.7137e-03,  2.4848e-03,  1.4365e-02,  1.1468e-02,\n",
       "          -1.1446e-02,  2.6236e-02, -1.9228e-02,  9.0977e-03,  2.1453e-02,\n",
       "           1.2403e-02, -1.2549e-02, -2.2166e-02,  1.4854e-02,  2.7757e-02,\n",
       "          -4.5457e-03,  1.4903e-02, -1.7137e-02, -6.2525e-03,  2.7511e-03,\n",
       "           2.4151e-02,  1.6507e-02,  2.6934e-02, -1.8559e-02, -1.9143e-02,\n",
       "           2.7314e-02, -1.8671e-02,  1.0199e-02,  1.6909e-02, -2.7830e-02,\n",
       "           4.7088e-03,  1.9871e-02, -3.3257e-03,  1.9909e-02,  2.2274e-02,\n",
       "           2.7697e-02, -8.7288e-05,  9.8662e-03, -1.7045e-02, -1.2584e-02,\n",
       "          -1.7768e-02,  1.1607e-02,  2.4616e-02,  1.2663e-02,  2.6470e-02,\n",
       "           2.6152e-02, -4.6842e-03, -3.1124e-03,  1.9300e-02, -2.4832e-02,\n",
       "           2.5916e-02,  6.7214e-03,  1.4227e-02, -5.3002e-04,  2.8877e-03,\n",
       "          -1.3274e-02,  2.0976e-02,  2.1916e-02, -6.0621e-03, -2.6862e-02,\n",
       "          -1.5041e-02,  1.0258e-02,  2.7597e-02, -5.0086e-03,  1.6958e-02,\n",
       "           3.1742e-04,  1.4140e-02, -2.9546e-04, -5.6974e-03,  2.6090e-02,\n",
       "          -9.2647e-03, -1.5983e-02,  1.3355e-02,  4.5114e-03, -5.9513e-04,\n",
       "           8.3943e-03,  4.3164e-03,  5.0273e-03,  2.3535e-02,  7.8980e-03,\n",
       "           1.8654e-02,  2.5458e-02, -1.8501e-02, -2.3829e-02,  6.8770e-03,\n",
       "           8.9627e-04, -1.4712e-02, -9.0220e-03,  5.6897e-03,  3.4314e-03,\n",
       "          -2.2886e-02,  1.6870e-02, -2.1195e-02,  1.6203e-02,  1.1685e-02,\n",
       "          -5.4085e-03, -2.2700e-02, -1.8238e-02,  2.4880e-03, -2.0495e-02,\n",
       "           3.9064e-03, -1.8993e-02, -5.0117e-03,  1.3855e-02, -6.7186e-03,\n",
       "           9.3445e-03, -1.9096e-02, -6.4286e-03, -5.0887e-03,  2.5483e-02,\n",
       "          -8.5849e-03,  1.4575e-02, -1.0169e-02, -2.5067e-02,  1.3127e-02,\n",
       "           1.3191e-03,  2.2468e-02, -5.2269e-03, -1.0462e-02, -1.6983e-02,\n",
       "           3.8465e-03,  1.1436e-02, -3.0665e-03, -8.1151e-03,  1.5614e-02,\n",
       "          -1.0725e-02, -1.9105e-02,  2.2440e-02,  7.2450e-03,  1.0688e-02,\n",
       "           1.5987e-02,  5.9848e-03,  9.2214e-03, -7.9356e-03, -4.1847e-03,\n",
       "          -1.6066e-02, -6.8654e-03, -1.8982e-02, -2.5633e-02, -1.9813e-02,\n",
       "           2.6419e-02, -3.4455e-03,  2.6769e-02, -1.7329e-02,  2.4217e-02,\n",
       "           8.6015e-03, -2.4555e-02,  2.4430e-02, -5.0181e-03, -1.9582e-02,\n",
       "          -2.2762e-03,  2.0481e-02,  1.1496e-02, -1.5294e-02, -1.0419e-02,\n",
       "          -4.9534e-03, -2.2246e-02,  9.9787e-03,  1.9436e-02,  3.9172e-03,\n",
       "          -8.5115e-03,  2.0756e-02, -5.6918e-03,  9.5780e-03,  4.4083e-03,\n",
       "          -2.3017e-02, -1.8368e-02,  1.9551e-02, -1.1911e-02,  8.0439e-03,\n",
       "           4.9706e-03,  2.3422e-02, -2.5887e-02, -8.6525e-03, -2.6749e-02,\n",
       "          -6.6537e-03,  2.7033e-02,  7.8486e-03,  2.4047e-02, -1.2867e-02,\n",
       "          -1.1438e-02,  1.2232e-02, -2.7118e-03,  2.4295e-02,  1.4960e-02,\n",
       "          -4.2872e-03,  2.3032e-02, -8.8014e-03, -2.5056e-04, -5.3126e-03,\n",
       "           3.1004e-03, -2.0394e-02,  3.9406e-03,  1.3860e-02, -2.6906e-02,\n",
       "           1.5308e-02,  1.0121e-02, -1.3683e-02,  2.2154e-02,  2.3220e-02,\n",
       "          -3.4475e-03,  1.0433e-02,  2.1427e-02,  3.7519e-03, -5.3808e-03,\n",
       "           1.4565e-02,  9.6609e-04,  1.7245e-02,  1.1758e-02,  1.2918e-02,\n",
       "           1.4454e-02, -1.5648e-02,  2.1108e-02,  1.9536e-02, -2.4801e-02,\n",
       "           2.6280e-03,  1.1975e-02, -8.7247e-03, -1.0577e-02, -1.7226e-02,\n",
       "          -5.4804e-03,  2.6862e-02, -2.4586e-02, -2.0078e-02, -2.6892e-02,\n",
       "           4.3968e-03, -1.3200e-02, -1.6450e-02,  1.1050e-02,  1.1929e-02,\n",
       "           8.7982e-04, -2.5066e-02, -2.2654e-02,  1.8452e-02,  2.3560e-02,\n",
       "           1.7035e-02, -4.1375e-03, -1.7120e-02, -1.3179e-02, -1.5074e-02,\n",
       "          -9.6680e-04,  2.2873e-02,  1.7895e-02,  8.8093e-03, -2.5763e-02,\n",
       "           1.6142e-02, -3.8670e-03,  1.4205e-02, -2.1311e-02,  7.0599e-03,\n",
       "          -8.4561e-03,  3.0587e-03, -2.2598e-02, -4.5859e-03, -1.1543e-02,\n",
       "          -1.3469e-02, -2.6400e-02,  2.5051e-03, -1.2199e-02,  2.7326e-02,\n",
       "          -4.7912e-03,  5.6731e-03, -1.8975e-02, -2.6354e-02, -3.7570e-03,\n",
       "          -1.9420e-02, -5.1590e-03,  1.5630e-02, -1.5159e-02,  4.5048e-03,\n",
       "           2.0760e-02,  1.1133e-03,  1.3368e-02, -7.2953e-03, -1.3953e-02,\n",
       "          -5.0300e-03, -7.8109e-03,  2.3064e-02, -1.7623e-02, -1.4790e-02,\n",
       "           1.8798e-02, -9.0530e-03, -1.2864e-02, -5.5227e-03,  6.8730e-03,\n",
       "           1.4772e-02,  2.1249e-02, -2.1318e-02,  1.3452e-04,  1.6146e-02,\n",
       "           1.3190e-02, -1.4539e-02, -2.5536e-02, -4.6781e-05, -5.9721e-03,\n",
       "          -2.6481e-02, -2.4970e-02, -1.1313e-03,  1.2146e-02,  4.9831e-03,\n",
       "           1.3183e-02,  1.3307e-02, -3.3702e-03,  7.2919e-03,  8.6487e-03,\n",
       "           1.1219e-02, -1.4005e-02, -1.7855e-02,  2.0166e-02,  1.1450e-02,\n",
       "           1.2295e-02, -2.4557e-02,  3.8690e-03, -3.7128e-04, -1.3179e-03,\n",
       "           2.2277e-04,  1.9987e-02, -1.3579e-02,  9.0942e-03, -1.8050e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0355, -0.0090, -0.0025,  ..., -0.0033,  0.0382, -0.0121],\n",
       "          [-0.0279, -0.0185, -0.0383,  ...,  0.0283,  0.0388, -0.0297],\n",
       "          [ 0.0154, -0.0072,  0.0062,  ..., -0.0057, -0.0271, -0.0075],\n",
       "          ...,\n",
       "          [-0.0195,  0.0355, -0.0380,  ..., -0.0219, -0.0017, -0.0101],\n",
       "          [ 0.0199,  0.0101,  0.0068,  ..., -0.0351, -0.0169, -0.0341],\n",
       "          [-0.0211, -0.0027,  0.0065,  ...,  0.0263, -0.0299,  0.0041]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0017, -0.0231, -0.0071, -0.0020, -0.0321,  0.0253,  0.0260, -0.0280],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0010,  0.0049,  0.0115,  ...,  0.0257,  0.0076, -0.0058],\n",
       "          [ 0.0043,  0.0016, -0.0101,  ...,  0.0127, -0.0189,  0.0028],\n",
       "          [-0.0300,  0.0119, -0.0171,  ...,  0.0053, -0.0350,  0.0028],\n",
       "          ...,\n",
       "          [-0.0165,  0.0002,  0.0035,  ...,  0.0051,  0.0044, -0.0139],\n",
       "          [-0.0088, -0.0074,  0.0058,  ...,  0.0135,  0.0064, -0.0265],\n",
       "          [-0.0111, -0.0080,  0.0133,  ...,  0.0115,  0.0015, -0.0042]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.6967e-02,  1.1673e-02,  9.2703e-04,  ...,  8.7177e-03,\n",
       "           -1.4566e-03,  5.9243e-03],\n",
       "          [ 1.3965e-02,  4.7697e-03, -1.3118e-02,  ..., -1.8362e-02,\n",
       "           -3.2903e-03,  9.8933e-03],\n",
       "          [ 7.9470e-03,  6.0677e-03, -5.6664e-03,  ...,  1.2856e-02,\n",
       "            2.3493e-02,  2.3888e-02],\n",
       "          ...,\n",
       "          [ 1.8058e-02,  1.7795e-03, -1.1805e-02,  ..., -6.2092e-03,\n",
       "            1.1113e-03, -1.6566e-03],\n",
       "          [ 1.9299e-04,  2.0068e-03,  9.4510e-03,  ...,  5.7048e-03,\n",
       "           -1.6593e-03,  1.0245e-02],\n",
       "          [-4.8046e-02,  3.9885e-03,  1.3388e-02,  ..., -9.5989e-04,\n",
       "           -8.8150e-03,  9.3732e-05]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0045, -0.0045, -0.0084,  ...,  0.0033,  0.0141, -0.0118],\n",
       "          [-0.0170,  0.0080,  0.0149,  ...,  0.0208, -0.0007,  0.0169],\n",
       "          [ 0.0143, -0.0050,  0.0018,  ..., -0.0058,  0.0004,  0.0037],\n",
       "          ...,\n",
       "          [ 0.0058,  0.0211,  0.0152,  ..., -0.0347, -0.0060,  0.0006],\n",
       "          [-0.0103, -0.0056, -0.0021,  ...,  0.0073,  0.0108, -0.0137],\n",
       "          [ 0.0534,  0.0072, -0.0045,  ..., -0.0053,  0.0146, -0.0209]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0438,  0.0002,  0.0073,  ...,  0.0063,  0.0176,  0.0196],\n",
       "          [-0.0072, -0.0090,  0.0220,  ...,  0.0047,  0.0094,  0.0169],\n",
       "          [ 0.0167, -0.0302, -0.0112,  ...,  0.0039, -0.0059, -0.0177],\n",
       "          ...,\n",
       "          [ 0.0304,  0.0030, -0.0230,  ..., -0.0110,  0.0190, -0.0049],\n",
       "          [ 0.0051, -0.0015, -0.0043,  ..., -0.0240,  0.0047,  0.0029],\n",
       "          [-0.0110, -0.0079,  0.0005,  ..., -0.0043, -0.0085, -0.0039]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0264, -0.0039,  0.0254,  ...,  0.0013, -0.0085, -0.0080],\n",
       "          [-0.0111, -0.0265, -0.0075,  ..., -0.0184,  0.0193, -0.0169],\n",
       "          [ 0.0037,  0.0157,  0.0055,  ..., -0.0084, -0.0239, -0.0046],\n",
       "          ...,\n",
       "          [-0.0110,  0.0239, -0.0141,  ..., -0.0166, -0.0054,  0.0261],\n",
       "          [-0.0126, -0.0200, -0.0246,  ..., -0.0110, -0.0229,  0.0149],\n",
       "          [-0.0097,  0.0266,  0.0012,  ...,  0.0192,  0.0003,  0.0003]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-2.2531e-02, -1.9114e-02,  1.7914e-02, -2.7112e-03,  1.2337e-04,\n",
       "          -2.1371e-03,  1.5089e-02,  2.7049e-02,  1.3017e-02, -2.0571e-02,\n",
       "           2.2717e-03,  2.6970e-04,  1.1965e-02,  2.1508e-02, -1.0695e-02,\n",
       "           3.9075e-03,  1.3728e-02,  4.7026e-03, -6.9661e-03,  7.6663e-03,\n",
       "           1.9670e-02, -3.0877e-03,  1.1261e-02,  7.1540e-03,  2.6640e-02,\n",
       "           1.2514e-02,  6.3525e-03, -5.0606e-03, -1.3505e-02, -1.0306e-02,\n",
       "          -2.5308e-02, -2.4265e-02,  1.9006e-02,  3.5052e-03, -2.3366e-02,\n",
       "           2.5096e-02,  1.7448e-02,  1.9234e-02,  8.5644e-03,  8.2230e-03,\n",
       "           1.5840e-02,  1.0661e-02, -1.4221e-02,  9.7526e-03, -2.6851e-02,\n",
       "          -3.4171e-03, -2.2298e-02, -2.0905e-02,  2.6507e-02,  2.6149e-02,\n",
       "           2.0671e-02,  1.7341e-02, -1.5700e-02, -2.3367e-02,  2.2911e-02,\n",
       "           3.0695e-03, -1.3379e-02, -2.0148e-02,  1.1556e-02, -2.2156e-02,\n",
       "           9.2868e-03, -1.6754e-02, -1.8764e-03,  6.3515e-03,  1.8691e-02,\n",
       "           2.2964e-02,  4.1593e-03, -1.1733e-02, -7.4678e-03, -1.4521e-02,\n",
       "          -2.2786e-02, -2.2494e-02, -5.2243e-03,  7.2726e-03,  1.8656e-02,\n",
       "          -1.4480e-02, -2.0730e-02,  3.6590e-03,  1.3292e-02, -2.3299e-02,\n",
       "           1.2868e-02,  2.5524e-02, -1.4296e-02, -2.7313e-02, -1.6177e-02,\n",
       "           6.3028e-04, -1.2397e-03,  6.1399e-03, -1.7277e-02, -2.4735e-02,\n",
       "           2.4060e-03, -1.6413e-02, -6.0625e-03, -2.7752e-02, -2.6394e-02,\n",
       "          -1.3966e-02, -8.9408e-04,  2.7242e-03, -2.6446e-02, -2.5459e-02,\n",
       "          -1.9054e-02,  1.7486e-02, -1.5218e-02, -1.3914e-02, -1.1651e-02,\n",
       "           1.6973e-02,  1.3982e-02,  1.2082e-02, -2.3640e-02,  2.0048e-03,\n",
       "           1.9473e-02,  4.3359e-03, -1.5661e-02,  2.6856e-02,  8.3800e-03,\n",
       "          -8.7306e-03,  1.4619e-02,  5.7058e-03, -1.3567e-02,  1.5496e-02,\n",
       "           1.2068e-02,  1.6697e-02, -1.0161e-02,  1.7884e-03, -2.2612e-02,\n",
       "          -1.2808e-02,  2.1246e-02, -1.3819e-03, -4.2244e-03,  2.7830e-02,\n",
       "           2.5517e-02, -1.7193e-02, -2.0063e-02,  3.9272e-03, -5.8184e-04,\n",
       "          -1.5037e-02, -8.4024e-03, -1.9515e-02, -1.1686e-02, -1.6862e-03,\n",
       "           1.9475e-02,  2.8830e-03,  1.5756e-02, -5.9804e-03,  5.8905e-03,\n",
       "          -2.4674e-02,  2.3258e-02, -2.7518e-02,  1.3966e-02, -1.7358e-03,\n",
       "          -6.5030e-03,  1.5144e-02, -2.6121e-02,  2.7511e-02,  2.0856e-02,\n",
       "          -4.0761e-03,  2.0789e-02, -2.1842e-02,  2.4506e-02, -3.3399e-03,\n",
       "           1.9459e-02, -2.2434e-02,  1.2606e-02,  2.2964e-02, -2.1624e-02,\n",
       "          -1.7531e-02,  1.4606e-02,  1.4434e-02,  3.1507e-03,  2.3843e-02,\n",
       "           6.1777e-03, -1.2282e-02,  2.4180e-02,  2.1637e-02, -1.4293e-02,\n",
       "          -1.6075e-02,  2.7043e-02, -2.2149e-02,  2.7079e-03, -2.1972e-02,\n",
       "          -2.4228e-02,  4.2801e-03,  2.3160e-02,  1.3255e-02, -3.3106e-03,\n",
       "           2.4390e-02,  1.1757e-02,  1.9599e-02,  4.6379e-03, -9.6556e-03,\n",
       "          -2.2697e-03,  1.5277e-02,  2.7209e-02, -1.8809e-02,  1.5442e-02,\n",
       "          -6.2688e-03,  9.1500e-04, -1.4504e-02, -2.7111e-02,  6.7988e-03,\n",
       "           1.1004e-02, -2.5963e-02,  1.0711e-02, -1.5230e-02,  9.5014e-03,\n",
       "           2.6665e-02, -1.3288e-02, -5.8019e-03,  1.9064e-02,  1.5114e-03,\n",
       "           1.8486e-02,  1.3677e-02,  2.3368e-02,  2.5840e-02, -2.1693e-02,\n",
       "          -2.2654e-02, -1.4023e-02,  4.8871e-03, -1.3592e-02,  1.0733e-02,\n",
       "          -7.0216e-03,  2.6703e-02,  1.9415e-02,  6.7075e-03,  8.1944e-03,\n",
       "          -2.2995e-02, -2.7561e-02, -1.4811e-02,  1.9155e-02, -2.2016e-02,\n",
       "           1.2153e-02,  2.6658e-02,  1.7586e-02,  2.2042e-02,  2.3124e-02,\n",
       "           9.3356e-03,  1.8668e-02, -2.2760e-02, -1.9499e-02,  1.7789e-02,\n",
       "           1.9270e-03,  2.3114e-03, -1.6718e-02,  2.7613e-02, -8.4870e-03,\n",
       "           8.6347e-03,  1.8766e-02,  2.2144e-03, -1.0516e-02,  2.6861e-02,\n",
       "           2.3683e-02,  1.0025e-03,  2.6339e-02,  7.3144e-03,  7.0897e-03,\n",
       "           1.2785e-02,  1.0242e-02,  2.4455e-02,  1.6733e-02,  1.0857e-02,\n",
       "          -9.6336e-03,  4.4621e-03,  1.9068e-03,  2.1182e-02, -3.3315e-03,\n",
       "           1.6368e-02, -2.6026e-02,  1.7959e-02, -1.0347e-02, -1.7734e-02,\n",
       "          -2.4664e-02, -2.4113e-02,  9.3167e-03,  1.0757e-02, -2.5432e-02,\n",
       "           1.6188e-02, -2.6272e-02, -1.3033e-02, -1.9986e-02, -9.4863e-03,\n",
       "          -5.4340e-03, -3.2884e-04,  1.8804e-02, -1.4665e-02,  2.0427e-03,\n",
       "           1.1412e-02, -2.1349e-02,  7.6700e-03, -8.9484e-03, -1.4267e-02,\n",
       "          -1.2196e-02,  1.8990e-02, -1.6063e-02,  9.1422e-03, -2.4635e-02,\n",
       "           2.7921e-02, -3.3727e-03, -1.9938e-02, -1.8683e-02,  1.9322e-02,\n",
       "           5.2451e-03, -2.6974e-02, -2.4102e-02, -2.7368e-02, -1.5251e-02,\n",
       "           2.7148e-02, -1.6956e-02,  1.2195e-02, -2.5164e-02, -2.6322e-02,\n",
       "          -8.0828e-03,  2.2851e-02,  2.3740e-03, -2.6463e-02, -9.5715e-03,\n",
       "           1.5710e-02, -1.1479e-03,  1.5684e-02,  5.9235e-03,  1.9210e-02,\n",
       "           2.5914e-02, -2.4770e-03, -4.4767e-03,  1.8469e-02, -2.1877e-02,\n",
       "          -1.7961e-02,  2.6535e-02, -4.9171e-03, -2.5865e-03,  1.6939e-02,\n",
       "           6.6252e-03, -5.1856e-03, -2.0028e-02,  7.1727e-03, -1.3748e-02,\n",
       "          -8.9693e-03,  1.9506e-02,  2.7449e-02, -1.4481e-02, -2.2633e-03,\n",
       "           1.5427e-02, -3.2075e-03,  5.0617e-03, -1.0351e-02,  1.2019e-02,\n",
       "          -2.3252e-02,  4.8772e-03,  2.0581e-02, -2.6753e-03,  1.7872e-02,\n",
       "           1.1616e-03, -2.1532e-02, -2.6122e-02, -2.0468e-02,  2.6346e-02,\n",
       "          -3.6354e-03, -2.6958e-02,  2.2826e-02, -2.3792e-02,  1.1643e-02,\n",
       "           2.3803e-03, -1.5696e-02,  1.2430e-02, -1.3557e-02, -2.7613e-02,\n",
       "          -1.1106e-02, -7.7675e-03, -2.0635e-02, -5.8417e-03,  1.5902e-02,\n",
       "          -9.5442e-05, -2.1180e-02, -1.5184e-02,  1.9228e-02,  2.5268e-02,\n",
       "          -2.0962e-02, -1.1576e-02,  6.5630e-03, -1.0812e-02,  1.1681e-02,\n",
       "           3.0120e-03, -1.7332e-02, -1.9323e-02, -1.8368e-02, -1.2605e-03,\n",
       "           1.6823e-02, -1.6562e-02, -2.4164e-02,  2.4682e-02, -1.4001e-02,\n",
       "           2.1346e-02,  8.1035e-03, -2.0158e-02, -5.8582e-03,  1.3071e-02,\n",
       "           3.2014e-03, -2.0297e-02,  2.1994e-02, -2.1505e-02, -6.8273e-03,\n",
       "           8.3722e-03,  2.2110e-02,  8.9876e-03,  2.3566e-02, -1.3515e-02,\n",
       "           1.7487e-02,  1.5623e-02,  1.8947e-04,  1.4295e-02,  2.5598e-02,\n",
       "          -1.5978e-02, -1.5821e-02,  2.3137e-02,  2.0406e-02, -2.7058e-02,\n",
       "           9.8784e-03,  5.7532e-03,  1.5485e-03,  1.3734e-02,  1.0478e-02,\n",
       "           2.5756e-02,  2.6169e-02,  1.9331e-03, -2.5886e-02,  2.7493e-02,\n",
       "          -3.4013e-03, -4.1952e-03,  1.7159e-02, -2.3011e-02,  1.1224e-02,\n",
       "          -8.8786e-03, -2.0999e-02,  6.0220e-03,  1.5569e-02,  1.6969e-02,\n",
       "           1.2038e-02, -3.9057e-03, -2.3579e-02,  2.1538e-02, -6.4973e-03,\n",
       "          -1.2409e-02,  2.7309e-02,  1.3882e-02,  2.6332e-02,  8.3237e-05,\n",
       "          -1.6738e-02,  7.2184e-03,  1.2698e-02,  8.4758e-03,  2.2157e-02,\n",
       "           1.8597e-02,  2.5373e-02, -2.3414e-03,  4.6488e-03,  2.3830e-02,\n",
       "          -2.2988e-02, -2.1383e-02, -2.1978e-02,  2.5843e-02, -1.0573e-03,\n",
       "          -1.9113e-02,  1.8459e-02, -2.1781e-02, -1.8916e-02,  2.3062e-02,\n",
       "           5.2907e-03,  6.9555e-03, -2.6221e-02,  1.1834e-02, -4.0169e-03,\n",
       "           1.6378e-02, -1.4244e-02,  1.7408e-02, -1.4523e-02,  2.4409e-02,\n",
       "          -1.0962e-02,  8.4201e-03, -1.7522e-02, -2.3737e-02,  1.1973e-02,\n",
       "          -1.3375e-03, -1.3470e-02,  1.9658e-02, -1.4294e-02,  2.6348e-02,\n",
       "          -2.1385e-02, -9.2436e-03, -1.1280e-02, -1.8026e-02,  2.0010e-02,\n",
       "           4.9622e-03,  7.0261e-03, -1.7522e-02,  9.8907e-03,  7.8507e-03,\n",
       "          -5.1004e-03, -2.4025e-03, -1.5241e-02, -8.1815e-03,  1.8645e-02,\n",
       "          -1.3679e-02,  1.9374e-02, -1.2416e-02, -2.7151e-02, -2.3935e-02,\n",
       "          -1.1088e-02, -2.1770e-02,  1.2628e-03, -2.7793e-02, -1.9063e-02,\n",
       "          -4.7114e-03,  2.4275e-02, -2.4963e-02, -1.0704e-03, -1.7651e-02,\n",
       "           1.5751e-02,  3.3101e-03,  1.5123e-02,  2.1718e-02, -1.6365e-02,\n",
       "          -1.2264e-02, -1.1780e-02, -1.0805e-02, -8.1172e-03,  1.3235e-02,\n",
       "          -2.4818e-02, -2.3212e-02, -9.4914e-03,  9.2688e-03, -1.2651e-02,\n",
       "          -1.8517e-02, -1.1228e-03, -2.6802e-02,  2.0229e-02, -1.9423e-02,\n",
       "          -2.6570e-02, -2.2068e-02,  2.0052e-02, -2.2086e-02, -1.8771e-02,\n",
       "           1.2736e-02, -4.4553e-03, -6.8611e-03,  2.2756e-02,  1.4203e-02,\n",
       "          -2.7833e-02,  2.3753e-02,  8.3535e-03,  2.6968e-02,  6.9471e-03,\n",
       "          -1.9633e-02,  2.4046e-02,  9.1865e-03,  9.8366e-03, -9.6852e-03,\n",
       "           2.6970e-02,  9.9527e-03, -2.6499e-02, -1.2173e-02,  8.4841e-03,\n",
       "          -9.4097e-03,  2.0393e-03,  1.1608e-02, -9.1529e-03, -2.7033e-02,\n",
       "           1.8789e-03, -4.5137e-03,  2.5266e-02,  1.2121e-02, -2.3454e-03,\n",
       "          -5.1996e-03, -2.6592e-02, -1.2020e-03,  1.8356e-02, -1.2497e-03,\n",
       "          -2.1749e-02,  2.6188e-02, -1.1786e-02, -2.2033e-03,  3.1883e-03,\n",
       "           1.4504e-02,  6.1590e-03, -1.6354e-02, -2.5410e-02, -2.3253e-02,\n",
       "           2.4289e-02, -7.7675e-03,  1.0567e-02,  1.4490e-02,  5.1086e-04,\n",
       "          -1.2969e-02, -1.8622e-02, -1.4672e-02,  1.8911e-02,  4.9870e-03,\n",
       "          -4.5427e-03,  9.1074e-03, -1.6057e-03,  2.7705e-02, -3.3113e-03,\n",
       "           2.4068e-04,  6.4134e-03, -9.2157e-03, -1.9830e-02, -1.0270e-02,\n",
       "           9.8457e-03,  2.0930e-02, -9.8959e-03,  7.6346e-03,  1.3815e-02,\n",
       "           1.8226e-02, -2.6615e-02, -6.8625e-03,  1.2444e-02, -2.8140e-03,\n",
       "          -1.7385e-02,  2.7495e-02, -2.7595e-03,  7.7921e-03, -2.0836e-02,\n",
       "           1.1523e-02,  1.8200e-02,  2.4942e-02, -2.7638e-02, -1.7686e-02,\n",
       "          -1.8523e-02,  1.0604e-02, -9.6162e-04, -6.7995e-03, -1.0252e-02,\n",
       "          -2.4243e-02,  5.8390e-03, -1.3265e-03,  1.1913e-02,  4.2188e-03,\n",
       "           1.2290e-02,  2.2987e-02,  5.1209e-03,  2.1090e-02, -2.1462e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0086, -0.0208,  0.0131,  ...,  0.0085, -0.0116, -0.0346],\n",
       "          [ 0.0058, -0.0242, -0.0338,  ...,  0.0346, -0.0208, -0.0006],\n",
       "          [ 0.0343, -0.0033, -0.0243,  ..., -0.0068,  0.0006,  0.0309],\n",
       "          ...,\n",
       "          [ 0.0049, -0.0369,  0.0228,  ...,  0.0262,  0.0081,  0.0122],\n",
       "          [ 0.0307,  0.0233, -0.0282,  ..., -0.0319,  0.0173, -0.0298],\n",
       "          [ 0.0086, -0.0316,  0.0293,  ..., -0.0047,  0.0340,  0.0107]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0389, -0.0386, -0.0353,  0.0214, -0.0233,  0.0247, -0.0349,  0.0169],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0153,  0.0269,  0.0067,  ..., -0.0155, -0.0002, -0.0202],\n",
       "          [ 0.0041, -0.0252,  0.0118,  ..., -0.0072, -0.0085, -0.0042],\n",
       "          [ 0.0065,  0.0030,  0.0156,  ..., -0.0056, -0.0023, -0.0061],\n",
       "          ...,\n",
       "          [ 0.0001, -0.0259, -0.0134,  ..., -0.0046,  0.0001, -0.0302],\n",
       "          [ 0.0214,  0.0113,  0.0064,  ...,  0.0151, -0.0059,  0.0046],\n",
       "          [ 0.0086,  0.0262,  0.0052,  ..., -0.0162,  0.0049,  0.0044]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0180, -0.0130, -0.0240,  ...,  0.0137, -0.0017,  0.0269],\n",
       "          [ 0.0214, -0.0099,  0.0114,  ...,  0.0025,  0.0183, -0.0252],\n",
       "          [-0.0199, -0.0097, -0.0005,  ...,  0.0111,  0.0216,  0.0161],\n",
       "          ...,\n",
       "          [ 0.0004,  0.0097,  0.0038,  ..., -0.0180,  0.0097, -0.0059],\n",
       "          [-0.0117,  0.0065, -0.0073,  ..., -0.0019,  0.0084,  0.0093],\n",
       "          [ 0.0188, -0.0040,  0.0269,  ...,  0.0177, -0.0203,  0.0054]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0038, -0.0138, -0.0010,  ..., -0.0275,  0.0130,  0.0259],\n",
       "          [-0.0075, -0.0008,  0.0007,  ..., -0.0041,  0.0105,  0.0048],\n",
       "          [ 0.0068,  0.0077,  0.0154,  ...,  0.0246, -0.0067,  0.0173],\n",
       "          ...,\n",
       "          [-0.0060,  0.0187,  0.0216,  ...,  0.0143,  0.0117,  0.0281],\n",
       "          [-0.0100, -0.0038,  0.0107,  ..., -0.0002, -0.0124, -0.0359],\n",
       "          [-0.0172, -0.0084, -0.0058,  ...,  0.0122, -0.0075, -0.0024]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0097,  0.0191, -0.0128,  ..., -0.0032, -0.0048,  0.0165],\n",
       "          [-0.0259, -0.0007, -0.0030,  ...,  0.0034, -0.0234,  0.0134],\n",
       "          [-0.0045,  0.0424,  0.0011,  ...,  0.0074,  0.0148, -0.0144],\n",
       "          ...,\n",
       "          [-0.0338, -0.0066, -0.0075,  ..., -0.0102,  0.0148, -0.0026],\n",
       "          [-0.0329,  0.0045,  0.0029,  ...,  0.0089,  0.0099, -0.0039],\n",
       "          [-0.0122, -0.0029, -0.0027,  ...,  0.0140, -0.0118,  0.0279]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0100,  0.0118,  0.0212,  ...,  0.0048, -0.0203, -0.0038],\n",
       "          [ 0.0144,  0.0012,  0.0063,  ...,  0.0209,  0.0223, -0.0260],\n",
       "          [ 0.0016, -0.0118, -0.0202,  ..., -0.0109,  0.0184,  0.0062],\n",
       "          ...,\n",
       "          [-0.0248, -0.0242,  0.0072,  ..., -0.0027, -0.0171, -0.0081],\n",
       "          [ 0.0087,  0.0234,  0.0122,  ..., -0.0024, -0.0001,  0.0270],\n",
       "          [-0.0141,  0.0127,  0.0160,  ..., -0.0104,  0.0091, -0.0056]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0102,  0.0256,  0.0087,  0.0185, -0.0093, -0.0079,  0.0113, -0.0082,\n",
       "           0.0017,  0.0229,  0.0275, -0.0039, -0.0185, -0.0114,  0.0184,  0.0219,\n",
       "          -0.0135, -0.0271, -0.0171,  0.0253,  0.0269,  0.0018, -0.0212,  0.0068,\n",
       "          -0.0022,  0.0067,  0.0184,  0.0066,  0.0232, -0.0102,  0.0106,  0.0116,\n",
       "          -0.0148,  0.0160,  0.0080,  0.0096, -0.0048,  0.0129, -0.0165, -0.0261,\n",
       "           0.0230,  0.0234, -0.0087, -0.0184, -0.0203, -0.0202, -0.0031,  0.0077,\n",
       "           0.0045,  0.0172, -0.0176, -0.0005, -0.0246,  0.0029, -0.0198, -0.0126,\n",
       "          -0.0105,  0.0006,  0.0210, -0.0111, -0.0117,  0.0107, -0.0150,  0.0070,\n",
       "           0.0025, -0.0007,  0.0026,  0.0276,  0.0276,  0.0058, -0.0121, -0.0039,\n",
       "           0.0204,  0.0047,  0.0188, -0.0144, -0.0046,  0.0005,  0.0062, -0.0225,\n",
       "           0.0182,  0.0200,  0.0193, -0.0102,  0.0094, -0.0045, -0.0211, -0.0267,\n",
       "           0.0129, -0.0057,  0.0197, -0.0191, -0.0238,  0.0279, -0.0072,  0.0188,\n",
       "          -0.0177,  0.0149, -0.0004, -0.0005,  0.0008,  0.0239,  0.0149,  0.0113,\n",
       "          -0.0028, -0.0176,  0.0117, -0.0024,  0.0170, -0.0016,  0.0254,  0.0085,\n",
       "          -0.0068,  0.0254, -0.0157,  0.0239,  0.0226,  0.0192,  0.0206,  0.0037,\n",
       "           0.0048, -0.0218,  0.0106,  0.0199,  0.0234, -0.0034, -0.0278,  0.0153,\n",
       "          -0.0266,  0.0105,  0.0115,  0.0103, -0.0116,  0.0125,  0.0004,  0.0206,\n",
       "           0.0205, -0.0181,  0.0178, -0.0061,  0.0169, -0.0106, -0.0044, -0.0169,\n",
       "           0.0055,  0.0212, -0.0241, -0.0232,  0.0022, -0.0264, -0.0008, -0.0246,\n",
       "           0.0029,  0.0173,  0.0220, -0.0279, -0.0141,  0.0217, -0.0127, -0.0118,\n",
       "           0.0155, -0.0093,  0.0147, -0.0055,  0.0082,  0.0019,  0.0141,  0.0011,\n",
       "           0.0203,  0.0222,  0.0151, -0.0113, -0.0056,  0.0080,  0.0113, -0.0057,\n",
       "          -0.0105, -0.0027, -0.0093,  0.0085,  0.0268,  0.0082, -0.0035, -0.0062,\n",
       "           0.0267, -0.0107, -0.0019,  0.0161, -0.0098, -0.0118, -0.0099, -0.0186,\n",
       "          -0.0106, -0.0257, -0.0188,  0.0097,  0.0035,  0.0112, -0.0245, -0.0093,\n",
       "          -0.0116, -0.0169, -0.0033,  0.0257, -0.0186, -0.0236, -0.0253,  0.0239,\n",
       "           0.0011,  0.0220, -0.0140, -0.0042,  0.0079,  0.0128, -0.0045,  0.0274,\n",
       "           0.0008,  0.0046,  0.0225,  0.0166,  0.0156,  0.0242, -0.0248, -0.0104,\n",
       "          -0.0273, -0.0156,  0.0094, -0.0179,  0.0246, -0.0107,  0.0180,  0.0248,\n",
       "           0.0074,  0.0218,  0.0047,  0.0081, -0.0242,  0.0112, -0.0107,  0.0075,\n",
       "          -0.0029,  0.0234,  0.0191,  0.0120, -0.0120, -0.0163, -0.0260,  0.0236,\n",
       "          -0.0114,  0.0102,  0.0068,  0.0092,  0.0059, -0.0221,  0.0133,  0.0128,\n",
       "          -0.0046,  0.0028,  0.0164,  0.0135, -0.0036, -0.0064,  0.0074,  0.0125,\n",
       "           0.0052,  0.0025, -0.0181,  0.0020,  0.0268,  0.0184, -0.0185,  0.0272,\n",
       "           0.0135, -0.0043, -0.0151,  0.0208, -0.0049,  0.0206,  0.0189, -0.0245,\n",
       "          -0.0211,  0.0127,  0.0211, -0.0032, -0.0144, -0.0090, -0.0220, -0.0106,\n",
       "          -0.0256, -0.0182, -0.0090, -0.0213,  0.0163, -0.0094,  0.0098, -0.0216,\n",
       "           0.0111, -0.0087, -0.0170, -0.0193, -0.0056,  0.0117,  0.0217,  0.0230,\n",
       "          -0.0055,  0.0004, -0.0233, -0.0257, -0.0172, -0.0200,  0.0188,  0.0115,\n",
       "          -0.0072,  0.0279, -0.0230,  0.0255, -0.0083,  0.0235,  0.0058,  0.0071,\n",
       "          -0.0012,  0.0098, -0.0274,  0.0204, -0.0180, -0.0083, -0.0202, -0.0210,\n",
       "           0.0241, -0.0190,  0.0215,  0.0149,  0.0204,  0.0224, -0.0066, -0.0106,\n",
       "          -0.0012,  0.0132,  0.0201, -0.0117,  0.0176,  0.0157, -0.0214,  0.0214,\n",
       "           0.0157,  0.0199, -0.0272,  0.0100,  0.0197,  0.0059,  0.0069,  0.0101,\n",
       "          -0.0088, -0.0279,  0.0042,  0.0217,  0.0200,  0.0036, -0.0207, -0.0020,\n",
       "          -0.0120,  0.0085, -0.0149, -0.0270, -0.0181,  0.0152,  0.0131,  0.0073,\n",
       "          -0.0109, -0.0159,  0.0150, -0.0196,  0.0065, -0.0210, -0.0214, -0.0201,\n",
       "          -0.0105, -0.0048, -0.0272,  0.0139,  0.0214,  0.0196,  0.0208, -0.0095,\n",
       "           0.0190,  0.0061,  0.0273,  0.0239, -0.0175,  0.0160,  0.0119, -0.0013,\n",
       "          -0.0246, -0.0235,  0.0223, -0.0154,  0.0085,  0.0020, -0.0069, -0.0213,\n",
       "          -0.0179,  0.0263, -0.0022,  0.0012,  0.0059,  0.0019,  0.0091, -0.0153,\n",
       "           0.0145, -0.0021,  0.0149,  0.0022, -0.0230,  0.0086, -0.0204, -0.0187,\n",
       "           0.0006, -0.0217,  0.0145, -0.0040,  0.0190, -0.0049,  0.0059, -0.0230,\n",
       "          -0.0039,  0.0213, -0.0064,  0.0257, -0.0046,  0.0168,  0.0275, -0.0137,\n",
       "           0.0279, -0.0029,  0.0232,  0.0135,  0.0182,  0.0143, -0.0086, -0.0273,\n",
       "           0.0185, -0.0168, -0.0024, -0.0009,  0.0278, -0.0032, -0.0004,  0.0188,\n",
       "          -0.0005,  0.0020, -0.0192,  0.0086, -0.0170, -0.0131, -0.0194,  0.0156,\n",
       "          -0.0053, -0.0062, -0.0155,  0.0217,  0.0062, -0.0265,  0.0030, -0.0208,\n",
       "          -0.0110,  0.0013,  0.0248, -0.0011, -0.0259,  0.0034, -0.0149, -0.0041,\n",
       "          -0.0002,  0.0203,  0.0129,  0.0009,  0.0178,  0.0185, -0.0197, -0.0225,\n",
       "           0.0246, -0.0241, -0.0103,  0.0235, -0.0164, -0.0199, -0.0174, -0.0069,\n",
       "          -0.0270,  0.0167,  0.0020,  0.0091,  0.0146, -0.0016, -0.0252,  0.0172,\n",
       "          -0.0245, -0.0123, -0.0057, -0.0161, -0.0047,  0.0256, -0.0158,  0.0239,\n",
       "           0.0002,  0.0026,  0.0140, -0.0120, -0.0229, -0.0146, -0.0195,  0.0073,\n",
       "          -0.0005,  0.0019, -0.0175, -0.0151, -0.0138,  0.0014,  0.0088,  0.0116,\n",
       "          -0.0124,  0.0233,  0.0174, -0.0241,  0.0189,  0.0128,  0.0245, -0.0099,\n",
       "          -0.0095, -0.0058, -0.0030, -0.0113, -0.0075,  0.0087, -0.0167,  0.0224,\n",
       "          -0.0256, -0.0061,  0.0177,  0.0077,  0.0264, -0.0099,  0.0149,  0.0097,\n",
       "          -0.0015, -0.0151,  0.0128,  0.0230, -0.0058,  0.0180,  0.0115, -0.0258,\n",
       "           0.0105,  0.0089,  0.0214,  0.0021, -0.0148, -0.0152, -0.0242,  0.0194,\n",
       "          -0.0139,  0.0157,  0.0100, -0.0158,  0.0016, -0.0109, -0.0220, -0.0079,\n",
       "          -0.0077,  0.0251, -0.0270,  0.0120,  0.0269, -0.0170,  0.0172, -0.0194,\n",
       "           0.0177, -0.0195, -0.0259,  0.0252, -0.0164,  0.0074,  0.0104,  0.0231,\n",
       "           0.0237,  0.0268,  0.0018,  0.0022, -0.0109,  0.0199,  0.0003,  0.0269,\n",
       "          -0.0029,  0.0092, -0.0157,  0.0043, -0.0271,  0.0259,  0.0264, -0.0224,\n",
       "          -0.0242,  0.0278,  0.0023,  0.0202,  0.0266,  0.0034,  0.0005,  0.0130,\n",
       "          -0.0163, -0.0113, -0.0142,  0.0273, -0.0132, -0.0233,  0.0158, -0.0226,\n",
       "          -0.0088,  0.0187,  0.0258, -0.0009,  0.0092,  0.0061, -0.0072,  0.0212,\n",
       "          -0.0099, -0.0167,  0.0008,  0.0069,  0.0256,  0.0127, -0.0093,  0.0278,\n",
       "          -0.0150,  0.0008,  0.0155,  0.0090, -0.0078, -0.0126, -0.0087,  0.0182],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.0605e-02,  1.9583e-02, -3.5258e-02,  ..., -1.3523e-03,\n",
       "            3.0416e-02, -1.3117e-02],\n",
       "          [-3.5658e-02,  2.6008e-02,  2.8234e-03,  ..., -7.9996e-03,\n",
       "            1.8793e-02, -2.9035e-02],\n",
       "          [ 2.6104e-02,  7.9195e-03, -1.4179e-05,  ..., -3.5878e-02,\n",
       "           -2.5787e-02, -3.3308e-03],\n",
       "          ...,\n",
       "          [-1.2253e-02, -9.5851e-04,  7.8604e-03,  ..., -2.3871e-02,\n",
       "            7.4640e-03,  1.5421e-02],\n",
       "          [ 8.9510e-03,  9.2159e-03, -1.8830e-02,  ...,  5.4662e-03,\n",
       "           -1.9263e-02, -2.5057e-02],\n",
       "          [ 1.7993e-02, -2.9457e-02, -3.8810e-02,  ...,  3.7249e-02,\n",
       "            2.4344e-02,  2.7918e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0391, -0.0144, -0.0319,  0.0139, -0.0203,  0.0204,  0.0173, -0.0098],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0153,  0.0136,  0.0105,  ...,  0.0192, -0.0034, -0.0210],\n",
       "          [ 0.0004,  0.0201,  0.0016,  ..., -0.0285, -0.0396,  0.0284],\n",
       "          [-0.0036, -0.0056,  0.0014,  ..., -0.0055, -0.0067,  0.0006],\n",
       "          ...,\n",
       "          [ 0.0092,  0.0142,  0.0136,  ..., -0.0036, -0.0163, -0.0036],\n",
       "          [ 0.0161, -0.0138,  0.0011,  ...,  0.0097, -0.0089,  0.0014],\n",
       "          [-0.0192, -0.0138, -0.0015,  ..., -0.0051,  0.0212, -0.0059]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.6220e-02,  7.6169e-03, -5.1948e-03,  ..., -5.3794e-03,\n",
       "            1.0883e-02,  1.2686e-03],\n",
       "          [-8.6776e-03,  1.0862e-02, -1.9365e-02,  ...,  1.3719e-02,\n",
       "           -4.1777e-02,  2.3241e-02],\n",
       "          [-5.6295e-03,  7.3887e-05,  1.8782e-03,  ...,  4.5498e-03,\n",
       "            5.4409e-03,  1.5876e-02],\n",
       "          ...,\n",
       "          [ 2.0516e-02,  1.4616e-02,  1.0067e-02,  ..., -1.7108e-02,\n",
       "           -1.6274e-02,  2.5798e-02],\n",
       "          [-3.6799e-02,  2.3742e-02, -1.4952e-02,  ...,  7.9567e-03,\n",
       "           -7.8237e-03, -2.9041e-03],\n",
       "          [ 1.7855e-02, -3.3147e-02,  4.0669e-04,  ..., -6.3198e-03,\n",
       "            8.6180e-03, -6.2329e-03]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0051, -0.0088,  0.0210,  ...,  0.0015,  0.0176, -0.0051],\n",
       "          [ 0.0043,  0.0268, -0.0344,  ...,  0.0140, -0.0083, -0.0074],\n",
       "          [-0.0396,  0.0143, -0.0106,  ...,  0.0090, -0.0087,  0.0014],\n",
       "          ...,\n",
       "          [ 0.0312, -0.0363,  0.0051,  ...,  0.0199,  0.0192, -0.0153],\n",
       "          [-0.0036, -0.0042, -0.0043,  ..., -0.0122, -0.0092, -0.0049],\n",
       "          [-0.0128,  0.0074, -0.0154,  ..., -0.0042,  0.0048, -0.0051]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0078, -0.0169, -0.0105,  ...,  0.0021, -0.0166,  0.0040],\n",
       "          [-0.0076, -0.0094,  0.0040,  ..., -0.0292, -0.0118,  0.0221],\n",
       "          [ 0.0158, -0.0055, -0.0026,  ..., -0.0126,  0.0077, -0.0246],\n",
       "          ...,\n",
       "          [-0.0276, -0.0268,  0.0106,  ..., -0.0023, -0.0053,  0.0054],\n",
       "          [-0.0099, -0.0282, -0.0171,  ..., -0.0030, -0.0008,  0.0079],\n",
       "          [-0.0084, -0.0066, -0.0039,  ...,  0.0060,  0.0183,  0.0200]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0137, -0.0068, -0.0172,  ...,  0.0119, -0.0144,  0.0038],\n",
       "          [-0.0028, -0.0014,  0.0257,  ...,  0.0151,  0.0196,  0.0107],\n",
       "          [-0.0011,  0.0096, -0.0246,  ...,  0.0228,  0.0245, -0.0004],\n",
       "          ...,\n",
       "          [-0.0232, -0.0269, -0.0132,  ..., -0.0128,  0.0191,  0.0229],\n",
       "          [ 0.0007, -0.0097, -0.0056,  ...,  0.0106,  0.0153, -0.0097],\n",
       "          [ 0.0122, -0.0036,  0.0097,  ..., -0.0114,  0.0269, -0.0078]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 2.7091e-02,  4.9275e-03,  5.3596e-03, -2.7748e-02,  1.0125e-02,\n",
       "          -1.6339e-02, -2.1544e-02,  3.0338e-03,  2.5778e-02, -2.3825e-02,\n",
       "          -6.7277e-03,  1.0879e-02, -1.6381e-03,  1.2457e-02,  2.3578e-02,\n",
       "           5.2522e-03,  1.7555e-02, -1.1819e-02,  1.8856e-02,  3.0802e-03,\n",
       "           1.4301e-02,  2.1392e-02,  2.8020e-03,  2.6039e-02, -1.1225e-02,\n",
       "          -3.0732e-03,  1.8951e-02,  2.4148e-02,  2.4854e-02, -4.9112e-03,\n",
       "           4.3001e-03,  7.3460e-03,  2.4939e-02, -1.2990e-02, -2.2455e-02,\n",
       "          -2.7440e-02, -2.0110e-02, -1.8169e-03, -2.0682e-02,  1.6435e-02,\n",
       "           2.3247e-02,  9.5941e-03, -6.7774e-03, -2.2792e-02, -1.5886e-02,\n",
       "           2.7377e-02,  4.1387e-03,  2.6023e-02, -1.5699e-02,  1.0257e-02,\n",
       "           1.6769e-02,  2.7777e-02, -2.4405e-02,  2.0598e-02, -3.5237e-03,\n",
       "           1.6143e-02,  2.4024e-02,  6.8703e-03,  8.5561e-03, -1.4640e-02,\n",
       "           2.2731e-02,  2.6411e-02, -2.3535e-02,  6.4113e-03, -1.1448e-02,\n",
       "           6.2568e-03,  9.0802e-03,  2.1040e-02,  2.1835e-02,  1.6939e-02,\n",
       "           2.2037e-02, -2.7888e-02,  1.0233e-02, -1.9775e-02,  1.8057e-02,\n",
       "          -2.6063e-03,  2.7467e-02, -4.2854e-03,  9.4926e-03,  1.0136e-02,\n",
       "           2.2978e-02, -8.0923e-03, -6.4460e-03, -2.3631e-02,  5.4475e-03,\n",
       "           1.2904e-03, -2.0982e-02,  1.2550e-02,  2.0746e-02, -1.2721e-02,\n",
       "           5.4398e-04,  2.7527e-02,  1.5008e-02, -1.7756e-02, -1.5352e-02,\n",
       "          -3.3224e-03,  2.6387e-02, -2.5273e-02, -5.9940e-03,  2.1328e-02,\n",
       "          -1.0495e-02, -2.2826e-02, -5.1472e-03,  1.2457e-03, -1.1230e-02,\n",
       "           1.7011e-02,  1.2177e-02, -1.1245e-02, -2.6538e-02, -7.2073e-03,\n",
       "           1.6197e-02,  2.3466e-02,  5.6140e-03,  2.1724e-02, -3.0324e-03,\n",
       "           7.5840e-03,  1.9570e-02, -1.6393e-02,  2.4103e-03, -3.7734e-03,\n",
       "          -8.7833e-03, -2.7946e-02, -3.3042e-03, -2.5780e-02,  8.0416e-03,\n",
       "          -2.1495e-03, -7.5105e-03,  5.9079e-03, -3.0688e-04,  1.3246e-02,\n",
       "          -2.1164e-02, -1.1017e-02, -1.9519e-02, -7.4676e-03, -2.0479e-02,\n",
       "           1.8872e-02, -2.4484e-02, -1.9987e-02,  2.0962e-02, -2.1822e-02,\n",
       "          -1.3457e-02,  2.1349e-02,  9.1057e-03, -2.6574e-03, -6.6556e-03,\n",
       "           1.2529e-02,  6.8291e-04,  6.4540e-04,  1.4236e-02, -4.6659e-03,\n",
       "           6.8478e-03, -1.8424e-02,  1.8033e-05,  8.2659e-03, -1.3516e-02,\n",
       "          -1.4215e-02,  2.0977e-02, -2.1387e-02, -1.0653e-02, -2.0223e-02,\n",
       "           1.6209e-04,  1.7978e-02,  2.6830e-02, -7.7706e-03,  9.7433e-03,\n",
       "          -1.5907e-02, -1.3626e-02,  2.7895e-02, -2.7897e-02,  1.7117e-02,\n",
       "           2.4978e-02,  2.7184e-02, -5.3819e-03, -1.6629e-02, -1.8484e-02,\n",
       "          -1.4769e-02,  9.9705e-03,  2.2377e-02, -2.5742e-02,  2.3255e-02,\n",
       "          -2.0526e-02, -2.2884e-02, -4.6329e-03,  1.4548e-02,  1.8236e-02,\n",
       "           4.2169e-03,  2.0061e-02, -1.3298e-02, -1.6832e-02,  4.9400e-03,\n",
       "           1.9226e-06,  2.3714e-02,  1.0395e-02,  7.9870e-03, -5.5280e-03,\n",
       "          -7.1242e-04,  1.7247e-03,  1.7708e-02,  4.3617e-03,  8.9385e-03,\n",
       "           2.7583e-02,  2.4190e-02, -1.6547e-02, -2.9436e-03, -1.0237e-02,\n",
       "           1.6936e-02,  1.1728e-02, -1.2393e-02, -1.7392e-02,  1.6293e-03,\n",
       "          -2.4512e-02,  2.3875e-02, -8.0788e-03,  5.4604e-03, -1.7628e-02,\n",
       "           9.6543e-03,  1.3285e-03,  8.2511e-03,  1.1710e-03,  1.8870e-02,\n",
       "          -1.0127e-02, -8.9752e-03,  7.4321e-03,  1.6668e-02,  3.2633e-03,\n",
       "          -8.1298e-03,  2.2346e-02,  2.0563e-02,  4.9200e-03,  1.9788e-02,\n",
       "          -2.6184e-02,  2.4662e-02, -2.4827e-02, -8.5008e-03, -1.4918e-02,\n",
       "           1.1373e-02,  5.9522e-03, -1.7553e-02,  1.0651e-02, -2.3857e-02,\n",
       "           1.6767e-04, -8.9046e-03,  3.6690e-03,  2.3325e-02,  9.5372e-04,\n",
       "          -2.6738e-02,  2.1089e-02, -1.1427e-02, -3.8170e-03, -2.5151e-03,\n",
       "          -1.4440e-02, -1.8162e-02, -1.8663e-02, -1.2748e-02, -4.6725e-03,\n",
       "          -2.6552e-02,  2.6608e-02,  1.2112e-02,  1.5580e-02,  2.1911e-02,\n",
       "          -2.0183e-02, -2.3829e-02, -5.0460e-03, -1.3307e-02, -8.2766e-03,\n",
       "           2.6683e-02,  2.7913e-02, -2.2294e-02, -2.2808e-02,  2.2004e-02,\n",
       "           3.4253e-03, -2.7587e-02, -6.4984e-05, -1.9881e-02, -2.3632e-02,\n",
       "           2.6946e-02,  2.5921e-02, -3.8343e-03, -2.4850e-02,  8.1130e-03,\n",
       "          -1.5505e-02, -3.9323e-03, -2.2771e-03,  2.2673e-03, -3.9477e-03,\n",
       "           8.9547e-03, -2.1619e-02, -2.4730e-02, -9.1523e-03,  2.5287e-02,\n",
       "           9.3214e-03,  1.1266e-02,  5.1964e-03,  1.9937e-02,  1.1490e-02,\n",
       "           2.5022e-02,  1.9021e-02, -1.5826e-02,  4.5733e-03,  1.2090e-02,\n",
       "          -1.6743e-02, -7.9398e-03, -2.5165e-02,  2.3959e-02,  3.0759e-03,\n",
       "           6.1996e-03, -1.2699e-02,  2.2708e-02, -1.0829e-03, -2.2739e-02,\n",
       "          -2.6631e-02, -1.1862e-02,  2.7608e-02, -2.1894e-02, -1.4446e-03,\n",
       "           1.1816e-02,  7.1428e-04, -1.4605e-02, -2.2847e-03,  2.2552e-02,\n",
       "           4.0285e-03,  4.4046e-03, -1.4964e-02,  1.4111e-02,  4.3377e-03,\n",
       "           2.7776e-02,  1.8884e-02, -1.5993e-02, -1.1936e-02,  6.7622e-03,\n",
       "          -2.5321e-02, -3.3753e-03, -2.0665e-02, -2.3194e-02, -5.5805e-03,\n",
       "          -9.6367e-04,  5.5848e-04,  5.3874e-03, -2.5690e-02,  2.5288e-02,\n",
       "           8.7043e-03,  1.2629e-02,  1.0768e-02,  2.1872e-02, -1.2704e-02,\n",
       "           2.2001e-02,  1.4358e-02,  1.4859e-02, -2.7451e-02, -4.6991e-03,\n",
       "           2.5309e-02, -1.9943e-02, -1.7105e-02,  4.3671e-03,  8.8264e-03,\n",
       "          -1.7722e-02,  1.0472e-02,  5.3628e-03, -1.5599e-02,  1.4337e-02,\n",
       "           1.8680e-03,  9.8041e-03,  1.4631e-02, -1.7633e-03,  1.2243e-02,\n",
       "          -2.7745e-02, -5.6850e-03,  2.5265e-03,  1.7280e-02,  1.9874e-02,\n",
       "          -2.6636e-02, -2.0934e-02,  1.5056e-02,  6.9087e-03,  7.5170e-03,\n",
       "          -1.2433e-02,  5.0886e-04,  1.7035e-02, -2.5667e-02, -1.6301e-02,\n",
       "           1.0586e-02,  1.7778e-02, -1.7733e-02,  9.5056e-03,  1.2461e-04,\n",
       "          -1.4715e-02,  1.6754e-02, -1.4862e-02,  2.4541e-02,  1.0112e-02,\n",
       "           1.0804e-02, -8.8106e-03, -1.8871e-02, -2.7907e-02, -8.8445e-03,\n",
       "           8.0054e-03, -6.9443e-04,  9.7471e-03,  2.2898e-03,  1.5527e-02,\n",
       "          -1.7118e-02,  3.0580e-03, -1.8462e-02, -3.3450e-04,  1.3401e-02,\n",
       "          -1.7543e-02,  1.6661e-02, -2.1033e-02,  4.4957e-03,  8.3081e-03,\n",
       "           5.6082e-03,  5.3478e-03, -2.6474e-02,  2.7903e-02,  1.6539e-02,\n",
       "           1.3415e-02,  9.7083e-03,  7.7483e-03,  4.2146e-03,  1.5405e-02,\n",
       "          -2.4952e-02, -1.1629e-02,  1.1397e-02,  3.4254e-04,  2.3549e-02,\n",
       "          -1.5239e-02, -2.3533e-02, -2.5012e-02, -8.1476e-03, -1.1407e-02,\n",
       "           5.8141e-03,  3.7211e-03, -2.0831e-02, -2.2233e-02, -6.2477e-03,\n",
       "           1.8993e-02,  1.6394e-02, -1.7459e-02, -1.1730e-02,  2.5600e-02,\n",
       "           6.1703e-03,  1.6916e-02,  1.8490e-02, -2.2404e-02, -3.9166e-03,\n",
       "           2.4923e-02,  6.8201e-04,  2.4011e-02,  1.4011e-02, -7.1176e-03,\n",
       "           2.6271e-02,  2.4152e-03, -2.2611e-02, -2.7517e-02, -1.9401e-02,\n",
       "           1.6009e-02,  3.1628e-03, -2.3390e-02,  1.6922e-02, -4.3584e-03,\n",
       "          -1.4901e-02,  1.8993e-02,  8.7542e-03,  1.8103e-02,  5.9618e-03,\n",
       "          -1.7236e-02,  1.0719e-02,  2.7059e-02, -9.1561e-03, -1.8856e-02,\n",
       "           1.9574e-02,  1.9183e-02, -2.0552e-02, -2.0032e-02,  1.9632e-02,\n",
       "          -5.8932e-03,  1.3209e-03, -1.6535e-03,  1.8151e-04, -1.1886e-02,\n",
       "           2.0238e-02,  9.6669e-03, -3.6319e-04,  2.3338e-03,  2.3600e-02,\n",
       "          -1.4130e-02,  1.0273e-02, -2.3801e-02, -1.8281e-02,  1.1581e-02,\n",
       "          -2.1138e-02,  2.3323e-02,  2.3278e-03, -1.6036e-02,  6.5544e-03,\n",
       "           1.1784e-02,  5.9912e-03,  3.1616e-03, -2.1209e-02, -2.7032e-02,\n",
       "           5.0784e-03,  1.1088e-02, -1.6375e-02, -2.4850e-02, -1.1789e-02,\n",
       "           2.4116e-02,  2.6135e-02, -2.0990e-02,  1.3013e-03, -2.7450e-02,\n",
       "           2.4801e-02,  1.0162e-02,  1.6935e-02,  1.2211e-02,  1.4585e-02,\n",
       "          -6.0010e-03, -2.4907e-02, -2.1860e-02, -2.4114e-02, -4.4471e-03,\n",
       "          -2.3934e-02, -1.1110e-02,  2.2539e-02,  1.8885e-03, -1.6005e-02,\n",
       "           2.0117e-02,  2.1121e-02, -1.7836e-02, -5.4680e-03, -1.3289e-02,\n",
       "           1.7865e-02,  2.7476e-02,  1.1993e-02, -1.5400e-02, -2.0942e-02,\n",
       "          -1.9932e-02, -2.6854e-02, -1.5574e-02, -2.1138e-02, -1.7237e-02,\n",
       "           1.8490e-04,  1.8733e-02, -1.7393e-02, -1.1314e-02,  2.5228e-03,\n",
       "          -3.3418e-03,  2.6838e-02,  3.6610e-03,  2.0513e-02,  2.6265e-02,\n",
       "          -1.7670e-02, -8.5708e-03,  1.8344e-02, -2.5189e-02,  2.3102e-02,\n",
       "          -8.7761e-04,  1.4535e-02,  1.9480e-02, -1.0291e-02, -8.0139e-03,\n",
       "           6.1442e-03,  8.9962e-03,  2.2872e-02, -2.4413e-02,  1.6100e-02,\n",
       "          -5.4288e-03,  1.5269e-02,  2.6108e-02,  2.2866e-03,  2.6037e-02,\n",
       "           2.4814e-02, -2.4318e-02,  1.4035e-02, -2.2011e-02,  1.3472e-02,\n",
       "           2.2643e-02,  1.4037e-02, -2.1677e-02,  3.6135e-03,  1.2454e-02,\n",
       "          -2.2152e-02,  1.0859e-02,  1.7577e-02,  1.5951e-02,  1.2067e-02,\n",
       "           7.9062e-03, -6.4407e-03, -2.3479e-02, -6.2331e-03,  1.7733e-02,\n",
       "           6.9941e-03,  6.4762e-03, -7.3889e-03, -7.8253e-03, -1.0823e-02,\n",
       "          -2.5950e-02, -5.7740e-03, -2.3701e-02,  2.3931e-02,  1.5332e-02,\n",
       "          -1.3299e-03,  2.3663e-02, -6.8304e-03,  2.1366e-02,  1.5171e-03,\n",
       "           4.6348e-03,  2.2219e-02,  1.9197e-02, -6.6672e-03, -1.9968e-02,\n",
       "           2.5198e-02,  2.2324e-02,  1.0620e-02,  9.4886e-04,  2.6379e-03,\n",
       "           2.5114e-02, -2.4554e-02, -6.0326e-04,  5.6077e-03,  1.2555e-02,\n",
       "          -2.4497e-02, -2.6084e-02,  5.3046e-03,  1.4403e-02, -7.2353e-04,\n",
       "           1.7511e-02,  2.5775e-02,  1.5580e-02, -2.0901e-02,  2.1520e-02,\n",
       "          -6.2209e-04, -6.6527e-04, -2.6387e-02, -2.3828e-02, -1.7291e-02,\n",
       "           5.8396e-03, -1.0436e-02,  2.6546e-04,  1.1655e-02,  1.4597e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0134,  0.0165,  0.0029,  ...,  0.0049, -0.0219, -0.0183],\n",
       "          [-0.0024, -0.0380,  0.0063,  ...,  0.0330, -0.0342, -0.0132],\n",
       "          [ 0.0016, -0.0283,  0.0162,  ..., -0.0163,  0.0084,  0.0106],\n",
       "          ...,\n",
       "          [ 0.0169, -0.0359, -0.0263,  ..., -0.0254, -0.0071, -0.0360],\n",
       "          [-0.0360, -0.0028, -0.0008,  ..., -0.0031,  0.0286,  0.0043],\n",
       "          [ 0.0021, -0.0221,  0.0176,  ...,  0.0080, -0.0102,  0.0284]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0154,  0.0364, -0.0079, -0.0386, -0.0249,  0.0077, -0.0331, -0.0365],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0015,  0.0003, -0.0111,  ...,  0.0129, -0.0070,  0.0053],\n",
       "          [-0.0077, -0.0105,  0.0176,  ...,  0.0131,  0.0184, -0.0172],\n",
       "          [-0.0008,  0.0048,  0.0173,  ...,  0.0044, -0.0148,  0.0151],\n",
       "          ...,\n",
       "          [ 0.0081,  0.0038,  0.0027,  ..., -0.0095, -0.0126, -0.0092],\n",
       "          [-0.0101, -0.0111, -0.0042,  ..., -0.0108, -0.0066, -0.0281],\n",
       "          [-0.0028,  0.0023,  0.0198,  ...,  0.0228,  0.0149,  0.0221]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0050, -0.0016, -0.0184,  ..., -0.0340, -0.0178, -0.0092],\n",
       "          [-0.0089,  0.0101,  0.0055,  ...,  0.0049,  0.0131,  0.0130],\n",
       "          [ 0.0042,  0.0226, -0.0074,  ..., -0.0272,  0.0078,  0.0410],\n",
       "          ...,\n",
       "          [ 0.0123,  0.0014,  0.0046,  ...,  0.0119, -0.0044,  0.0012],\n",
       "          [ 0.0149,  0.0148,  0.0003,  ..., -0.0111, -0.0072,  0.0133],\n",
       "          [-0.0109, -0.0065, -0.0170,  ..., -0.0027, -0.0144, -0.0213]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0301, -0.0015,  0.0219,  ..., -0.0134,  0.0102, -0.0052],\n",
       "          [ 0.0142,  0.0008, -0.0003,  ...,  0.0194, -0.0071,  0.0053],\n",
       "          [-0.0163, -0.0182,  0.0106,  ...,  0.0112,  0.0123,  0.0074],\n",
       "          ...,\n",
       "          [ 0.0128,  0.0014, -0.0014,  ..., -0.0173, -0.0162, -0.0100],\n",
       "          [ 0.0053,  0.0100,  0.0035,  ..., -0.0006,  0.0019, -0.0134],\n",
       "          [ 0.0081,  0.0256, -0.0054,  ...,  0.0178,  0.0076,  0.0033]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0145, -0.0248, -0.0199,  ...,  0.0160,  0.0248, -0.0062],\n",
       "          [-0.0013,  0.0087,  0.0104,  ..., -0.0046, -0.0065,  0.0236],\n",
       "          [-0.0181,  0.0012,  0.0204,  ..., -0.0027, -0.0103,  0.0174],\n",
       "          ...,\n",
       "          [-0.0041, -0.0033,  0.0008,  ..., -0.0033, -0.0173, -0.0253],\n",
       "          [ 0.0087, -0.0303, -0.0069,  ...,  0.0133, -0.0193, -0.0097],\n",
       "          [-0.0066,  0.0112,  0.0048,  ..., -0.0062,  0.0022,  0.0168]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0245, -0.0209, -0.0215,  ..., -0.0173,  0.0005, -0.0256],\n",
       "          [ 0.0064,  0.0092, -0.0240,  ...,  0.0206,  0.0094, -0.0189],\n",
       "          [ 0.0232,  0.0031,  0.0227,  ...,  0.0260,  0.0079,  0.0039],\n",
       "          ...,\n",
       "          [-0.0206, -0.0200, -0.0003,  ..., -0.0005,  0.0173,  0.0235],\n",
       "          [-0.0148, -0.0124, -0.0038,  ...,  0.0279,  0.0172, -0.0009],\n",
       "          [-0.0178, -0.0078,  0.0200,  ...,  0.0118, -0.0149, -0.0090]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-8.9843e-03, -1.6671e-02,  2.7249e-02, -1.6733e-02, -3.3511e-03,\n",
       "           2.3565e-02, -2.1666e-02,  1.7941e-02,  1.3824e-02,  6.3391e-03,\n",
       "          -1.8279e-02, -2.8808e-03, -1.4330e-03, -1.5227e-02, -1.8819e-02,\n",
       "          -8.0851e-03,  4.0756e-03,  9.3407e-03,  8.9569e-03,  5.1521e-03,\n",
       "           2.2458e-02, -2.5573e-02,  2.2795e-02,  8.5317e-03, -1.1333e-03,\n",
       "           2.3753e-02,  2.6899e-02,  1.5333e-02, -1.6736e-02,  1.7243e-02,\n",
       "           2.3377e-02,  5.0659e-03,  2.9728e-03, -8.9673e-03,  2.6876e-02,\n",
       "          -5.9626e-03,  2.1107e-02,  2.5294e-02, -8.9917e-03, -2.4228e-02,\n",
       "          -2.5805e-02,  2.6112e-02, -1.6986e-02, -1.4382e-02,  8.0345e-03,\n",
       "           1.3583e-02,  1.7647e-02, -1.8540e-02,  7.7788e-03,  5.6227e-03,\n",
       "          -6.5487e-03,  1.5244e-02, -1.8190e-02,  2.4524e-02, -7.7244e-03,\n",
       "           3.4534e-03,  1.5414e-02, -2.2347e-02,  1.3065e-02,  1.2165e-04,\n",
       "           2.6995e-02, -7.7770e-03,  1.2259e-03,  2.4434e-02,  1.4263e-02,\n",
       "           1.9146e-02,  2.1725e-02,  1.3499e-02, -2.6447e-02, -2.0513e-02,\n",
       "           9.1150e-03,  6.1110e-04, -7.9043e-03, -1.6920e-02, -2.6237e-02,\n",
       "           1.4669e-03,  2.9880e-03,  6.0334e-03,  1.3452e-02,  1.2556e-02,\n",
       "           2.7055e-02,  2.4706e-02,  7.3364e-05,  2.3744e-02, -7.2901e-03,\n",
       "           1.8563e-02,  7.1500e-03,  3.6653e-03,  9.1625e-03, -2.5750e-02,\n",
       "           6.9418e-04,  1.5264e-02, -9.8958e-03,  2.7519e-02, -2.6352e-02,\n",
       "           2.7780e-02,  1.6581e-02, -2.0780e-03,  1.1832e-02,  2.6062e-02,\n",
       "           1.5839e-02,  1.9551e-02,  1.0899e-02, -2.5590e-06, -2.2026e-02,\n",
       "          -2.6701e-02, -7.6589e-03, -4.3631e-03, -1.8619e-02, -6.9373e-03,\n",
       "           1.4605e-02, -7.3756e-04, -4.0849e-03,  5.5805e-03,  2.2401e-02,\n",
       "          -3.1031e-03, -1.9315e-02, -8.9489e-03,  7.5695e-03, -6.7001e-03,\n",
       "          -1.4834e-02,  1.9901e-02, -3.3613e-03, -1.9926e-02, -1.8852e-02,\n",
       "          -2.2528e-03,  2.3537e-02,  1.9916e-02,  7.7450e-03,  2.3223e-02,\n",
       "           2.3735e-02,  7.4587e-03, -1.6413e-02, -1.1534e-02,  1.4969e-02,\n",
       "          -8.3172e-03,  6.1042e-03,  1.6592e-02,  3.8308e-03,  2.7430e-02,\n",
       "           5.7530e-03,  1.7552e-03, -7.5635e-03,  1.8350e-02,  1.8492e-02,\n",
       "          -2.2506e-02, -1.1310e-02, -1.7652e-03,  2.2033e-02,  5.3436e-04,\n",
       "          -1.9568e-02, -8.2133e-03, -2.9716e-03,  8.1599e-03,  8.4008e-03,\n",
       "           2.0578e-02,  2.0706e-02,  9.3684e-03, -2.3423e-02,  1.0829e-02,\n",
       "           1.6354e-02,  2.0571e-03, -2.1353e-02,  2.4610e-02, -2.5676e-02,\n",
       "          -5.2898e-03,  6.4859e-03,  2.1545e-03, -9.0469e-03, -8.6524e-03,\n",
       "          -2.7215e-02,  1.0008e-02,  2.5960e-02,  8.3222e-03, -2.4618e-02,\n",
       "          -2.2232e-04,  7.8109e-03,  1.4762e-02,  3.8359e-03, -1.7500e-02,\n",
       "          -2.1086e-02, -2.3493e-04, -1.0973e-02, -7.0983e-04,  2.0622e-02,\n",
       "          -1.9662e-03, -7.5528e-03, -2.7372e-03, -1.4288e-02,  2.3850e-02,\n",
       "           1.2348e-02, -7.8055e-03,  2.8429e-03,  1.7205e-02, -5.8551e-03,\n",
       "           1.7695e-02, -1.0783e-02,  1.8052e-02, -1.9095e-03, -3.9896e-03,\n",
       "          -2.3964e-03, -1.6305e-02,  2.4045e-02, -1.6675e-02, -2.2155e-02,\n",
       "           1.7745e-02, -2.1711e-02,  1.4092e-02,  2.0663e-02,  9.5315e-03,\n",
       "           2.1403e-02, -1.8802e-02, -2.7350e-02,  2.9405e-03, -1.4842e-02,\n",
       "          -2.2076e-03,  2.2678e-02, -3.2754e-03, -1.2815e-02,  1.6188e-02,\n",
       "           2.2841e-02, -1.8781e-02,  2.3657e-02,  3.4447e-03,  5.4668e-03,\n",
       "          -1.0654e-02,  4.2166e-03, -1.9160e-02, -2.4065e-02,  1.5389e-02,\n",
       "          -1.9679e-02, -5.8164e-03, -3.8752e-03,  7.8683e-03,  1.8910e-02,\n",
       "          -9.4040e-03, -2.6015e-02,  1.4334e-02,  6.0071e-03,  2.8165e-03,\n",
       "           1.5882e-02, -4.9622e-03,  1.6146e-03, -2.5430e-02,  1.8531e-03,\n",
       "           1.4635e-02,  2.7802e-02,  2.1442e-02, -2.1041e-02,  1.9370e-02,\n",
       "           1.6976e-02, -9.0664e-03, -1.1658e-03, -1.1601e-02, -2.4284e-02,\n",
       "           1.9521e-02, -2.2501e-02,  8.1145e-03,  1.4700e-02,  2.3233e-02,\n",
       "           2.3268e-02,  9.4292e-03,  3.8736e-04, -8.9116e-03, -2.8896e-03,\n",
       "           1.7867e-02, -2.7541e-02, -1.2650e-02,  1.4421e-02, -5.8017e-03,\n",
       "           2.4761e-03,  9.4522e-03, -1.3692e-02, -6.2759e-04, -1.0242e-02,\n",
       "           2.4590e-02,  5.1985e-03,  1.2760e-02, -1.8104e-02,  4.3236e-03,\n",
       "          -8.0638e-03, -5.4810e-03, -5.5919e-03,  2.6710e-02, -1.4640e-02,\n",
       "           1.3366e-02, -5.5982e-03,  1.4777e-02, -1.3130e-02,  3.5361e-04,\n",
       "          -1.8947e-02,  1.2487e-02,  1.5374e-02,  1.5531e-02,  4.0492e-04,\n",
       "          -2.5455e-02,  1.3210e-02, -1.8695e-02,  1.7469e-02, -1.2310e-02,\n",
       "          -1.0274e-02, -4.0380e-03,  1.8366e-02,  2.1078e-02,  2.2657e-02,\n",
       "           2.5834e-02,  2.0839e-02, -6.6141e-03, -4.2836e-03, -1.9995e-02,\n",
       "          -3.1913e-03,  1.7924e-02,  7.7371e-03,  3.9038e-03,  1.0195e-02,\n",
       "          -1.8906e-02, -2.7068e-02, -3.1588e-03, -2.7789e-02, -2.3541e-02,\n",
       "           6.7172e-03,  2.3001e-02,  2.7896e-02,  2.7394e-02,  1.6384e-02,\n",
       "           1.7178e-02, -1.0648e-02,  2.1828e-02, -7.7098e-03,  6.3197e-03,\n",
       "           1.9794e-03,  3.4017e-03, -1.2199e-02,  1.7964e-02,  9.3290e-03,\n",
       "          -7.9142e-03, -3.7585e-03, -1.5831e-02, -2.5913e-02, -2.0511e-02,\n",
       "          -1.2553e-02, -5.2901e-03,  1.2782e-02, -2.2034e-02, -2.5450e-02,\n",
       "           2.4803e-02,  1.5011e-02, -2.6668e-02, -2.0054e-02, -1.5805e-02,\n",
       "          -2.4308e-02, -1.1040e-02, -5.5496e-04,  2.2982e-02,  2.1384e-02,\n",
       "           2.5319e-02, -5.3368e-03, -2.2044e-02,  2.0059e-02, -1.9238e-02,\n",
       "           2.7305e-02,  1.5508e-02, -7.6413e-03, -1.8705e-02, -1.6784e-02,\n",
       "          -1.8092e-02,  2.2880e-02,  9.0210e-03, -1.5961e-02, -2.1498e-02,\n",
       "           1.5802e-02, -4.3532e-03, -1.3255e-02,  1.0736e-02,  2.4713e-02,\n",
       "          -6.3758e-03,  2.4865e-02, -2.6059e-02,  1.2233e-02,  7.3426e-03,\n",
       "          -2.5615e-02,  5.5043e-03,  7.1634e-04,  1.9105e-02,  9.2734e-03,\n",
       "          -8.5905e-03,  1.0756e-02,  2.5806e-02, -1.4036e-02, -2.1852e-02,\n",
       "          -7.4300e-03, -1.1573e-02,  1.4811e-03,  1.6774e-02, -2.2106e-02,\n",
       "          -1.6154e-02,  1.6312e-02,  9.2093e-03,  1.1611e-02, -1.6580e-03,\n",
       "           1.7171e-02,  2.2814e-02,  3.6967e-03,  1.5466e-02, -1.3848e-02,\n",
       "          -2.7353e-02,  1.6497e-02, -9.6333e-03,  8.9152e-03,  2.4231e-02,\n",
       "           1.2645e-02, -2.4905e-02,  1.6434e-02,  1.5397e-02,  2.6473e-02,\n",
       "          -1.1273e-02, -2.1366e-02,  1.9583e-02,  1.5195e-02,  2.1309e-02,\n",
       "          -1.0696e-02, -3.2213e-03, -1.2993e-02, -2.5073e-02,  1.0377e-02,\n",
       "           1.0019e-02, -2.6029e-02,  2.6543e-02,  1.2321e-02,  1.7682e-02,\n",
       "          -8.5630e-03,  1.8960e-02,  1.5222e-03,  2.0035e-02,  2.1498e-02,\n",
       "           2.8729e-03, -1.0167e-02,  3.7319e-03, -2.1060e-02, -1.2249e-02,\n",
       "           1.5764e-02,  3.1140e-03,  1.3595e-02, -8.1343e-03,  1.9402e-02,\n",
       "          -2.2035e-02, -2.7397e-02,  2.6142e-02, -9.3013e-03, -1.8119e-02,\n",
       "          -1.8651e-02, -1.1709e-02,  1.2992e-03,  1.5463e-02, -1.0218e-02,\n",
       "           2.2147e-03,  1.9174e-02,  2.3904e-02, -1.2806e-02,  2.2370e-02,\n",
       "           2.4024e-02,  2.0546e-02,  1.0700e-03, -1.2523e-02, -2.3291e-02,\n",
       "           1.4755e-02, -2.1231e-02,  1.6606e-02, -1.7333e-02,  1.8258e-02,\n",
       "          -1.2184e-02,  1.5325e-03, -4.3915e-03,  1.4728e-02,  1.3456e-02,\n",
       "           2.6597e-02, -1.5774e-02, -1.6819e-02,  2.1329e-03, -2.5569e-02,\n",
       "           2.1113e-02,  2.3482e-02, -1.4550e-02, -3.7105e-03, -8.6068e-03,\n",
       "          -5.1403e-03,  2.7263e-02,  2.5850e-03,  1.0361e-02, -3.0997e-03,\n",
       "           2.0947e-02,  2.2589e-02,  1.5988e-02, -8.0755e-03,  2.2463e-02,\n",
       "          -1.5427e-02,  1.5597e-02, -2.6402e-04,  2.5859e-02, -1.1188e-03,\n",
       "          -1.7370e-02,  1.7448e-02, -7.7347e-03, -2.1121e-02,  2.3966e-02,\n",
       "           2.0680e-02,  1.5312e-02,  1.6745e-02, -6.9507e-04,  1.2426e-02,\n",
       "           4.2357e-03, -6.2781e-03,  2.1416e-02,  2.9966e-03,  1.3058e-02,\n",
       "          -2.3100e-02,  2.4454e-02,  4.1155e-03, -1.0922e-02,  2.0484e-02,\n",
       "          -2.0920e-03, -2.5039e-02, -2.4315e-02, -2.2435e-02,  5.2993e-03,\n",
       "           2.3133e-03, -1.4953e-02, -8.9896e-03,  2.4392e-02,  2.5440e-02,\n",
       "          -2.7502e-02, -1.6903e-02, -2.3823e-02,  2.1834e-02, -1.7242e-02,\n",
       "          -8.8711e-03, -1.9408e-02, -1.4430e-02,  1.2871e-02, -9.6855e-03,\n",
       "           2.0203e-02,  2.8018e-04, -1.8507e-02, -2.5426e-03, -2.0560e-02,\n",
       "          -4.1566e-03,  1.4687e-03, -9.3460e-03,  2.7573e-02,  1.9304e-02,\n",
       "          -1.6693e-02, -5.8152e-03, -1.5247e-02, -8.8376e-03, -2.3111e-02,\n",
       "           2.6973e-02, -9.8798e-03,  1.5030e-02, -1.7893e-02,  1.5390e-02,\n",
       "          -9.3157e-03, -1.1181e-02,  2.5597e-02, -8.9195e-03,  7.2529e-03,\n",
       "          -2.6674e-02, -2.5171e-02,  5.9913e-03,  1.8532e-02, -2.6881e-03,\n",
       "           2.0297e-02, -1.1729e-02, -2.0258e-02,  2.4686e-02,  2.7334e-02,\n",
       "          -1.4476e-02, -1.6567e-02,  1.8939e-02, -2.7091e-02,  9.7126e-03,\n",
       "          -1.0357e-02, -2.4775e-02, -6.8872e-03, -2.1005e-03,  1.2561e-02,\n",
       "          -7.0204e-03, -3.3905e-03,  1.7830e-02, -7.9353e-04,  3.8358e-03,\n",
       "           8.3693e-03,  2.5577e-02,  1.4208e-02,  2.4304e-02, -1.9626e-02,\n",
       "          -1.8719e-02, -1.9278e-02,  1.3956e-02, -1.2438e-02, -1.4556e-02,\n",
       "           2.0018e-03,  2.6104e-02, -7.9264e-03, -2.2907e-02, -1.9430e-02,\n",
       "          -2.6127e-02, -1.4936e-02,  2.5976e-02,  2.3172e-02,  3.4648e-03,\n",
       "          -1.3710e-02, -1.0395e-02, -1.9604e-02,  2.5489e-02, -1.1327e-02,\n",
       "          -2.9097e-03,  2.7494e-02,  1.1008e-02,  2.4095e-02, -2.1669e-02,\n",
       "          -1.6636e-02, -1.1261e-02, -5.0863e-03, -2.9474e-03,  9.4230e-03,\n",
       "          -8.4894e-03,  1.8852e-02, -8.5624e-03,  2.2092e-02, -1.5261e-02,\n",
       "           3.3186e-03,  2.2513e-02, -2.0121e-02, -1.5829e-02, -3.5314e-03,\n",
       "           5.2126e-03,  2.6673e-02,  1.7737e-02, -1.5820e-02, -2.2980e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0134,  0.0146,  0.0267,  ...,  0.0021,  0.0020, -0.0069],\n",
       "          [ 0.0102,  0.0041, -0.0258,  ..., -0.0212, -0.0128,  0.0328],\n",
       "          [-0.0084, -0.0250,  0.0354,  ..., -0.0288,  0.0046,  0.0290],\n",
       "          ...,\n",
       "          [ 0.0089, -0.0382, -0.0169,  ..., -0.0058, -0.0367, -0.0014],\n",
       "          [ 0.0251, -0.0273, -0.0365,  ..., -0.0219,  0.0113,  0.0130],\n",
       "          [-0.0109,  0.0278, -0.0318,  ..., -0.0360,  0.0039,  0.0264]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0295, -0.0346,  0.0340, -0.0234,  0.0119,  0.0052, -0.0041, -0.0176],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 1.2366e-03, -3.1179e-03, -1.0170e-02,  ..., -5.4392e-03,\n",
       "           -5.4182e-02, -2.1913e-02],\n",
       "          [-4.1713e-03,  2.7013e-03,  2.7388e-02,  ..., -1.0033e-02,\n",
       "            2.2780e-02, -2.2922e-02],\n",
       "          [ 2.4031e-03,  1.4379e-02,  8.9018e-03,  ...,  1.9578e-03,\n",
       "           -1.1979e-02, -3.0017e-05],\n",
       "          ...,\n",
       "          [ 9.7880e-03, -1.0965e-02, -1.7716e-03,  ...,  7.8162e-03,\n",
       "           -1.4943e-03,  1.3873e-02],\n",
       "          [-1.6836e-02,  6.8200e-03,  1.4288e-02,  ...,  2.1762e-03,\n",
       "           -4.1071e-03,  1.7381e-02],\n",
       "          [-7.7008e-03, -2.5758e-02,  1.1993e-02,  ..., -5.3281e-03,\n",
       "           -1.8664e-02, -1.3308e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.7795e-02, -6.1637e-03,  1.8261e-03,  ..., -3.5005e-03,\n",
       "           -2.2545e-02,  1.2914e-04],\n",
       "          [-2.7368e-03, -1.7986e-02,  6.2770e-03,  ..., -5.2564e-03,\n",
       "            1.7151e-02, -1.4938e-03],\n",
       "          [-1.7424e-02, -1.3925e-02, -1.0990e-02,  ..., -1.4559e-02,\n",
       "            1.9782e-02, -1.8300e-02],\n",
       "          ...,\n",
       "          [ 8.5556e-03,  6.8188e-03,  7.1340e-03,  ..., -1.3880e-02,\n",
       "           -3.0164e-03, -1.6839e-02],\n",
       "          [ 8.9137e-03,  9.9250e-03,  9.7306e-05,  ..., -1.4922e-02,\n",
       "           -1.2197e-02,  1.9512e-02],\n",
       "          [-6.5119e-03,  1.3324e-03,  1.3985e-02,  ...,  2.6053e-02,\n",
       "           -1.8836e-02,  8.9717e-03]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0182, -0.0094,  0.0077,  ...,  0.0695, -0.0252, -0.0027],\n",
       "          [-0.0123,  0.0307,  0.0162,  ...,  0.0111,  0.0061,  0.0258],\n",
       "          [ 0.0132,  0.0083, -0.0111,  ...,  0.0285, -0.0115, -0.0055],\n",
       "          ...,\n",
       "          [-0.0032, -0.0093, -0.0154,  ...,  0.0293,  0.0201,  0.0110],\n",
       "          [ 0.0101,  0.0005,  0.0342,  ...,  0.0017, -0.0076, -0.0079],\n",
       "          [-0.0104,  0.0105, -0.0118,  ...,  0.0083, -0.0159, -0.0587]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0077, -0.0100,  0.0076,  ..., -0.0088,  0.0158,  0.0175],\n",
       "          [-0.0200,  0.0048, -0.0285,  ..., -0.0031, -0.0047,  0.0050],\n",
       "          [-0.0323,  0.0106, -0.0176,  ...,  0.0019,  0.0078,  0.0003],\n",
       "          ...,\n",
       "          [-0.0064, -0.0121, -0.0040,  ..., -0.0196,  0.0231, -0.0044],\n",
       "          [-0.0383,  0.0099, -0.0101,  ...,  0.0260,  0.0006,  0.0161],\n",
       "          [ 0.0066,  0.0047, -0.0085,  ..., -0.0067,  0.0208, -0.0031]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0044,  0.0176,  0.0213,  ...,  0.0187,  0.0099, -0.0146],\n",
       "          [ 0.0162, -0.0149,  0.0230,  ..., -0.0059, -0.0147,  0.0026],\n",
       "          [-0.0202, -0.0059,  0.0249,  ...,  0.0242, -0.0172,  0.0094],\n",
       "          ...,\n",
       "          [-0.0154,  0.0128, -0.0025,  ..., -0.0167, -0.0241, -0.0082],\n",
       "          [ 0.0220, -0.0059, -0.0091,  ...,  0.0096, -0.0155, -0.0229],\n",
       "          [ 0.0171,  0.0194,  0.0105,  ...,  0.0011, -0.0269, -0.0035]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 2.5246e-02,  2.5835e-03,  2.7519e-02, -6.3488e-03, -2.7764e-02,\n",
       "          -5.3211e-03, -2.7822e-02,  1.7830e-02,  8.4163e-03,  4.8212e-03,\n",
       "           1.1084e-02, -2.2280e-02, -1.5993e-02,  4.3611e-03, -1.1098e-02,\n",
       "          -8.8115e-03, -1.6672e-04,  6.3120e-03,  9.1440e-03,  1.4846e-02,\n",
       "          -8.5824e-03,  2.3377e-02,  9.6224e-03,  2.1282e-02, -1.4659e-02,\n",
       "          -1.9412e-02,  2.2048e-02,  2.0204e-02,  1.1492e-02,  1.9206e-02,\n",
       "          -1.1822e-02, -4.0314e-03, -1.5333e-02,  1.4591e-03,  2.2624e-02,\n",
       "           2.2583e-02,  2.4470e-02,  2.7091e-02,  2.5333e-02, -1.6765e-02,\n",
       "          -2.7790e-02,  2.4063e-03, -2.3824e-02,  1.5274e-02, -1.3424e-02,\n",
       "          -2.2631e-02,  2.2477e-02,  2.6117e-02,  1.4442e-03, -2.7011e-02,\n",
       "          -2.3216e-02, -1.2149e-02, -1.2129e-02,  7.5111e-03, -1.9503e-02,\n",
       "          -2.4032e-02, -2.1156e-02,  2.3567e-02,  2.4720e-02,  2.1646e-02,\n",
       "           1.2561e-02, -2.7167e-02,  1.8894e-02,  3.8459e-03,  5.8597e-03,\n",
       "           6.0093e-03, -1.6652e-03,  2.6171e-02,  2.3598e-02, -1.4268e-02,\n",
       "          -9.4406e-03,  5.3695e-03,  1.7647e-02,  2.5447e-02, -9.8049e-03,\n",
       "           1.7043e-02,  1.4799e-02,  1.1684e-02,  8.4286e-03, -1.9919e-02,\n",
       "           2.0457e-03,  3.2001e-03,  2.1252e-02, -1.0535e-02,  2.7656e-02,\n",
       "          -1.5900e-02,  1.9256e-03, -4.4908e-03,  2.6557e-02,  1.0956e-02,\n",
       "          -3.9598e-04, -2.0102e-02, -1.6190e-05, -4.8549e-03, -2.1466e-02,\n",
       "           1.0252e-02,  9.0643e-03, -1.0899e-02, -7.7364e-03, -2.1113e-02,\n",
       "          -2.1210e-02, -8.0501e-04,  1.3890e-02,  2.3761e-02,  2.4276e-02,\n",
       "           6.6378e-03, -2.6206e-02, -2.0210e-03,  2.4329e-02,  1.4050e-02,\n",
       "           2.2636e-02, -1.5565e-02, -1.7166e-02,  3.4552e-03, -2.1882e-02,\n",
       "          -8.9034e-03, -7.4520e-03, -2.2950e-02,  1.6043e-02,  1.3602e-02,\n",
       "           1.0854e-03, -2.1875e-02, -1.9051e-03, -1.7872e-02, -8.6360e-03,\n",
       "          -2.2457e-02,  9.3927e-04, -2.6958e-02, -7.0936e-03,  1.4054e-02,\n",
       "           3.5015e-03, -7.6262e-03,  1.8082e-02, -2.5570e-02, -6.9316e-03,\n",
       "          -2.4854e-02, -5.8665e-03,  2.3874e-02, -1.2074e-02,  3.0537e-03,\n",
       "           5.4666e-04,  2.1546e-02, -2.1035e-02, -2.3030e-02,  1.4141e-02,\n",
       "          -2.1647e-02, -2.3104e-02, -5.4116e-03, -9.0456e-03, -4.0567e-03,\n",
       "          -2.4226e-02,  2.4431e-02, -1.1946e-02,  1.5073e-03,  1.8036e-02,\n",
       "          -1.1799e-02, -2.5610e-02, -2.2831e-03,  2.3549e-02,  2.6765e-02,\n",
       "           9.6999e-03,  1.4019e-02, -2.3761e-02,  1.5068e-04, -2.6914e-02,\n",
       "          -1.5840e-02,  2.2771e-02,  5.3742e-03, -8.6038e-03,  8.8189e-03,\n",
       "           2.2469e-02,  2.4389e-02, -2.7010e-02,  1.6002e-02,  7.5093e-03,\n",
       "          -2.2268e-02,  2.0903e-02,  2.2946e-03, -1.1917e-02,  8.4352e-03,\n",
       "           1.9583e-02,  1.5765e-02,  2.0033e-02,  2.6196e-02, -1.2573e-02,\n",
       "           2.4311e-02, -7.4327e-03, -5.3332e-03,  1.8740e-02,  1.8842e-02,\n",
       "           2.4145e-03, -5.3657e-03,  1.1329e-02,  1.5494e-02, -2.1672e-02,\n",
       "           1.1512e-02,  1.0572e-02, -2.1186e-02, -8.9969e-03,  2.4978e-03,\n",
       "          -2.2434e-02, -4.7054e-03,  1.4230e-02, -2.7338e-03, -9.5848e-03,\n",
       "           1.2952e-02, -2.4269e-02, -2.6427e-02, -1.4099e-03, -2.4615e-02,\n",
       "           6.2090e-03, -2.5554e-03,  1.4301e-02, -2.7861e-03, -2.4035e-02,\n",
       "          -1.1431e-02, -2.0762e-02, -2.5484e-02,  2.2220e-02, -8.8096e-03,\n",
       "           1.2772e-02, -1.3183e-02,  5.3981e-04, -4.7335e-03,  5.5241e-03,\n",
       "          -1.8455e-02,  2.6838e-02, -2.4894e-02,  9.7353e-03, -2.4336e-02,\n",
       "           2.5286e-02,  6.7342e-03, -2.3049e-02, -7.0269e-04,  1.4576e-02,\n",
       "          -5.3377e-03,  1.7211e-02,  1.8463e-02,  1.8197e-02, -1.1058e-02,\n",
       "           2.0629e-02, -1.7744e-02,  7.4361e-03, -1.4904e-02,  1.6104e-02,\n",
       "           2.3454e-02,  1.3597e-02, -7.6993e-03,  2.6833e-03,  9.1567e-03,\n",
       "          -1.2989e-02, -1.9412e-02, -8.2171e-03,  1.3873e-02, -1.3594e-02,\n",
       "          -1.0042e-02, -8.6556e-03, -1.2296e-02,  4.4798e-03,  1.1502e-02,\n",
       "          -6.3779e-03, -1.5216e-02,  1.8762e-02,  2.1925e-02, -1.4074e-02,\n",
       "          -1.8680e-02, -1.4042e-03, -4.9274e-03, -1.6606e-02,  1.2209e-02,\n",
       "          -1.0294e-02,  1.7117e-02, -2.4635e-02, -5.9950e-04,  1.0771e-02,\n",
       "          -1.8672e-02, -2.1558e-02, -1.9698e-02,  2.5815e-02, -1.5878e-02,\n",
       "          -2.2506e-02, -7.9540e-03, -1.9401e-03,  7.7837e-03,  8.2809e-03,\n",
       "          -1.0518e-02, -1.7901e-02,  4.0968e-03, -4.4837e-03, -1.6495e-02,\n",
       "           8.0517e-03,  2.2542e-02,  1.0081e-02, -2.6096e-02, -3.5193e-03,\n",
       "           2.7352e-02, -2.5150e-02,  8.4461e-03, -2.2118e-02, -2.4134e-02,\n",
       "           2.3479e-03,  1.1284e-02, -8.3998e-03,  2.7653e-02, -4.7134e-03,\n",
       "          -2.0543e-03, -1.0069e-02, -2.6006e-02,  2.1176e-02, -2.5222e-02,\n",
       "           1.3394e-02, -1.2168e-02,  2.5765e-03, -1.0612e-02, -1.1409e-02,\n",
       "          -2.4616e-02, -9.4454e-03,  2.0954e-03,  1.6950e-02,  6.5930e-03,\n",
       "           4.5872e-03, -1.8256e-02, -2.6243e-02,  1.3960e-02,  1.7139e-02,\n",
       "           1.7224e-02, -8.8697e-03,  1.9678e-03, -8.5417e-03, -2.3201e-02,\n",
       "           1.9131e-02,  2.0435e-02, -8.8825e-03, -1.1534e-02,  1.4252e-02,\n",
       "          -1.2917e-02, -2.1556e-02, -7.1744e-03,  2.4457e-02, -3.9952e-03,\n",
       "           1.5337e-02, -4.1697e-03,  9.1978e-03, -7.9260e-03, -2.3606e-03,\n",
       "           4.9880e-03, -1.4962e-02,  2.3018e-03, -1.8385e-02, -2.3225e-02,\n",
       "          -1.2896e-02,  1.1083e-02, -1.7723e-02,  4.7769e-03, -1.8365e-02,\n",
       "          -1.6290e-02, -3.9818e-03,  1.3346e-03, -1.5295e-02, -7.4712e-03,\n",
       "           6.8196e-04,  1.5872e-02,  2.6758e-02, -1.7442e-02,  2.3063e-02,\n",
       "          -2.4737e-03,  1.5453e-02, -1.5792e-02,  1.4774e-02,  6.3954e-04,\n",
       "          -7.3607e-03,  4.8270e-03, -2.3675e-02, -7.8757e-03,  2.1146e-03,\n",
       "          -4.2728e-03,  1.4724e-02, -2.4494e-02, -1.4074e-02, -2.6568e-02,\n",
       "          -1.2615e-02,  2.5783e-02,  1.1777e-02, -1.7884e-02,  1.8958e-02,\n",
       "          -1.2744e-02, -2.6888e-02,  6.0491e-03, -2.0596e-02, -2.6387e-02,\n",
       "          -1.5344e-02,  1.3801e-02, -2.3244e-02,  2.6474e-02,  1.2387e-02,\n",
       "           2.0320e-03, -1.0097e-02, -2.6336e-03, -2.1013e-02, -2.1216e-02,\n",
       "           1.7619e-02,  2.6976e-02,  4.4743e-03,  1.3504e-02,  2.0303e-02,\n",
       "          -5.0621e-03,  3.3678e-03, -2.6680e-02, -2.0220e-04, -2.6585e-02,\n",
       "          -1.4332e-02,  2.2810e-02, -2.8560e-03, -1.6144e-03, -2.6954e-02,\n",
       "          -2.1441e-02,  1.0047e-02,  6.2135e-03,  2.5029e-02,  2.0262e-02,\n",
       "           3.6164e-03, -2.4871e-02, -8.9678e-03, -2.2270e-02, -1.0783e-02,\n",
       "          -2.0126e-02,  1.2473e-02, -6.6391e-03,  1.4172e-02,  2.7563e-03,\n",
       "           1.2527e-02, -4.0493e-03,  1.5051e-02, -2.3621e-02,  1.3943e-02,\n",
       "           1.8880e-02,  1.2115e-02,  2.5794e-03,  2.0790e-02,  1.1032e-02,\n",
       "           9.7922e-03, -6.3279e-04, -1.6009e-02, -1.5863e-02, -2.3968e-03,\n",
       "          -9.5554e-03, -2.2026e-02, -1.6745e-02, -2.5859e-02,  1.6804e-02,\n",
       "           6.3067e-03, -1.8833e-03,  1.1739e-02,  1.7468e-02,  2.6564e-02,\n",
       "          -1.0230e-02,  2.1587e-02, -6.5683e-03, -2.2610e-02,  3.9029e-03,\n",
       "           6.6624e-03, -2.4152e-02,  2.5917e-02,  7.2974e-04,  1.8292e-02,\n",
       "          -2.2132e-03, -2.2494e-02, -7.6203e-03, -2.6623e-02,  8.1824e-03,\n",
       "          -1.7898e-02, -2.1512e-02,  2.7223e-02, -1.7515e-02, -2.5989e-02,\n",
       "           2.4048e-02,  2.3027e-02, -7.5806e-03, -8.4944e-03,  2.5349e-02,\n",
       "          -8.0395e-03,  1.8965e-02, -1.1954e-02, -5.8968e-04,  2.5703e-02,\n",
       "           7.0054e-03,  5.5526e-03,  2.5015e-02, -2.5775e-02,  2.2922e-02,\n",
       "          -1.4314e-03, -2.9859e-03,  2.7130e-02,  1.0099e-02,  6.1730e-04,\n",
       "          -1.7411e-02, -2.1989e-02,  1.9162e-02,  1.5090e-02, -2.1623e-02,\n",
       "           2.2225e-02,  2.5871e-02, -1.9042e-02,  7.8119e-04,  1.0334e-03,\n",
       "           1.0878e-02, -1.8298e-02,  2.0358e-02, -1.6520e-02,  6.5264e-03,\n",
       "           1.3630e-02, -1.2660e-03,  1.4452e-03, -1.2635e-02,  1.3186e-02,\n",
       "          -3.0732e-03, -1.9006e-02,  1.9540e-02,  5.7440e-03,  7.6962e-03,\n",
       "           2.3005e-02,  8.1226e-03, -5.9029e-03, -8.2010e-03,  2.5043e-02,\n",
       "          -5.1989e-04, -1.3918e-02,  1.3836e-02, -2.1453e-02,  1.0969e-02,\n",
       "          -1.1512e-02, -1.4203e-02, -1.9943e-02, -2.0012e-02,  6.1085e-03,\n",
       "          -7.8569e-03, -2.7224e-02, -3.1170e-03,  2.6021e-02,  2.2034e-02,\n",
       "          -5.3125e-04, -1.9678e-02, -1.1558e-02,  1.4889e-02,  1.6559e-02,\n",
       "          -1.9919e-02, -9.3119e-04, -2.3364e-02,  1.7119e-02, -1.9934e-03,\n",
       "           2.6362e-03,  2.1637e-04,  1.7035e-03, -8.0885e-03,  1.4873e-02,\n",
       "           3.5105e-03,  6.2595e-04,  7.8749e-03,  1.8101e-02, -2.3070e-02,\n",
       "          -1.6866e-02,  9.0659e-03, -2.1545e-02, -2.3764e-03,  2.5948e-02,\n",
       "          -1.0182e-02,  2.7702e-02,  1.0803e-02,  2.5979e-02, -9.3761e-03,\n",
       "          -2.2245e-02, -5.0094e-03, -2.0068e-02,  3.1720e-03, -1.8240e-03,\n",
       "           2.6553e-03,  3.5751e-03,  1.7007e-02,  2.7637e-02, -2.7178e-02,\n",
       "          -2.7760e-02,  2.2359e-02,  1.2674e-02, -1.3100e-02, -4.6631e-03,\n",
       "           1.0319e-02, -2.6866e-02,  2.0691e-02,  7.7378e-03, -1.9068e-02,\n",
       "          -5.0231e-03, -1.4165e-02, -1.6842e-02,  2.5844e-02, -1.8593e-02,\n",
       "          -9.7830e-03, -2.3201e-02,  7.6791e-03, -2.7614e-03,  2.0952e-02,\n",
       "          -1.8537e-02, -3.1117e-03,  1.0169e-02,  2.1205e-02, -1.9666e-02,\n",
       "           5.3344e-03,  2.2492e-02,  7.1929e-03,  1.4758e-02,  1.4342e-02,\n",
       "           2.2258e-02,  5.3516e-03, -1.6570e-02, -2.9092e-03,  2.1504e-02,\n",
       "           1.0822e-02, -6.0515e-03, -6.6809e-03, -6.3871e-03, -1.8910e-02,\n",
       "           1.7780e-02,  6.1335e-03,  1.4528e-02,  2.1353e-02, -1.7395e-02,\n",
       "           5.6132e-03, -2.6258e-02, -4.7798e-05,  4.9562e-03,  5.4478e-04,\n",
       "           2.6542e-02, -3.3361e-03,  9.9320e-03,  6.1402e-03, -2.5875e-02,\n",
       "          -1.0867e-02,  3.1067e-03,  2.7358e-02,  1.1084e-02, -7.0581e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0257, -0.0216, -0.0182,  ...,  0.0335,  0.0060,  0.0204],\n",
       "          [-0.0273, -0.0206,  0.0061,  ..., -0.0368, -0.0149,  0.0244],\n",
       "          [ 0.0297,  0.0232, -0.0113,  ..., -0.0327,  0.0123,  0.0087],\n",
       "          ...,\n",
       "          [ 0.0009, -0.0068,  0.0343,  ..., -0.0197, -0.0050,  0.0242],\n",
       "          [ 0.0057, -0.0344,  0.0209,  ..., -0.0017,  0.0027, -0.0267],\n",
       "          [ 0.0062, -0.0200, -0.0032,  ...,  0.0067, -0.0217,  0.0022]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0039, -0.0026, -0.0083,  0.0002, -0.0304,  0.0058,  0.0161,  0.0142],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0051,  0.0233,  0.0095,  ..., -0.0092,  0.0039,  0.0192],\n",
       "          [-0.0021,  0.0106,  0.0447,  ...,  0.0071, -0.0041, -0.0267],\n",
       "          [ 0.0230, -0.0093,  0.0096,  ...,  0.0226, -0.0177,  0.0264],\n",
       "          ...,\n",
       "          [ 0.0100, -0.0260, -0.0072,  ...,  0.0120, -0.0124, -0.0074],\n",
       "          [-0.0195, -0.0086, -0.0170,  ...,  0.0019,  0.0127,  0.0098],\n",
       "          [ 0.0051,  0.0028, -0.0196,  ..., -0.0306, -0.0035,  0.0141]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 9.6616e-03,  1.7311e-02, -3.2486e-02,  ...,  2.5326e-03,\n",
       "           -2.7502e-02,  2.8603e-03],\n",
       "          [ 5.2871e-03, -1.5922e-02, -1.1031e-02,  ..., -2.8934e-03,\n",
       "            6.7846e-05,  3.8243e-03],\n",
       "          [ 1.2341e-02,  7.9152e-03, -1.0251e-02,  ..., -9.0170e-03,\n",
       "            2.6998e-03, -2.8549e-03],\n",
       "          ...,\n",
       "          [-1.9532e-02,  1.2851e-03, -6.4372e-03,  ..., -9.1722e-03,\n",
       "           -1.4023e-02, -1.7720e-02],\n",
       "          [-1.8014e-03, -2.7778e-02,  1.5837e-02,  ...,  6.3870e-03,\n",
       "            2.6733e-02,  3.9209e-03],\n",
       "          [ 5.6012e-03,  1.4825e-02,  1.3438e-02,  ...,  2.4218e-03,\n",
       "            1.1648e-02,  2.0304e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0110,  0.0111, -0.0207,  ...,  0.0192, -0.0049, -0.0105],\n",
       "          [-0.0016, -0.0053, -0.0224,  ...,  0.0020, -0.0012,  0.0090],\n",
       "          [-0.0003,  0.0044,  0.0073,  ..., -0.0305, -0.0017,  0.0194],\n",
       "          ...,\n",
       "          [-0.0199, -0.0069, -0.0288,  ..., -0.0040,  0.0078,  0.0197],\n",
       "          [ 0.0081, -0.0006, -0.0004,  ..., -0.0031, -0.0037, -0.0128],\n",
       "          [ 0.0144, -0.0104,  0.0057,  ..., -0.0011, -0.0033, -0.0005]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0058,  0.0012,  0.0135,  ..., -0.0126, -0.0135,  0.0135],\n",
       "          [ 0.0066,  0.0109, -0.0353,  ...,  0.0004,  0.0205, -0.0015],\n",
       "          [ 0.0053,  0.0133, -0.0087,  ...,  0.0060,  0.0233,  0.0219],\n",
       "          ...,\n",
       "          [ 0.0195, -0.0006, -0.0227,  ..., -0.0134, -0.0067, -0.0306],\n",
       "          [-0.0038, -0.0303,  0.0184,  ..., -0.0040,  0.0057,  0.0132],\n",
       "          [-0.0049, -0.0140, -0.0042,  ..., -0.0018,  0.0150, -0.0063]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.3283e-02, -1.2444e-02,  2.5585e-02,  ...,  4.1890e-03,\n",
       "            1.5720e-02,  4.6714e-03],\n",
       "          [ 6.7406e-03,  1.2366e-02, -1.1850e-02,  ...,  1.2299e-02,\n",
       "            1.4880e-02, -2.4525e-04],\n",
       "          [ 7.7872e-03, -1.6100e-02,  1.4941e-02,  ..., -4.9311e-03,\n",
       "            1.5176e-02, -5.9537e-04],\n",
       "          ...,\n",
       "          [-2.7499e-02,  1.2505e-03,  2.2054e-02,  ...,  7.4647e-05,\n",
       "            1.7842e-02,  4.7710e-03],\n",
       "          [-1.7448e-02,  2.1055e-02, -2.7369e-02,  ..., -7.3491e-03,\n",
       "           -1.4131e-02, -2.4160e-02],\n",
       "          [-2.4044e-02, -1.5821e-02, -1.5294e-02,  ...,  2.5148e-02,\n",
       "           -1.0036e-02, -2.2538e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.1581e-02, -2.2121e-02,  6.1839e-03,  1.2755e-02, -2.0037e-03,\n",
       "          -2.8137e-03,  2.1354e-02,  1.7811e-02,  7.3524e-03, -1.9830e-02,\n",
       "          -9.1266e-03,  2.5590e-02,  2.6334e-02,  1.0980e-02,  2.6645e-02,\n",
       "          -2.4522e-02, -1.5964e-02, -3.6935e-03,  1.5431e-02, -1.0137e-02,\n",
       "          -8.5815e-03, -1.6053e-02,  2.3170e-02, -2.1342e-02, -1.2652e-02,\n",
       "          -2.6793e-02,  1.7735e-02,  5.4115e-03, -1.4212e-02, -2.0258e-02,\n",
       "          -1.9088e-03,  1.9734e-02,  1.5608e-02,  1.2043e-02,  1.5697e-03,\n",
       "           2.6411e-02, -1.3309e-02, -1.6167e-02, -9.0593e-03,  8.7316e-03,\n",
       "           3.5611e-03, -1.2191e-02,  1.1833e-02,  3.8828e-03,  2.7452e-02,\n",
       "           1.4229e-02, -3.9448e-03, -6.6894e-03,  1.2712e-02, -1.3653e-02,\n",
       "          -6.0564e-03,  8.8710e-03, -3.5210e-03,  1.9983e-02,  5.3492e-03,\n",
       "          -1.3984e-02,  1.6028e-02, -3.2880e-03, -1.1569e-02, -3.4498e-03,\n",
       "           1.1348e-02, -2.2820e-02,  1.6734e-03, -2.1075e-02,  1.2808e-02,\n",
       "          -1.9784e-02,  1.3245e-02, -2.3858e-02,  6.8106e-03, -6.8924e-03,\n",
       "           2.0745e-02,  1.8916e-03,  3.1122e-03,  1.1248e-02, -1.3175e-02,\n",
       "          -2.3045e-02,  2.6087e-02,  2.0449e-02, -2.0760e-02,  1.5667e-02,\n",
       "          -8.9514e-03,  2.6218e-02, -7.0047e-03, -1.2981e-02,  1.8218e-02,\n",
       "           2.6309e-02, -1.8103e-03, -1.1329e-02,  1.9768e-02,  2.5879e-02,\n",
       "           8.2924e-03,  2.5092e-03, -2.4918e-02, -1.8125e-02, -1.3582e-02,\n",
       "           1.0013e-02, -2.1380e-02, -1.7640e-02,  1.9239e-02,  2.6516e-02,\n",
       "          -2.5726e-02,  1.5136e-02,  1.0665e-02, -1.7975e-02, -1.9456e-02,\n",
       "          -2.4925e-02,  1.6993e-02,  2.4727e-02, -1.9886e-02,  7.6314e-03,\n",
       "           4.3753e-03,  2.6203e-02,  2.0040e-02, -3.1251e-03,  1.3412e-02,\n",
       "           2.4310e-02, -2.1420e-02,  7.8375e-03,  2.1314e-03, -8.8447e-03,\n",
       "          -3.6743e-03, -2.6623e-03, -9.6624e-03, -1.1714e-02,  2.7467e-02,\n",
       "          -7.2341e-03,  1.0368e-02, -2.0972e-02, -2.0752e-02,  1.7532e-02,\n",
       "          -2.5373e-02,  1.6841e-02, -2.3350e-03, -1.6245e-02, -2.2462e-02,\n",
       "           1.3936e-02, -1.6989e-02, -3.1578e-03, -1.2104e-02,  1.0695e-02,\n",
       "          -3.3941e-03,  2.5299e-02, -2.2083e-03,  1.8332e-02, -3.9473e-04,\n",
       "           3.4376e-03,  1.4230e-02, -2.5035e-02, -1.4149e-02,  1.7520e-02,\n",
       "          -5.7385e-03,  9.5096e-03, -4.4559e-03,  2.5305e-02, -1.6060e-02,\n",
       "           2.5457e-02,  2.0382e-02, -6.9276e-05,  2.5027e-03, -7.6448e-03,\n",
       "          -1.7589e-02, -2.5156e-02, -1.0096e-02,  1.7459e-03, -2.1767e-02,\n",
       "           3.0455e-03,  1.0495e-02, -2.0199e-02,  2.3299e-02, -1.8296e-02,\n",
       "          -9.8027e-03, -1.6148e-02,  2.1909e-02, -1.0154e-02,  2.1497e-02,\n",
       "          -1.9443e-02, -4.7300e-03, -2.1408e-02, -1.3025e-03,  1.0129e-02,\n",
       "          -2.7854e-02, -3.7100e-03,  6.5885e-03, -1.5786e-02, -1.9221e-02,\n",
       "           2.1773e-02,  4.7093e-03, -1.8615e-03, -1.6343e-02, -2.0255e-02,\n",
       "           2.5601e-02,  2.0720e-02,  2.1661e-02,  6.1044e-03,  6.9479e-03,\n",
       "           1.1675e-02, -3.3915e-03, -2.1099e-02,  9.2629e-03,  2.1958e-02,\n",
       "          -2.0364e-03,  1.6800e-02,  3.0735e-03, -1.5440e-02, -1.1969e-02,\n",
       "           2.0452e-03, -2.2225e-02, -1.5725e-02,  1.4574e-02,  3.9378e-04,\n",
       "          -2.1999e-02,  2.4966e-02,  4.3066e-03, -1.1615e-02, -8.7253e-03,\n",
       "          -1.9297e-03,  8.9616e-03,  2.3396e-02, -4.8012e-03,  1.7851e-02,\n",
       "          -7.9896e-03,  2.1504e-02, -2.7162e-03,  1.8839e-02,  1.0973e-02,\n",
       "           7.1101e-03,  1.3546e-02,  1.4945e-02, -2.4779e-03,  8.0127e-03,\n",
       "           2.3524e-02,  1.6343e-02, -1.8682e-02, -9.2994e-03,  5.2023e-03,\n",
       "           7.0251e-03,  5.1320e-03, -2.4627e-02,  2.3936e-02, -1.3249e-02,\n",
       "           2.1029e-02, -2.1248e-02,  1.5376e-02, -1.7059e-02, -1.7843e-02,\n",
       "          -2.1980e-02, -9.5041e-03, -2.7859e-02, -1.0612e-02, -8.7690e-03,\n",
       "           1.2148e-02, -1.4843e-03,  1.1696e-02, -2.8353e-03, -1.4026e-02,\n",
       "          -1.0533e-02, -1.4151e-02,  1.3934e-02,  7.2655e-03,  1.4430e-02,\n",
       "           2.5685e-02,  1.4378e-02,  2.0449e-03,  1.4265e-02,  1.0413e-02,\n",
       "           1.0934e-02,  8.4148e-03,  2.6681e-03, -1.6934e-02,  1.2353e-02,\n",
       "           2.4310e-02, -2.1607e-02, -5.0887e-03,  2.7588e-02,  7.0631e-03,\n",
       "          -1.1654e-02, -9.7754e-03, -6.6648e-03, -9.7281e-03,  1.4098e-02,\n",
       "           1.2996e-02,  8.4931e-03, -1.5403e-02, -4.8595e-03, -1.2675e-02,\n",
       "           2.6502e-03, -2.0940e-02, -1.2117e-02,  1.5371e-02, -1.2161e-02,\n",
       "          -1.5622e-02,  3.1866e-03, -2.1763e-02, -1.3705e-02,  2.3817e-02,\n",
       "           2.3955e-02,  1.1948e-02,  1.4011e-02, -1.2301e-02, -1.0328e-02,\n",
       "          -2.1104e-02, -5.1795e-03,  1.7426e-02, -1.0692e-02, -1.3919e-02,\n",
       "          -4.2807e-03, -5.3851e-03, -1.2677e-02, -2.1154e-03, -1.0902e-02,\n",
       "           1.7725e-02, -7.5649e-03, -1.7595e-02,  1.3527e-02, -2.6627e-02,\n",
       "          -3.2113e-03, -2.3629e-02,  5.2452e-03,  6.7800e-03,  1.4397e-02,\n",
       "           5.4391e-03, -2.3661e-02, -2.7770e-03,  2.7659e-02, -1.5189e-02,\n",
       "           1.6421e-02, -2.5355e-02, -2.8645e-03,  1.9066e-02, -1.0339e-02,\n",
       "           1.1505e-02, -4.2323e-03, -2.5610e-02, -1.0363e-02,  5.0644e-03,\n",
       "           9.7874e-03,  1.8806e-02,  2.2577e-02,  1.4010e-02,  6.2356e-03,\n",
       "           1.5579e-02,  2.8629e-03,  2.0916e-02,  5.9955e-03,  8.6198e-03,\n",
       "          -1.6214e-02,  8.8280e-03, -2.4264e-02, -2.5668e-02,  1.6609e-02,\n",
       "          -8.4051e-03, -7.7359e-03,  1.7597e-02,  9.5422e-03, -1.5606e-02,\n",
       "          -5.0034e-03, -2.5977e-02,  1.2948e-02, -4.3860e-03, -1.8573e-02,\n",
       "           1.4644e-02, -8.2259e-04,  3.0118e-03,  2.7364e-02,  3.6286e-03,\n",
       "          -2.2270e-02, -4.0942e-03,  1.0869e-02, -2.4914e-02,  2.3017e-02,\n",
       "          -1.5064e-02, -1.7928e-02,  2.2267e-02,  1.5153e-02, -3.9021e-03,\n",
       "          -1.5246e-02, -1.8378e-02, -2.2904e-03,  1.9582e-02,  8.3837e-04,\n",
       "           2.4535e-02, -1.9817e-03,  1.5710e-02, -1.4300e-02, -1.8769e-03,\n",
       "           2.1818e-02,  2.2623e-02,  6.1073e-03, -2.0494e-02, -4.4811e-03,\n",
       "          -2.1188e-02,  1.0599e-02, -2.0166e-02,  8.1000e-03,  1.0357e-02,\n",
       "           1.4648e-02, -4.6298e-03, -5.7910e-04, -1.2189e-02,  1.2815e-02,\n",
       "           2.1414e-02, -1.8866e-02, -1.9821e-02,  1.9086e-02,  1.6030e-02,\n",
       "          -1.1029e-02, -2.1657e-02, -2.2915e-02, -2.6566e-02, -2.6673e-02,\n",
       "          -1.8583e-02,  7.6437e-03, -8.5307e-03,  5.8746e-03,  2.1022e-02,\n",
       "           1.0263e-02,  1.9712e-02,  2.7296e-02,  2.6137e-03, -8.5562e-03,\n",
       "           2.6735e-03, -2.0385e-02, -1.6674e-02,  1.0212e-02,  6.1670e-03,\n",
       "          -5.2738e-03, -6.8919e-03,  2.3396e-02,  8.0996e-03, -9.7703e-03,\n",
       "           2.3748e-02,  1.4470e-02, -1.7408e-03,  2.5603e-02, -1.3415e-02,\n",
       "           1.7086e-02,  1.1423e-02,  4.0685e-03, -1.9038e-02,  2.0847e-02,\n",
       "          -2.6843e-02, -1.1695e-02, -2.3515e-02, -4.5327e-03, -7.6307e-03,\n",
       "           1.3082e-02,  2.1922e-02,  2.1290e-02,  1.0964e-02, -1.3788e-02,\n",
       "           6.1248e-03,  1.9512e-02,  2.5926e-02, -1.3773e-02, -2.4738e-02,\n",
       "           1.7367e-02, -9.0813e-03, -5.2706e-03,  1.6631e-02,  4.5461e-03,\n",
       "           1.4820e-02,  2.4114e-02, -1.0053e-02, -2.2478e-02, -1.6256e-02,\n",
       "           1.6617e-02,  2.5887e-02, -2.7561e-02, -2.3107e-02, -1.7291e-02,\n",
       "           8.0678e-03,  8.0520e-03,  2.4770e-02, -6.7698e-03,  2.0153e-02,\n",
       "          -3.2801e-03,  9.6590e-03, -5.6798e-03,  1.1617e-02, -2.5674e-02,\n",
       "           1.0729e-02,  2.1314e-04, -7.5801e-03, -2.7510e-02, -1.3076e-02,\n",
       "           2.3412e-02,  1.8657e-02, -1.3783e-02,  2.1040e-02,  9.7506e-03,\n",
       "           6.1170e-03,  1.4217e-02, -1.9251e-02, -1.6440e-02,  1.5221e-02,\n",
       "           3.0448e-03, -1.3174e-02,  1.9535e-03, -6.6802e-03, -1.8126e-02,\n",
       "           2.1754e-02, -3.7448e-03, -2.4139e-02, -2.3418e-02,  2.8919e-03,\n",
       "           2.6814e-02,  1.7183e-02,  4.1586e-03, -1.4606e-02, -1.1938e-02,\n",
       "           7.9851e-03,  1.9028e-02,  3.3525e-03,  1.0206e-02, -1.4022e-02,\n",
       "          -1.8882e-02,  9.9171e-04,  1.4700e-02,  1.1460e-02,  2.2128e-03,\n",
       "           6.4329e-03,  1.0394e-02,  1.0723e-02, -2.2429e-02,  3.1814e-03,\n",
       "          -2.3769e-02,  2.7201e-02, -1.5322e-02,  7.9500e-03, -1.7446e-04,\n",
       "           5.5536e-04,  8.5354e-03, -8.2092e-03,  8.4590e-03,  2.4454e-02,\n",
       "          -1.7058e-02,  1.4513e-02, -2.6000e-02, -2.2587e-02, -2.6592e-02,\n",
       "           1.0368e-02,  8.6740e-03, -2.1216e-03, -1.3282e-02, -4.5161e-03,\n",
       "          -1.0735e-02,  1.5658e-02, -1.8828e-02,  1.1684e-02, -3.3457e-03,\n",
       "          -4.3491e-04, -4.6206e-03, -1.8596e-03,  1.9273e-02,  6.6093e-03,\n",
       "          -2.4076e-02,  4.4642e-03,  1.3127e-02,  1.0129e-02, -3.4105e-03,\n",
       "          -2.6764e-02, -2.0346e-02,  4.9500e-03,  6.4318e-03, -2.1182e-03,\n",
       "          -2.3783e-02, -1.4036e-02, -1.7398e-02, -1.0013e-02, -8.6625e-03,\n",
       "          -1.3396e-02, -2.4461e-03,  2.5630e-03,  2.7779e-02,  5.2278e-03,\n",
       "           1.4872e-02,  2.5907e-02,  1.3261e-03,  1.8092e-02,  2.8190e-03,\n",
       "           2.2514e-02,  1.4066e-02,  1.3642e-02,  4.6147e-03,  1.5738e-03,\n",
       "          -7.4641e-03,  9.4243e-04, -1.8005e-03, -2.5236e-02, -1.7714e-03,\n",
       "           1.5383e-03,  2.6286e-02,  2.0995e-02,  1.2046e-02,  1.2859e-02,\n",
       "          -4.9461e-03, -2.3612e-02, -1.0334e-02, -8.6202e-03,  3.4598e-03,\n",
       "           1.3697e-02, -2.7614e-02,  1.8221e-02,  2.1189e-03, -7.9956e-03,\n",
       "           1.1727e-02, -6.0044e-03, -1.3839e-02, -1.9134e-02,  6.0832e-03,\n",
       "           1.6675e-03,  2.1207e-02, -2.0674e-02,  6.3604e-03,  2.7774e-03,\n",
       "           3.3301e-03,  9.8987e-03, -1.5305e-02,  1.5608e-02,  2.7875e-02,\n",
       "           6.3606e-03, -8.2499e-03, -2.2898e-02, -2.2823e-02, -1.6601e-02,\n",
       "           2.3833e-02,  2.1201e-03, -8.7707e-03,  7.0065e-03, -5.5159e-03,\n",
       "          -8.3060e-03,  2.3690e-02, -2.4440e-02, -5.3101e-03, -1.4189e-02,\n",
       "          -1.5676e-02, -2.1041e-02, -1.9293e-02,  1.7518e-02,  2.9641e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 2.7310e-03,  1.6718e-02, -2.8794e-02,  ..., -3.1159e-02,\n",
       "           -1.3400e-02, -4.1441e-03],\n",
       "          [-5.9042e-03,  3.5136e-02,  2.2077e-02,  ...,  1.5674e-02,\n",
       "            3.2627e-02, -2.7906e-02],\n",
       "          [ 1.6996e-02, -2.7357e-02, -2.5944e-02,  ...,  2.7423e-02,\n",
       "           -5.6892e-03,  7.9568e-03],\n",
       "          ...,\n",
       "          [ 2.9716e-02, -3.8121e-02,  3.3671e-02,  ..., -1.6187e-02,\n",
       "           -2.0732e-02, -1.1764e-02],\n",
       "          [-2.0023e-02,  1.2109e-02,  9.4979e-04,  ...,  1.2096e-02,\n",
       "            2.8813e-02, -2.1212e-02],\n",
       "          [-2.5120e-02, -9.8185e-03, -3.8048e-03,  ..., -9.1986e-05,\n",
       "           -3.2447e-02,  3.5994e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0103,  0.0144,  0.0393,  0.0285, -0.0373,  0.0177,  0.0155, -0.0126],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0105,  0.0125,  0.0112,  ...,  0.0017, -0.0045, -0.0055],\n",
       "          [ 0.0061, -0.0011,  0.0209,  ..., -0.0127, -0.0133,  0.0192],\n",
       "          [-0.0190,  0.0040, -0.0078,  ...,  0.0205,  0.0065, -0.0126],\n",
       "          ...,\n",
       "          [ 0.0043, -0.0035,  0.0023,  ...,  0.0121, -0.0221,  0.0061],\n",
       "          [ 0.0153,  0.0180,  0.0029,  ...,  0.0103, -0.0011,  0.0091],\n",
       "          [-0.0054, -0.0032,  0.0046,  ...,  0.0031, -0.0109,  0.0067]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-1.5027e-02,  1.1020e-02,  6.9823e-03,  ..., -6.0435e-03,\n",
       "            1.4315e-02,  5.2821e-03],\n",
       "          [-3.4003e-03,  4.2887e-03,  6.0633e-03,  ..., -6.3717e-03,\n",
       "            7.6471e-03,  1.7686e-02],\n",
       "          [ 2.8060e-02, -8.6273e-03, -2.0297e-02,  ..., -1.3127e-02,\n",
       "           -3.8980e-03, -4.7014e-03],\n",
       "          ...,\n",
       "          [ 9.7330e-03, -3.1599e-02,  2.2410e-03,  ..., -4.5142e-03,\n",
       "            2.5363e-06, -2.8431e-03],\n",
       "          [ 5.5690e-03,  1.3949e-02, -9.4162e-03,  ..., -6.7472e-05,\n",
       "           -8.8533e-03,  1.4624e-02],\n",
       "          [ 1.5957e-02,  3.2968e-03,  3.9670e-04,  ...,  2.3624e-04,\n",
       "            1.4222e-02, -3.7719e-03]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0182,  0.0083, -0.0338,  ...,  0.0071,  0.0072,  0.0021],\n",
       "          [ 0.0023,  0.0065, -0.0051,  ...,  0.0201,  0.0182,  0.0037],\n",
       "          [ 0.0002,  0.0005,  0.0070,  ..., -0.0189, -0.0063,  0.0047],\n",
       "          ...,\n",
       "          [-0.0033,  0.0287,  0.0176,  ..., -0.0101, -0.0002, -0.0128],\n",
       "          [-0.0045,  0.0256,  0.0266,  ...,  0.0038, -0.0049,  0.0014],\n",
       "          [-0.0024,  0.0076,  0.0033,  ...,  0.0122,  0.0188, -0.0175]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0175, -0.0034,  0.0070,  ..., -0.0242,  0.0066,  0.0066],\n",
       "          [-0.0169,  0.0013,  0.0086,  ..., -0.0136,  0.0185, -0.0253],\n",
       "          [ 0.0089, -0.0076,  0.0288,  ...,  0.0057,  0.0068,  0.0002],\n",
       "          ...,\n",
       "          [-0.0124, -0.0158,  0.0245,  ...,  0.0018, -0.0189,  0.0215],\n",
       "          [-0.0094,  0.0023, -0.0027,  ...,  0.0045,  0.0061, -0.0250],\n",
       "          [-0.0104,  0.0274,  0.0230,  ..., -0.0041, -0.0203,  0.0298]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0018, -0.0214,  0.0108,  ...,  0.0255, -0.0159, -0.0268],\n",
       "          [-0.0159, -0.0062,  0.0157,  ...,  0.0110, -0.0215,  0.0171],\n",
       "          [ 0.0189,  0.0095,  0.0076,  ..., -0.0184, -0.0142, -0.0042],\n",
       "          ...,\n",
       "          [-0.0262,  0.0148,  0.0246,  ..., -0.0185, -0.0253, -0.0181],\n",
       "          [-0.0125,  0.0278,  0.0276,  ..., -0.0226,  0.0203, -0.0065],\n",
       "          [-0.0201, -0.0098,  0.0176,  ..., -0.0123, -0.0090, -0.0028]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.9239e-02, -3.9980e-03,  2.6798e-02,  1.9647e-03, -1.4265e-02,\n",
       "          -2.7584e-02, -1.1289e-02, -1.2328e-02,  1.8708e-02, -1.0616e-02,\n",
       "           5.6879e-03,  1.3692e-02,  5.0366e-03,  1.1116e-02, -1.1794e-02,\n",
       "          -2.7110e-02,  2.6606e-02,  5.4674e-04, -9.5072e-03, -2.2819e-02,\n",
       "           2.0422e-03,  1.8138e-02, -1.1938e-03, -3.9674e-04, -1.8347e-02,\n",
       "          -2.6859e-02, -1.4697e-02, -8.6973e-03,  4.3993e-03, -2.6484e-02,\n",
       "           6.2642e-06,  9.5479e-03, -1.9361e-02, -1.5119e-02,  1.6836e-02,\n",
       "          -1.8052e-03, -7.3632e-03,  1.4434e-02,  1.4851e-02,  1.9045e-02,\n",
       "          -1.5722e-03,  7.4908e-03, -2.4791e-02,  3.2346e-03,  1.5424e-02,\n",
       "           3.2430e-03, -2.7170e-02, -2.6071e-02,  2.4095e-02, -7.0299e-05,\n",
       "           1.3973e-02, -6.2982e-04, -1.5373e-02, -2.4263e-02, -1.3664e-02,\n",
       "          -2.4049e-02, -1.7910e-02,  2.7647e-02,  7.8748e-03,  1.6032e-04,\n",
       "          -1.9455e-02, -8.5615e-03,  4.4952e-03,  1.7767e-02, -2.3454e-02,\n",
       "          -1.8732e-02, -1.1551e-02, -4.1549e-03, -1.6855e-03, -2.0790e-02,\n",
       "          -1.6991e-02,  1.2573e-02,  1.2582e-02, -1.2746e-02,  1.8901e-02,\n",
       "          -9.0894e-04,  2.2915e-02, -1.1551e-02, -2.4426e-02, -2.1528e-03,\n",
       "          -2.1993e-02,  6.4279e-03, -3.7435e-03, -5.1616e-03,  2.2932e-02,\n",
       "          -2.3209e-02, -2.2235e-02, -1.9237e-02,  1.6700e-02,  1.1551e-02,\n",
       "           8.0181e-03, -6.6762e-03,  1.3099e-02,  1.4683e-02,  1.5245e-02,\n",
       "          -2.6430e-02,  1.6624e-02,  2.6018e-02,  3.0000e-03,  1.4463e-04,\n",
       "           3.5388e-03, -2.5238e-02, -6.3460e-03,  1.7528e-02, -2.6891e-02,\n",
       "           1.5502e-02, -1.9221e-02,  1.1693e-02, -4.5505e-03,  2.9451e-03,\n",
       "          -1.2516e-02,  2.2913e-02, -7.0687e-03,  1.4967e-02,  1.3349e-02,\n",
       "           2.0716e-02, -2.6437e-02,  1.0435e-02,  2.7099e-02, -9.0833e-03,\n",
       "          -1.1702e-02,  1.3179e-03,  2.0323e-02,  2.7217e-02, -2.4281e-02,\n",
       "          -1.2258e-02,  2.4489e-03,  1.1162e-02, -2.2944e-02,  1.9505e-02,\n",
       "          -2.4729e-02,  1.1206e-02, -4.5826e-03,  1.7515e-02,  8.2117e-03,\n",
       "           1.2403e-02, -8.5122e-03,  1.6375e-02, -2.1344e-02,  1.6861e-02,\n",
       "          -2.3570e-02, -1.7671e-02, -9.0178e-04,  1.7346e-02, -1.1984e-02,\n",
       "          -1.3454e-03,  2.3312e-02, -1.6919e-02, -1.3120e-02, -3.4860e-03,\n",
       "           1.6965e-02,  1.6226e-02, -2.1052e-02, -2.6710e-02,  1.5538e-02,\n",
       "          -2.2442e-02,  1.1002e-03, -2.2901e-02, -5.1308e-03,  9.0039e-03,\n",
       "           2.5259e-02, -2.5297e-02,  1.2022e-02,  2.3103e-02, -2.2817e-02,\n",
       "           8.2527e-03,  6.2467e-03, -1.3656e-02,  1.9880e-02,  2.2143e-02,\n",
       "          -1.2013e-02,  4.6905e-03, -2.2323e-02,  8.4141e-03, -2.4543e-02,\n",
       "          -1.0309e-04, -2.4346e-02, -9.6772e-03,  1.8556e-02,  1.8595e-02,\n",
       "           8.6623e-03,  2.6989e-02, -1.2006e-02,  2.6529e-02, -8.4687e-03,\n",
       "          -1.0762e-02,  2.0041e-03, -1.0017e-02,  8.0791e-04, -1.2497e-02,\n",
       "          -5.3859e-03, -1.4709e-03,  1.4256e-02,  2.2670e-02, -2.4511e-02,\n",
       "          -4.1065e-03, -2.6438e-02,  2.0679e-02,  2.8123e-03, -2.2765e-02,\n",
       "          -2.3183e-03,  1.7889e-02, -3.5873e-03, -1.3038e-02,  1.6258e-02,\n",
       "           2.3498e-02, -2.0007e-02, -2.3578e-02, -2.0383e-02,  7.7456e-03,\n",
       "          -2.1737e-02, -1.4497e-02,  3.8881e-03,  1.2158e-02, -1.9781e-02,\n",
       "          -2.5493e-02,  2.6014e-02,  1.5841e-02, -7.5779e-03,  1.9457e-02,\n",
       "           1.4099e-02, -2.3364e-02,  1.7791e-02,  2.0225e-03, -1.6600e-02,\n",
       "           1.4922e-02,  1.5895e-02, -4.5715e-03,  1.8529e-02,  1.4392e-02,\n",
       "           2.7731e-02,  1.1578e-02,  1.0337e-02, -1.4175e-02, -6.9612e-03,\n",
       "          -2.4494e-02, -1.9445e-02,  2.5101e-02, -7.1762e-03, -1.7136e-02,\n",
       "           1.0357e-02, -1.3547e-03,  1.4378e-02, -2.4992e-02,  1.0803e-02,\n",
       "           2.5430e-02,  1.5256e-02,  1.6295e-02,  9.2661e-03,  2.4351e-02,\n",
       "           2.7436e-02,  2.4119e-02,  2.1020e-02, -6.5120e-03, -7.0363e-03,\n",
       "          -8.2234e-04,  1.9206e-02,  8.6241e-03,  2.4731e-02, -2.7102e-02,\n",
       "          -4.6264e-03,  1.4581e-02,  6.1339e-03,  6.6927e-03,  8.0648e-03,\n",
       "          -1.5911e-02, -2.1866e-02,  9.9054e-03, -1.5249e-02,  2.2847e-02,\n",
       "           1.9512e-02,  2.0868e-02,  4.0025e-04, -7.8345e-03,  2.1828e-02,\n",
       "          -1.3753e-02, -1.5272e-02, -2.4088e-02,  1.0692e-02, -2.0842e-02,\n",
       "          -9.0843e-04, -1.3977e-02, -1.0968e-02, -2.4117e-03,  2.2826e-02,\n",
       "           2.1205e-02, -1.5394e-02, -1.1047e-02,  6.4363e-03, -2.3695e-02,\n",
       "          -1.5403e-03,  4.7237e-03,  1.7851e-02,  7.4595e-03, -3.4230e-03,\n",
       "           2.3267e-02, -9.3287e-03,  6.4618e-03, -3.3984e-04, -1.5408e-03,\n",
       "           1.8543e-02,  2.3541e-02, -7.7070e-03,  1.8895e-02, -1.0198e-02,\n",
       "           8.9606e-03, -9.7339e-03,  4.6633e-03,  1.0035e-02,  1.4751e-02,\n",
       "           1.1445e-02,  1.5970e-03, -1.3679e-02, -1.0669e-02,  9.3357e-03,\n",
       "           2.7142e-02,  1.6782e-02,  1.7513e-02,  2.3996e-02,  2.8652e-04,\n",
       "           1.5009e-02, -1.9577e-02, -8.0734e-04, -4.6425e-03, -1.8325e-03,\n",
       "          -2.3755e-02,  3.5327e-03,  2.6922e-02, -1.3589e-02, -2.6805e-02,\n",
       "           6.8039e-04,  9.3013e-03,  6.7421e-03,  1.8415e-02, -9.6970e-03,\n",
       "           2.3025e-02, -1.8617e-02, -3.7197e-03,  2.2129e-02, -3.8232e-03,\n",
       "          -1.2637e-02, -1.4910e-02, -2.2358e-02, -1.9534e-02, -8.8560e-03,\n",
       "           5.4336e-03, -1.4678e-02, -2.4917e-03,  8.3310e-03, -2.2327e-02,\n",
       "          -1.2577e-02, -2.7114e-02, -4.3499e-03,  1.1804e-02, -5.7326e-03,\n",
       "          -2.1583e-02,  2.1520e-02, -1.3836e-03, -5.5033e-03, -2.2985e-02,\n",
       "          -2.1907e-02, -6.1916e-03, -2.0816e-02, -1.9508e-02, -5.9437e-03,\n",
       "           1.1877e-02,  2.3429e-02, -2.3405e-02,  1.9487e-02, -1.8902e-02,\n",
       "          -1.7406e-02,  1.8294e-02,  2.4345e-02,  1.3444e-03, -2.5596e-02,\n",
       "           1.9031e-02, -8.8789e-03,  2.5612e-02,  2.7493e-02,  1.8283e-02,\n",
       "          -2.2970e-02, -3.8417e-03, -2.4234e-02, -9.3395e-03, -2.2666e-02,\n",
       "          -1.1016e-02,  8.6318e-03,  1.6930e-02, -1.5623e-02,  1.2494e-02,\n",
       "          -2.3370e-03, -2.1976e-02,  1.2654e-02, -4.5834e-03, -2.0428e-02,\n",
       "          -4.9645e-03, -2.6023e-02, -1.4713e-02,  2.1241e-02, -7.9263e-03,\n",
       "           5.5172e-03, -2.3186e-02, -7.2594e-03,  2.7899e-02,  1.3391e-02,\n",
       "          -5.9682e-03,  2.4839e-02,  8.6737e-03,  8.1457e-03, -2.1922e-02,\n",
       "           2.7960e-03, -2.4817e-02, -1.1129e-02, -1.8552e-02,  5.4417e-03,\n",
       "           1.7004e-02, -2.1057e-02, -6.5879e-03, -2.1036e-02,  2.0676e-02,\n",
       "          -2.1359e-02, -2.1355e-02,  2.6694e-02, -1.4690e-02,  8.2763e-03,\n",
       "          -1.2384e-02, -4.4547e-03, -1.1116e-02, -2.5538e-03,  7.1265e-03,\n",
       "           2.0615e-02,  5.4732e-03, -1.3600e-02, -3.7053e-03, -1.0014e-02,\n",
       "           9.2130e-03,  7.5887e-03, -1.3362e-02, -4.8024e-03, -4.2663e-04,\n",
       "           9.4769e-03, -1.4988e-02, -4.6508e-03, -5.6645e-04,  1.7220e-02,\n",
       "          -2.4027e-02,  1.2572e-02,  1.5340e-02,  9.8483e-03,  1.7179e-02,\n",
       "           9.9524e-05,  1.0089e-02,  5.8730e-03,  9.3241e-03,  2.1242e-03,\n",
       "          -2.6546e-02, -9.4077e-03, -8.0697e-03, -1.2418e-02,  2.4666e-02,\n",
       "          -1.8470e-02,  5.5003e-03,  2.4040e-02,  1.8207e-02, -2.3440e-02,\n",
       "          -2.7659e-02, -6.5028e-03,  2.7685e-02, -2.0329e-02,  2.6805e-03,\n",
       "           4.2599e-03,  2.6254e-03, -1.7903e-02, -2.1586e-02, -1.9330e-02,\n",
       "          -2.2397e-02,  2.3801e-02, -2.0032e-02, -1.9909e-02, -1.1447e-03,\n",
       "           1.5221e-02,  2.5270e-04, -2.2655e-02, -1.6819e-02, -2.6185e-02,\n",
       "           2.0291e-02,  1.0319e-02,  2.4898e-02,  1.4808e-02, -5.0881e-03,\n",
       "           1.3175e-02, -2.7784e-02,  2.4233e-02, -2.5604e-02, -1.0049e-02,\n",
       "          -8.1567e-03,  4.1898e-03, -1.6356e-02, -2.4254e-03, -8.8073e-03,\n",
       "          -2.2648e-02, -1.2740e-02,  1.4560e-02,  2.1219e-02, -1.6758e-02,\n",
       "           2.5082e-02,  9.8803e-03, -3.3037e-04, -5.3444e-03,  2.2297e-02,\n",
       "          -6.8865e-03,  5.7396e-03, -5.6016e-03,  1.1850e-02,  3.5717e-03,\n",
       "           2.3483e-02, -2.3722e-02, -2.3281e-02,  1.0981e-02, -2.6202e-02,\n",
       "           1.2082e-02,  5.4251e-03, -2.0067e-02, -6.4096e-03,  1.6984e-02,\n",
       "          -2.5114e-02,  1.3411e-02,  1.4410e-02,  9.8386e-03,  2.1515e-02,\n",
       "          -9.2688e-04, -1.7852e-02, -2.0561e-02, -1.0410e-02, -9.5093e-04,\n",
       "           2.3936e-02,  2.2323e-02, -1.4157e-02,  2.7188e-02,  2.5824e-03,\n",
       "          -2.6067e-03,  6.0264e-03,  1.5001e-02, -4.0684e-03, -2.7334e-02,\n",
       "           2.0959e-02,  7.8170e-03, -6.2970e-03, -2.3773e-02, -4.4683e-03,\n",
       "           1.4124e-02,  3.3854e-03, -1.4814e-02,  1.7063e-02,  1.2435e-03,\n",
       "           9.9324e-03,  7.5743e-05,  3.4088e-03,  1.2023e-02, -1.7202e-02,\n",
       "           1.9615e-02,  8.4205e-03, -1.4160e-02,  4.4018e-03,  1.2879e-02,\n",
       "          -4.0322e-04,  6.8952e-03, -9.9132e-03, -2.3436e-02,  1.8180e-02,\n",
       "          -6.9667e-03, -1.0155e-02, -7.2985e-03,  1.9162e-02,  3.1816e-03,\n",
       "          -2.3020e-03, -2.7286e-02,  9.6504e-03, -2.7620e-03, -2.2939e-02,\n",
       "          -2.1673e-02, -1.1688e-02, -1.5117e-03, -1.5825e-03, -1.0199e-02,\n",
       "          -1.9350e-02,  1.5213e-02,  2.0865e-02,  1.8529e-02,  1.3067e-02,\n",
       "          -6.2287e-04,  8.3019e-04, -3.9335e-03,  2.3097e-02, -1.7589e-02,\n",
       "          -2.4175e-02, -2.1763e-02,  1.5451e-02,  2.2828e-03, -6.8504e-03,\n",
       "           1.8038e-02,  1.4402e-02,  2.1540e-02,  2.4687e-02, -1.3881e-02,\n",
       "          -8.7824e-03, -3.1056e-03,  2.6288e-02, -1.2455e-03, -7.9971e-03,\n",
       "          -2.6895e-02,  7.0972e-06,  1.8497e-02,  1.0663e-02,  2.0158e-02,\n",
       "          -2.4665e-02,  4.1203e-03, -1.9611e-02, -2.2868e-02,  1.3388e-02,\n",
       "          -2.2400e-02, -2.2651e-02,  2.1054e-02,  2.6512e-02,  1.6578e-03,\n",
       "          -2.2878e-02,  3.6870e-03, -1.3285e-02, -2.0808e-02,  1.5145e-02,\n",
       "          -2.4119e-02, -9.9566e-03, -2.5158e-02,  9.5108e-03,  2.5661e-02,\n",
       "           1.8707e-02, -1.0426e-02, -2.4749e-02, -1.6824e-02,  2.6001e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0316, -0.0119, -0.0013,  ...,  0.0224, -0.0244, -0.0035],\n",
       "          [-0.0328,  0.0162, -0.0049,  ..., -0.0329,  0.0387, -0.0385],\n",
       "          [ 0.0247,  0.0359, -0.0310,  ...,  0.0302, -0.0348,  0.0035],\n",
       "          ...,\n",
       "          [ 0.0011, -0.0296, -0.0291,  ..., -0.0369,  0.0267,  0.0370],\n",
       "          [-0.0095,  0.0096, -0.0037,  ...,  0.0252,  0.0117,  0.0268],\n",
       "          [ 0.0374,  0.0383,  0.0040,  ..., -0.0064, -0.0253,  0.0014]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0389, -0.0187, -0.0178, -0.0025, -0.0350, -0.0344,  0.0093,  0.0115],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0078, -0.0206,  0.0276,  ..., -0.0273,  0.0092, -0.0056],\n",
       "          [ 0.0305,  0.0140,  0.0213,  ..., -0.0228,  0.0218, -0.0015],\n",
       "          [ 0.0041,  0.0307, -0.0270,  ..., -0.0173, -0.0191,  0.0033],\n",
       "          ...,\n",
       "          [ 0.0030,  0.0116,  0.0256,  ...,  0.0100,  0.0297, -0.0026],\n",
       "          [-0.0041, -0.0046,  0.0077,  ...,  0.0062,  0.0046,  0.0063],\n",
       "          [-0.0181, -0.0163,  0.0098,  ...,  0.0206, -0.0113,  0.0060]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0261,  0.0164, -0.0233,  ..., -0.0067, -0.0155, -0.0307],\n",
       "          [-0.0119,  0.0101,  0.0071,  ...,  0.0214, -0.0034,  0.0053],\n",
       "          [-0.0188, -0.0018, -0.0247,  ...,  0.0227, -0.0066,  0.0099],\n",
       "          ...,\n",
       "          [ 0.0037, -0.0035,  0.0016,  ...,  0.0129,  0.0126, -0.0117],\n",
       "          [ 0.0052,  0.0269,  0.0080,  ...,  0.0097,  0.0080, -0.0323],\n",
       "          [-0.0095, -0.0354, -0.0059,  ...,  0.0017,  0.0159,  0.0249]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 5.4775e-03, -2.5122e-02, -9.2372e-03,  ..., -1.4324e-02,\n",
       "            8.3128e-03,  1.0182e-02],\n",
       "          [-1.0755e-06, -5.7632e-03,  2.4372e-02,  ..., -2.5753e-02,\n",
       "            5.2874e-03, -1.4843e-02],\n",
       "          [-5.7129e-03, -6.3656e-04, -5.4584e-04,  ..., -1.4231e-02,\n",
       "           -5.9536e-03, -2.0416e-02],\n",
       "          ...,\n",
       "          [ 1.5661e-02,  3.3141e-02, -1.2794e-02,  ..., -7.0259e-04,\n",
       "           -2.0102e-02,  2.6792e-02],\n",
       "          [ 1.1441e-02, -3.8526e-03, -2.0341e-02,  ..., -7.7292e-03,\n",
       "           -2.5609e-04, -3.4658e-02],\n",
       "          [-1.2726e-02,  1.3230e-02, -3.1730e-02,  ..., -3.8987e-02,\n",
       "           -2.8997e-02,  3.4035e-03]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0070, -0.0400,  0.0102,  ..., -0.0132, -0.0161, -0.0222],\n",
       "          [-0.0170, -0.0122, -0.0095,  ..., -0.0187, -0.0066, -0.0333],\n",
       "          [-0.0225,  0.0133,  0.0294,  ..., -0.0094,  0.0065,  0.0046],\n",
       "          ...,\n",
       "          [-0.0089,  0.0241,  0.0007,  ..., -0.0180, -0.0074, -0.0088],\n",
       "          [-0.0176, -0.0308,  0.0037,  ...,  0.0045, -0.0009, -0.0215],\n",
       "          [-0.0088,  0.0085, -0.0093,  ...,  0.0103, -0.0004,  0.0073]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0096, -0.0189,  0.0173,  ...,  0.0184,  0.0107, -0.0200],\n",
       "          [ 0.0221,  0.0072,  0.0211,  ...,  0.0221,  0.0198, -0.0179],\n",
       "          [-0.0098, -0.0094, -0.0232,  ...,  0.0198,  0.0238,  0.0169],\n",
       "          ...,\n",
       "          [ 0.0157,  0.0086, -0.0101,  ..., -0.0012, -0.0211, -0.0221],\n",
       "          [-0.0114, -0.0039, -0.0182,  ..., -0.0131, -0.0220,  0.0099],\n",
       "          [-0.0252,  0.0117,  0.0217,  ...,  0.0236, -0.0171,  0.0276]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 2.3008e-02,  5.9604e-03, -9.2446e-04, -2.7465e-02, -2.5401e-02,\n",
       "          -2.3867e-02,  1.6691e-02,  3.9245e-03,  2.1578e-02, -1.5979e-02,\n",
       "           2.0004e-02, -1.9756e-02,  2.7683e-02,  1.3338e-02,  1.6380e-02,\n",
       "           1.7317e-02,  7.4749e-03, -1.4055e-02, -1.3152e-02,  2.4915e-02,\n",
       "           1.6181e-02,  1.2714e-02,  1.2492e-02, -1.3198e-02,  9.0843e-03,\n",
       "          -3.7709e-03,  1.7176e-02, -8.4457e-03, -2.6141e-02,  2.3534e-03,\n",
       "          -1.8959e-02, -1.7862e-02,  1.5833e-02,  2.7482e-02,  2.4676e-03,\n",
       "           2.5645e-04, -2.0413e-02, -2.3134e-02,  1.7580e-02, -8.5758e-03,\n",
       "          -1.9146e-02,  2.6736e-02,  2.4588e-02, -1.7089e-02,  8.9686e-03,\n",
       "          -2.2568e-02,  1.7622e-02, -7.9445e-03, -2.7479e-02,  2.5091e-02,\n",
       "          -3.3417e-03,  1.6041e-02, -2.0886e-02,  1.2559e-02, -1.1628e-02,\n",
       "           1.5763e-02,  2.7195e-02, -2.4486e-02, -2.7427e-02,  2.6715e-03,\n",
       "          -2.0742e-02, -3.3509e-03,  6.5838e-03, -7.2081e-03, -5.7261e-03,\n",
       "           2.3882e-02,  2.4937e-03,  2.5167e-02,  3.2440e-03,  2.7114e-02,\n",
       "          -2.4811e-02,  1.8522e-03,  4.6953e-03,  1.0189e-02, -3.7588e-03,\n",
       "          -7.6083e-03, -4.7928e-03, -1.9707e-02, -1.1722e-02,  1.4312e-02,\n",
       "          -1.2224e-02, -1.9918e-02, -1.7012e-02, -1.2400e-02, -2.3835e-02,\n",
       "           8.7362e-03,  7.1548e-03, -9.8069e-03,  1.9466e-02,  1.5309e-02,\n",
       "           2.1316e-02, -1.0731e-02, -2.5704e-02, -1.9632e-02, -1.2804e-02,\n",
       "          -2.1723e-03, -2.1437e-03,  1.5740e-02, -4.3700e-03,  5.9995e-04,\n",
       "           7.6506e-03, -1.3861e-02, -4.6953e-04, -9.3423e-03,  3.4220e-03,\n",
       "          -2.4153e-02, -3.0193e-03, -2.6193e-02,  1.7428e-02,  1.2510e-02,\n",
       "           1.2955e-02,  1.6120e-02, -2.9350e-03,  8.4932e-04, -1.7877e-03,\n",
       "          -6.8700e-03,  7.9493e-03,  2.0932e-02,  1.3555e-03,  1.1403e-02,\n",
       "           1.1720e-02,  1.5543e-02, -4.8084e-03,  1.9326e-02,  2.6411e-02,\n",
       "           2.5701e-02,  1.7247e-02, -3.1428e-03, -1.9878e-02,  1.5349e-02,\n",
       "          -2.0618e-02,  2.5498e-02, -2.0454e-02, -1.1189e-02,  1.2689e-02,\n",
       "          -2.1561e-02,  7.4595e-03,  1.0084e-02, -2.2405e-02, -1.0670e-02,\n",
       "           8.5337e-03, -8.5737e-04,  1.3972e-02,  1.0614e-02,  2.0846e-02,\n",
       "          -2.3296e-02, -1.7266e-02,  1.6929e-02, -2.5631e-02,  1.9933e-02,\n",
       "           3.0379e-03,  5.0856e-03, -2.7402e-02, -1.6139e-02,  7.5869e-03,\n",
       "          -1.6049e-04, -2.0500e-02,  1.2489e-02,  5.7105e-03,  9.3212e-03,\n",
       "          -5.8862e-03, -4.4096e-03,  2.1469e-02,  8.8061e-03,  1.8124e-02,\n",
       "           1.8111e-02, -8.1529e-03,  1.1097e-02,  1.9867e-02, -6.2199e-03,\n",
       "          -2.1656e-02, -9.0202e-04, -1.3198e-02, -1.8676e-02, -1.3945e-02,\n",
       "           4.3382e-03,  8.8103e-03, -1.4784e-02, -1.5504e-02, -3.6104e-04,\n",
       "          -3.4613e-03,  4.6112e-03,  1.5422e-02,  7.9865e-03, -1.4974e-02,\n",
       "           2.5302e-02,  2.6721e-02,  2.5781e-02, -3.6974e-04,  1.2861e-02,\n",
       "          -1.4241e-02,  9.7000e-03, -1.4415e-02, -9.0193e-03,  1.5833e-03,\n",
       "           2.0585e-02,  1.6747e-02, -1.1262e-02,  2.6188e-02,  6.9825e-03,\n",
       "           1.6188e-03,  1.7513e-02,  2.6748e-02,  2.4061e-02, -2.2206e-02,\n",
       "           2.9155e-06, -2.4123e-02,  2.0119e-02, -7.0802e-03,  1.0107e-02,\n",
       "          -2.5584e-02,  1.9131e-02, -1.9636e-02, -1.4876e-03,  2.6303e-02,\n",
       "           1.0626e-02,  7.7648e-03, -3.4382e-03, -2.6377e-02, -2.0086e-02,\n",
       "          -4.0380e-03,  1.0593e-03,  3.0722e-03, -1.4645e-02,  2.6199e-02,\n",
       "           2.1393e-02, -1.9471e-02, -1.1309e-02,  2.4512e-02,  1.7210e-02,\n",
       "           1.5889e-02, -1.1243e-02,  1.6773e-02,  1.3728e-02, -2.3901e-02,\n",
       "          -2.3996e-02,  1.9837e-03,  2.3517e-02, -2.3741e-02, -2.7433e-02,\n",
       "           2.0982e-03,  1.8392e-02, -3.4813e-03, -9.4581e-03,  2.5103e-02,\n",
       "          -3.6883e-03, -2.5892e-02, -1.8843e-02,  1.5952e-02,  1.7481e-02,\n",
       "           2.7646e-02, -9.5006e-03,  2.1464e-02,  2.4253e-02, -1.0045e-02,\n",
       "          -3.3805e-03, -1.0832e-02,  1.5908e-02, -3.7830e-04, -1.5258e-02,\n",
       "           2.1909e-02,  2.3991e-02,  2.1468e-02,  1.2237e-02,  2.6649e-02,\n",
       "           1.7133e-02,  2.5443e-02,  9.9109e-04, -8.5713e-03,  2.0854e-02,\n",
       "           1.6358e-02, -2.7184e-02,  1.8953e-02, -1.3226e-02, -2.2365e-02,\n",
       "           1.0552e-02, -1.9169e-02, -1.4374e-02,  8.2270e-04, -6.5173e-03,\n",
       "           7.7445e-03,  1.5418e-02,  2.2100e-02, -2.2279e-02,  1.2497e-02,\n",
       "           2.6936e-02, -6.1835e-03, -2.2092e-02, -1.0220e-02,  1.0860e-02,\n",
       "           1.3286e-03,  2.1711e-02,  2.1760e-02,  1.5433e-02,  1.5438e-03,\n",
       "          -2.6334e-03, -4.3589e-03, -1.5970e-02, -7.2372e-04, -1.7243e-02,\n",
       "          -1.7483e-02,  4.8983e-03,  2.6330e-02, -1.5319e-02, -9.0151e-03,\n",
       "           1.5538e-02, -2.6515e-02, -1.8025e-02, -1.4922e-02, -7.4477e-03,\n",
       "          -3.2619e-04,  2.4432e-02, -2.0821e-02,  2.7576e-03,  2.4396e-02,\n",
       "          -1.9473e-02, -4.7682e-03,  2.4593e-02, -3.4082e-03, -1.5028e-02,\n",
       "           2.7027e-02,  7.8299e-04,  5.1089e-03, -2.3849e-02, -3.6582e-03,\n",
       "           1.4197e-02, -2.1037e-03, -3.3268e-03, -2.2534e-02, -1.3152e-03,\n",
       "          -1.2148e-02, -1.2335e-03, -1.3205e-02,  1.7563e-02,  2.6481e-02,\n",
       "           1.3132e-02,  2.2400e-03,  3.0539e-03, -2.1757e-02,  2.7361e-02,\n",
       "          -1.6158e-02, -1.3447e-02, -5.3406e-03, -1.4321e-05,  1.6755e-02,\n",
       "          -8.2512e-04, -6.5673e-03, -3.5835e-04,  2.0916e-02, -2.0771e-02,\n",
       "          -1.2254e-02, -1.5563e-02, -2.3953e-02, -1.0869e-02, -1.6630e-02,\n",
       "           2.1151e-02,  2.8540e-03,  2.7057e-02,  6.7224e-03,  2.5891e-02,\n",
       "           5.5030e-03, -2.7521e-02,  1.1028e-02, -1.6300e-02,  1.4053e-02,\n",
       "          -2.4842e-02,  1.4599e-02, -8.7946e-03, -2.0401e-02,  1.4318e-02,\n",
       "          -1.1534e-02, -1.6182e-02,  1.3923e-02,  1.2336e-02, -2.3813e-02,\n",
       "          -2.0945e-02,  2.2595e-02, -2.2312e-02,  1.1737e-02, -4.4449e-03,\n",
       "          -1.4891e-02,  1.6509e-02, -1.7496e-02, -1.1086e-02,  5.7247e-03,\n",
       "          -2.3258e-02, -1.4932e-02,  9.4393e-03,  1.3355e-02,  3.0709e-03,\n",
       "           1.6911e-02,  2.2442e-02, -6.7407e-03, -1.0749e-02, -4.9746e-03,\n",
       "          -1.1123e-03,  2.5682e-02,  1.7754e-02,  1.1140e-02, -1.9514e-02,\n",
       "           2.0439e-02,  1.8733e-02, -2.2665e-03, -1.2248e-02,  1.5141e-02,\n",
       "          -2.7885e-02, -1.0042e-02,  3.1765e-03,  7.9902e-03, -2.3004e-02,\n",
       "           4.8168e-03,  2.5782e-02, -2.6526e-02,  1.6602e-02,  1.4250e-02,\n",
       "           8.7866e-03,  2.0099e-02, -8.0247e-03, -3.7586e-03, -1.8344e-02,\n",
       "          -1.9412e-02, -1.5757e-02, -2.0916e-02, -5.3253e-03,  1.4217e-02,\n",
       "           2.0856e-03, -2.9288e-03,  7.5048e-03,  3.1700e-03, -2.4551e-02,\n",
       "           2.9284e-03, -1.3639e-02,  1.3367e-02, -4.3262e-03,  1.0382e-03,\n",
       "          -1.6300e-02,  2.2993e-02,  2.8149e-03,  2.4228e-02,  2.0377e-02,\n",
       "          -1.5298e-02,  1.3812e-02,  1.5716e-02, -6.7372e-03,  1.1332e-02,\n",
       "          -2.3349e-02, -2.6412e-02, -1.3149e-02, -6.6866e-04, -2.2919e-02,\n",
       "          -6.3484e-03,  2.0536e-02,  9.6610e-03, -1.9046e-02, -1.8345e-02,\n",
       "          -3.4215e-03,  1.4336e-02, -5.0188e-03, -1.6768e-02, -6.3181e-03,\n",
       "           7.5358e-03, -1.1597e-02,  2.4172e-02,  1.5740e-02,  2.5067e-02,\n",
       "          -2.7334e-02,  2.7725e-02, -2.6958e-02,  1.5534e-02,  5.0823e-03,\n",
       "          -1.8053e-02,  1.8749e-02,  1.7593e-02,  2.0172e-02,  2.4275e-02,\n",
       "          -2.0310e-02,  1.1561e-02,  1.5512e-02,  2.1417e-02, -1.0532e-02,\n",
       "          -1.7881e-02, -1.0536e-02, -1.8922e-02,  1.5619e-02, -1.0997e-02,\n",
       "          -2.1598e-02,  2.6803e-03, -2.0980e-02, -5.9735e-03, -2.5916e-04,\n",
       "           1.3751e-02,  3.3465e-03,  2.6144e-02,  7.5886e-03,  1.1462e-02,\n",
       "           2.5365e-02, -1.1276e-02, -2.6181e-02,  6.5685e-04,  1.3943e-02,\n",
       "          -6.6014e-03,  1.0365e-02, -2.5216e-02,  1.9331e-03,  2.2466e-02,\n",
       "           9.1944e-03,  2.3956e-02, -1.6327e-02, -1.7447e-02,  4.2394e-03,\n",
       "          -4.5805e-03, -2.0668e-02,  9.3670e-03,  2.2520e-02, -1.8476e-03,\n",
       "          -1.4415e-02, -2.2703e-02,  6.7389e-03,  8.4066e-03,  1.8970e-02,\n",
       "           1.2785e-02, -2.7106e-02,  1.6056e-02,  6.1783e-04, -1.1309e-02,\n",
       "          -1.0598e-02, -1.8070e-02, -1.5541e-02,  2.7341e-02, -8.6130e-03,\n",
       "          -2.5934e-02, -1.7083e-02,  2.0173e-03, -1.5465e-02,  3.7385e-03,\n",
       "           1.4629e-02,  2.0922e-03, -5.6229e-03,  1.0764e-02,  2.9671e-04,\n",
       "           4.5872e-03, -1.1859e-02, -2.3175e-02,  9.1484e-03,  1.7423e-02,\n",
       "           1.7650e-02, -1.3901e-02, -1.6702e-02,  2.7257e-03,  1.6578e-02,\n",
       "           1.0786e-02, -2.3121e-02, -1.6636e-02,  2.7393e-02, -2.3629e-02,\n",
       "           2.4707e-02, -1.1294e-02,  2.5021e-02,  2.7343e-02, -9.7300e-03,\n",
       "           2.0326e-02, -1.2687e-02, -1.5903e-02,  1.7112e-02, -1.0452e-02,\n",
       "          -8.4389e-03,  3.4065e-03, -1.8238e-02, -2.6211e-02,  7.6511e-03,\n",
       "           2.5596e-03, -2.4487e-02, -4.4665e-04, -2.5100e-02,  6.2448e-03,\n",
       "          -1.6144e-02, -9.9879e-03,  9.2633e-03,  1.7111e-02,  1.6989e-02,\n",
       "          -1.0185e-02,  1.1771e-02, -2.3742e-02,  3.1592e-03,  3.9718e-03,\n",
       "           1.2390e-02,  2.3006e-02, -2.5246e-04, -3.9029e-03,  2.4946e-02,\n",
       "           1.5704e-03, -2.6221e-02,  2.4246e-02,  2.4444e-02,  2.5789e-02,\n",
       "          -1.3335e-02, -1.8797e-02,  9.4046e-03, -2.5615e-02, -9.6668e-03,\n",
       "           2.2920e-03,  9.3255e-03,  9.7623e-03, -1.0405e-02, -1.3456e-04,\n",
       "           1.5106e-03,  1.5557e-02,  8.8814e-03,  4.8328e-03,  1.6017e-02,\n",
       "          -2.6176e-02,  2.5564e-02,  3.3449e-03, -1.1232e-02,  1.0275e-02,\n",
       "           1.6189e-02, -2.6739e-02,  1.8574e-02, -6.8373e-03,  1.8719e-02,\n",
       "          -2.2310e-02, -1.5152e-02, -1.0578e-02,  8.4107e-03,  5.8557e-03,\n",
       "          -2.4781e-02,  7.0678e-03,  6.9650e-04,  2.7210e-02, -8.5400e-04,\n",
       "           1.1694e-02,  4.3443e-03,  2.4065e-02, -5.2851e-03,  1.9213e-02,\n",
       "          -1.3805e-02,  1.1547e-02,  7.8560e-03,  2.3688e-02, -4.7770e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0329,  0.0045,  0.0312,  ...,  0.0015, -0.0389,  0.0111],\n",
       "          [ 0.0001, -0.0034,  0.0127,  ..., -0.0046, -0.0363,  0.0039],\n",
       "          [ 0.0171,  0.0232, -0.0219,  ...,  0.0064, -0.0037, -0.0183],\n",
       "          ...,\n",
       "          [-0.0130, -0.0356, -0.0247,  ...,  0.0247,  0.0306, -0.0030],\n",
       "          [ 0.0054, -0.0312,  0.0193,  ...,  0.0292, -0.0276, -0.0046],\n",
       "          [-0.0231,  0.0380,  0.0079,  ...,  0.0299,  0.0354,  0.0306]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0173,  0.0025,  0.0280,  0.0152,  0.0034,  0.0188, -0.0124, -0.0305],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0222, -0.0017, -0.0203,  ..., -0.0296, -0.0276, -0.0005],\n",
       "          [ 0.0177,  0.0143, -0.0057,  ...,  0.0036,  0.0175,  0.0132],\n",
       "          [ 0.0192,  0.0084,  0.0118,  ..., -0.0011, -0.0282,  0.0003],\n",
       "          ...,\n",
       "          [ 0.0181, -0.0274, -0.0041,  ..., -0.0096,  0.0114,  0.0140],\n",
       "          [ 0.0161,  0.0008,  0.0234,  ..., -0.0069, -0.0197,  0.0156],\n",
       "          [-0.0040,  0.0188,  0.0071,  ...,  0.0037, -0.0013,  0.0375]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0138,  0.0047,  0.0118,  ..., -0.0108,  0.0028, -0.0149],\n",
       "          [-0.0151,  0.0179,  0.0164,  ..., -0.0056, -0.0121, -0.0064],\n",
       "          [-0.0061,  0.0009,  0.0228,  ..., -0.0107,  0.0125,  0.0107],\n",
       "          ...,\n",
       "          [-0.0164,  0.0074,  0.0133,  ...,  0.0182,  0.0019,  0.0101],\n",
       "          [-0.0118,  0.0067,  0.0305,  ...,  0.0148, -0.0081,  0.0001],\n",
       "          [ 0.0010,  0.0032, -0.0244,  ..., -0.0069, -0.0255, -0.0120]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0045,  0.0126, -0.0066,  ...,  0.0020,  0.0074,  0.0009],\n",
       "          [-0.0024, -0.0121,  0.0198,  ...,  0.0139,  0.0002, -0.0178],\n",
       "          [-0.0140, -0.0240,  0.0028,  ...,  0.0044, -0.0170,  0.0189],\n",
       "          ...,\n",
       "          [ 0.0024,  0.0005, -0.0123,  ...,  0.0063, -0.0013, -0.0119],\n",
       "          [ 0.0237, -0.0052,  0.0170,  ..., -0.0027,  0.0111,  0.0180],\n",
       "          [-0.0356,  0.0256, -0.0041,  ...,  0.0023,  0.0168,  0.0024]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0240, -0.0304, -0.0123,  ..., -0.0219, -0.0182, -0.0119],\n",
       "          [-0.0007, -0.0034,  0.0108,  ..., -0.0084,  0.0124, -0.0005],\n",
       "          [ 0.0014, -0.0128, -0.0208,  ...,  0.0018,  0.0187,  0.0131],\n",
       "          ...,\n",
       "          [-0.0253, -0.0126, -0.0033,  ..., -0.0040,  0.0040,  0.0016],\n",
       "          [ 0.0269,  0.0103, -0.0041,  ..., -0.0294,  0.0015, -0.0042],\n",
       "          [-0.0015, -0.0186, -0.0098,  ..., -0.0205, -0.0122, -0.0122]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0200,  0.0184,  0.0085,  ...,  0.0063, -0.0188,  0.0264],\n",
       "          [ 0.0059,  0.0195,  0.0047,  ...,  0.0198, -0.0182, -0.0211],\n",
       "          [ 0.0273,  0.0222,  0.0265,  ..., -0.0146,  0.0217, -0.0084],\n",
       "          ...,\n",
       "          [-0.0010,  0.0011, -0.0240,  ...,  0.0274, -0.0196,  0.0054],\n",
       "          [ 0.0108, -0.0144,  0.0066,  ...,  0.0056, -0.0186,  0.0013],\n",
       "          [-0.0152,  0.0199,  0.0093,  ..., -0.0073,  0.0240, -0.0144]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 1.9697e-02, -1.7489e-02,  2.5937e-02,  1.9733e-02, -1.3384e-02,\n",
       "          -3.2323e-03,  1.0085e-02, -2.4140e-02, -2.0438e-03,  5.0881e-03,\n",
       "           1.9797e-02,  2.6093e-02,  2.6070e-02,  2.5220e-02,  4.2997e-03,\n",
       "           1.4527e-02,  9.0698e-03,  1.4867e-02,  2.4673e-03, -1.4569e-02,\n",
       "          -5.9656e-03, -1.5074e-02, -2.7601e-02,  1.8272e-02, -1.6727e-02,\n",
       "          -1.2973e-02, -2.4396e-02,  2.3420e-02,  6.3347e-03,  1.2492e-02,\n",
       "          -9.3588e-03, -7.2940e-03, -1.7463e-02, -3.3140e-03,  1.1020e-03,\n",
       "           2.6388e-02,  5.8838e-03,  1.0741e-02, -2.7119e-02, -1.1549e-02,\n",
       "           8.1387e-03, -1.5155e-02, -1.2123e-02,  1.9554e-02,  1.7047e-02,\n",
       "          -1.2888e-02,  1.5628e-03, -2.7238e-02,  5.6317e-05,  2.6437e-02,\n",
       "          -2.7085e-02, -1.9151e-02, -2.1607e-02,  2.3817e-02,  1.1604e-02,\n",
       "          -2.7351e-02,  2.2846e-02, -2.1166e-02, -2.1732e-02,  5.0503e-04,\n",
       "           1.2790e-02, -8.4558e-03, -1.2482e-02,  7.5967e-03,  4.9610e-03,\n",
       "           1.4328e-03, -1.4071e-02, -1.1731e-02, -7.7499e-03, -1.7168e-02,\n",
       "          -6.3487e-03,  1.1576e-03, -1.9316e-02,  2.5138e-02, -1.0667e-02,\n",
       "          -7.4261e-03, -1.0923e-02,  1.3851e-02,  2.1270e-02, -1.2428e-03,\n",
       "           1.6873e-02, -9.4368e-03, -6.2213e-03, -2.8704e-03,  2.5884e-02,\n",
       "           1.8165e-02,  6.5343e-03, -9.3648e-04, -1.6682e-02,  1.3850e-04,\n",
       "           2.2063e-02, -2.6831e-02,  1.2611e-02, -1.7843e-02, -1.9720e-02,\n",
       "          -8.5064e-03, -5.7643e-03,  3.4235e-03,  1.1491e-02,  1.8574e-02,\n",
       "           1.3729e-02,  6.2903e-03, -5.1918e-03,  1.9290e-02, -1.1670e-02,\n",
       "           7.7638e-03,  2.5416e-02,  1.8191e-02,  1.2464e-02, -2.4618e-02,\n",
       "          -8.3310e-03,  8.1819e-03,  3.7737e-03,  1.0801e-02, -1.6792e-02,\n",
       "          -2.7368e-03,  1.4580e-03,  1.1363e-02, -1.8285e-02, -2.6566e-02,\n",
       "           1.0919e-02,  7.0652e-03, -1.4366e-02,  9.0779e-03, -2.3603e-02,\n",
       "          -2.5045e-02,  1.4962e-02,  2.6885e-02, -2.4379e-02,  2.3558e-02,\n",
       "           5.5479e-03, -1.8581e-02,  2.4891e-02,  2.1797e-02, -1.3554e-02,\n",
       "          -2.4163e-02, -1.2836e-02,  1.5643e-02, -1.8164e-02,  2.2235e-02,\n",
       "           4.3699e-03,  3.7236e-03, -7.6460e-04, -9.7098e-03, -2.6275e-02,\n",
       "          -1.7151e-03,  2.0786e-02,  1.8815e-02, -5.0091e-03,  1.1720e-03,\n",
       "          -1.0751e-02,  1.8256e-02,  2.4776e-02, -5.0969e-03,  7.5138e-03,\n",
       "           2.6271e-02,  2.7540e-02,  1.8165e-02,  2.5820e-02,  2.7706e-02,\n",
       "           4.4652e-03,  8.8126e-03, -2.6173e-02,  2.7123e-02,  9.1419e-03,\n",
       "           2.7487e-02,  2.7784e-02,  2.4060e-02, -1.9185e-02, -2.7926e-02,\n",
       "          -2.1155e-02, -1.2298e-02, -4.3629e-03,  2.1292e-02,  5.9150e-04,\n",
       "           2.7307e-02, -1.1989e-02, -9.8513e-03, -2.6519e-02,  2.2932e-02,\n",
       "          -1.8528e-02, -1.8428e-02,  2.6755e-02,  1.0531e-02, -5.1167e-03,\n",
       "          -3.2471e-03, -2.7530e-02,  2.7349e-02,  2.5957e-02, -6.4185e-03,\n",
       "          -2.1138e-02, -2.3598e-02, -2.7842e-02,  5.1775e-03, -1.6797e-02,\n",
       "           3.5013e-05,  1.4341e-02,  7.4616e-03, -8.6365e-05, -2.7844e-02,\n",
       "           1.7391e-02,  2.7288e-02, -1.3406e-04,  3.5674e-03,  1.3708e-02,\n",
       "          -2.5287e-02,  2.7131e-02, -1.5722e-02, -1.3144e-02, -2.4907e-03,\n",
       "           9.6435e-03,  1.0003e-03,  9.8452e-04,  1.9258e-02, -2.5374e-02,\n",
       "           1.0005e-02,  2.4398e-02,  2.0524e-02,  5.3932e-03, -2.4721e-02,\n",
       "          -1.6563e-02, -1.0228e-02, -1.0228e-02, -2.5121e-02, -2.2352e-02,\n",
       "          -5.1013e-04, -1.6015e-02, -2.0255e-02,  2.6721e-02, -9.2291e-03,\n",
       "          -2.0231e-02, -3.3122e-03,  1.5195e-02,  2.3399e-02, -6.0173e-04,\n",
       "           7.4983e-03,  1.6308e-02, -7.5147e-03,  1.1963e-02, -2.7683e-02,\n",
       "           2.4997e-02, -1.6184e-02, -1.1703e-02, -4.7367e-03, -4.0412e-03,\n",
       "          -9.2999e-03,  2.6367e-02,  1.9413e-02, -1.5207e-02,  2.6535e-02,\n",
       "           6.1352e-03, -4.3213e-03,  4.8907e-03, -9.3300e-03,  1.3274e-02,\n",
       "          -2.7227e-02,  2.3682e-02, -6.8906e-04,  1.3762e-02, -1.6347e-02,\n",
       "           3.2347e-03,  8.1458e-03,  2.0496e-02, -2.4974e-02, -1.4443e-02,\n",
       "           2.2619e-02, -2.1283e-02,  2.6996e-02,  2.6578e-02,  2.6099e-02,\n",
       "          -1.5265e-02, -1.4066e-02, -2.3498e-02,  2.1565e-02,  1.7774e-02,\n",
       "          -2.5278e-02, -1.5354e-02, -2.2220e-02, -2.9994e-03, -5.9796e-03,\n",
       "           1.7010e-03, -1.4376e-02,  1.8245e-02, -1.3858e-02, -4.6133e-03,\n",
       "          -1.4621e-02, -8.8032e-03,  2.2974e-02,  8.9133e-03,  6.8382e-03,\n",
       "          -6.7374e-03, -2.6845e-02,  9.9245e-03,  4.9363e-03,  1.5286e-02,\n",
       "          -1.7790e-02, -1.7302e-02, -6.6239e-03, -1.8640e-02, -2.5973e-02,\n",
       "           2.4636e-02, -9.7838e-03, -8.8862e-03, -1.4311e-02, -2.7246e-02,\n",
       "          -2.0554e-02,  1.7451e-02,  8.8410e-03, -1.1661e-02,  2.5013e-02,\n",
       "          -3.9854e-03, -2.8769e-03, -1.7505e-02,  1.4705e-02,  5.3593e-03,\n",
       "           1.5785e-02, -4.1140e-03, -8.6663e-03, -7.7618e-03,  1.0471e-03,\n",
       "          -2.7130e-02, -2.4375e-02, -3.2795e-03, -9.9653e-03,  6.2195e-03,\n",
       "           2.6021e-02,  1.3022e-02,  9.3740e-03, -1.6753e-02,  6.7430e-03,\n",
       "           2.6575e-02, -2.1720e-04,  2.2429e-02, -1.7608e-03, -8.5453e-03,\n",
       "          -2.4792e-02, -1.8234e-02, -3.4035e-03, -3.8041e-03,  3.3841e-03,\n",
       "          -8.7551e-03, -2.4869e-02,  5.6529e-03, -3.4315e-04, -1.8386e-02,\n",
       "          -2.3634e-02, -2.5722e-02, -4.4606e-03,  5.6567e-03,  6.3166e-04,\n",
       "           2.4795e-02, -5.8897e-03, -7.4014e-03, -2.6717e-02,  1.4729e-02,\n",
       "           9.5992e-03, -6.3680e-03, -1.9103e-02,  1.4418e-02, -8.9065e-03,\n",
       "           1.0122e-02,  1.7273e-02,  1.9565e-02, -1.3621e-02, -2.3844e-02,\n",
       "           2.6109e-02,  2.2465e-02,  2.1850e-02, -1.5386e-02,  8.1647e-03,\n",
       "           7.0100e-03, -2.7778e-02,  1.1002e-02, -4.3157e-04, -1.3524e-02,\n",
       "           1.4619e-02,  1.8846e-02, -1.0005e-02,  1.0378e-02,  4.9745e-03,\n",
       "          -9.4353e-03,  1.0814e-02, -2.0130e-02, -1.6299e-02,  1.5111e-02,\n",
       "          -2.0366e-03, -4.5312e-05, -2.4922e-02,  6.4366e-03,  4.3192e-03,\n",
       "           1.1729e-02,  3.6542e-03,  3.2072e-03, -4.6152e-03, -1.6412e-02,\n",
       "           2.3640e-02, -1.7145e-02, -6.1197e-04,  1.6076e-02, -2.0950e-02,\n",
       "           2.6613e-02,  1.1551e-02,  1.7352e-02, -1.1153e-02,  2.3668e-02,\n",
       "          -1.2285e-02,  6.2055e-03, -2.0642e-02, -2.4286e-02,  1.1501e-02,\n",
       "           1.0302e-02,  2.4228e-02,  1.4768e-02,  6.3041e-04,  8.0226e-03,\n",
       "          -8.9590e-03,  9.0977e-03, -2.1421e-02,  3.3277e-03,  2.5042e-02,\n",
       "           5.9464e-03, -2.3591e-02, -1.1540e-02, -4.9121e-03,  1.6125e-02,\n",
       "           1.7983e-03,  2.0572e-02, -5.6630e-03,  2.3619e-02, -4.4583e-03,\n",
       "          -1.2319e-02, -5.8381e-03,  2.5451e-02,  5.2778e-03,  2.5074e-03,\n",
       "           8.3910e-03, -2.2364e-02, -2.5053e-02, -3.9884e-04, -2.5810e-02,\n",
       "          -2.0838e-02, -2.7225e-02,  3.5069e-03,  1.7832e-02,  2.3658e-02,\n",
       "          -2.3693e-02, -9.3286e-03, -2.7570e-02,  1.5331e-02, -1.5402e-03,\n",
       "           1.0070e-02,  2.3018e-03,  2.6466e-02, -2.1728e-02,  6.7531e-03,\n",
       "           1.5215e-02,  1.9883e-03, -2.3800e-02,  1.2945e-02, -1.5398e-02,\n",
       "          -2.1629e-02,  1.2299e-03,  1.0165e-02,  1.7668e-02,  2.0580e-02,\n",
       "          -2.0459e-03, -2.2870e-02,  1.6415e-02,  1.9291e-02,  2.4124e-02,\n",
       "          -2.3388e-02,  1.5819e-03, -5.2871e-03,  1.4234e-02, -4.3741e-03,\n",
       "           1.7895e-03, -2.4675e-02,  3.7168e-03,  8.8463e-03, -2.2907e-02,\n",
       "           2.1899e-02,  1.7600e-02, -3.4664e-03,  1.6888e-02,  1.4253e-02,\n",
       "          -1.3420e-02,  5.0903e-03, -9.0653e-03,  2.0229e-02, -8.5103e-03,\n",
       "           3.2008e-04,  1.3293e-02, -2.0414e-02, -3.4883e-03, -1.8420e-02,\n",
       "           3.2357e-03, -2.1672e-03, -7.1316e-03, -2.7552e-02,  6.4349e-03,\n",
       "           3.5046e-03, -1.0722e-02,  8.2107e-03,  1.8413e-02, -2.6336e-02,\n",
       "          -2.7278e-02,  1.6932e-02, -1.7908e-02, -1.0586e-02,  2.6745e-02,\n",
       "           1.5914e-02,  1.6922e-02, -2.0571e-02,  2.0200e-02, -2.6047e-02,\n",
       "           1.5955e-02, -1.4964e-02,  2.0537e-02, -7.9904e-03,  4.8514e-03,\n",
       "          -1.6782e-02, -1.1301e-03,  1.1344e-02,  1.9381e-02,  7.0309e-03,\n",
       "           1.4440e-02, -2.3998e-02,  2.7395e-02, -9.1719e-03,  1.6507e-02,\n",
       "           2.1683e-02,  6.9502e-03, -7.7830e-03,  9.2459e-03, -2.6102e-02,\n",
       "           1.1678e-02,  6.4570e-04, -1.9323e-02, -5.7826e-03,  2.5678e-02,\n",
       "           2.7908e-02, -2.6517e-02,  5.3920e-03, -1.5507e-02,  2.7283e-02,\n",
       "          -8.4864e-03,  9.4743e-03, -2.2440e-02, -9.5085e-03,  1.7733e-02,\n",
       "          -1.6945e-03,  1.1002e-02, -2.3477e-02, -1.1880e-02,  1.4436e-02,\n",
       "           2.0208e-03,  2.6021e-02, -5.4589e-03, -1.9819e-02,  1.0049e-02,\n",
       "          -5.6221e-03,  2.0583e-02, -2.6767e-03, -2.3599e-02, -4.8080e-03,\n",
       "           8.5038e-04,  1.4532e-02,  9.8829e-03,  1.1612e-02,  2.4886e-02,\n",
       "           2.0892e-02, -1.2212e-02,  1.6653e-02, -2.0473e-02, -1.7570e-02,\n",
       "           8.3776e-03,  5.8603e-03,  3.0467e-03,  2.6433e-02,  2.3797e-02,\n",
       "           2.2365e-02,  5.1324e-03, -8.5658e-03,  9.5157e-03,  1.9911e-02,\n",
       "           6.4640e-03, -2.0212e-02,  1.9233e-02, -1.0288e-02, -2.2593e-02,\n",
       "          -1.9114e-02, -5.6537e-03,  9.7847e-03, -8.2439e-03,  2.3802e-02,\n",
       "          -1.7664e-02, -1.6803e-02,  8.7525e-03, -1.6518e-02, -1.3593e-02,\n",
       "          -1.2246e-02, -2.4785e-02,  8.8616e-03,  1.4213e-02, -1.5283e-02,\n",
       "           7.8301e-03, -9.3494e-03, -2.3387e-02,  8.8063e-03, -1.4951e-02,\n",
       "           1.5794e-03,  8.1422e-03, -5.6059e-03,  4.2594e-03, -3.4584e-03,\n",
       "          -7.7098e-03, -8.5625e-03, -1.5462e-02,  1.7886e-03,  4.3249e-03,\n",
       "          -8.9393e-03,  2.0034e-02, -1.6244e-02, -3.3002e-03, -1.7169e-02,\n",
       "           7.1926e-03,  8.1272e-04, -1.2726e-02,  7.4229e-03, -7.2986e-03,\n",
       "          -2.2121e-02,  1.0750e-02, -1.4275e-03, -1.8823e-03, -2.0488e-02,\n",
       "           2.7315e-02,  1.2916e-02, -1.6964e-02, -1.1184e-03,  1.0946e-02],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0220,  0.0073, -0.0345,  ..., -0.0257,  0.0361, -0.0223],\n",
       "          [ 0.0261,  0.0322, -0.0035,  ...,  0.0242, -0.0188,  0.0083],\n",
       "          [ 0.0108,  0.0390, -0.0057,  ...,  0.0192,  0.0029, -0.0037],\n",
       "          ...,\n",
       "          [ 0.0185, -0.0353, -0.0072,  ..., -0.0287, -0.0145,  0.0018],\n",
       "          [-0.0159, -0.0393, -0.0290,  ...,  0.0380, -0.0104,  0.0198],\n",
       "          [ 0.0360,  0.0076,  0.0141,  ...,  0.0264, -0.0078,  0.0244]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0131, -0.0055,  0.0216, -0.0371, -0.0109,  0.0272,  0.0047,  0.0214],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0049,  0.0072,  0.0149,  ...,  0.0159, -0.0060,  0.0057],\n",
       "          [ 0.0063, -0.0050, -0.0001,  ..., -0.0034,  0.0120,  0.0076],\n",
       "          [ 0.0175,  0.0260,  0.0073,  ..., -0.0079, -0.0120, -0.0043],\n",
       "          ...,\n",
       "          [ 0.0212,  0.0149, -0.0224,  ..., -0.0190,  0.0059, -0.0075],\n",
       "          [-0.0234, -0.0120,  0.0313,  ...,  0.0064,  0.0292, -0.0160],\n",
       "          [-0.0171, -0.0152,  0.0055,  ...,  0.0153, -0.0175,  0.0143]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0058,  0.0179, -0.0051,  ..., -0.0043, -0.0148, -0.0045],\n",
       "          [-0.0099, -0.0177,  0.0123,  ...,  0.0009, -0.0012,  0.0113],\n",
       "          [-0.0196, -0.0122, -0.0135,  ...,  0.0166, -0.0075, -0.0229],\n",
       "          ...,\n",
       "          [-0.0110,  0.0167, -0.0251,  ...,  0.0129,  0.0274, -0.0259],\n",
       "          [ 0.0162,  0.0069,  0.0004,  ..., -0.0108,  0.0228, -0.0069],\n",
       "          [ 0.0161,  0.0117,  0.0048,  ..., -0.0098,  0.0011,  0.0198]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0050,  0.0069,  0.0024,  ..., -0.0119,  0.0120,  0.0112],\n",
       "          [ 0.0044,  0.0130,  0.0116,  ..., -0.0296, -0.0104, -0.0108],\n",
       "          [ 0.0026,  0.0107, -0.0120,  ..., -0.0073, -0.0062, -0.0015],\n",
       "          ...,\n",
       "          [-0.0126,  0.0051,  0.0434,  ..., -0.0081, -0.0106, -0.0018],\n",
       "          [-0.0176,  0.0105, -0.0052,  ..., -0.0171,  0.0203, -0.0182],\n",
       "          [ 0.0094, -0.0252, -0.0096,  ..., -0.0002, -0.0245,  0.0008]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0054, -0.0044,  0.0118,  ..., -0.0261, -0.0209, -0.0042],\n",
       "          [ 0.0155,  0.0098, -0.0170,  ..., -0.0115,  0.0111,  0.0009],\n",
       "          [-0.0100,  0.0069, -0.0068,  ...,  0.0196,  0.0032, -0.0261],\n",
       "          ...,\n",
       "          [-0.0136,  0.0202, -0.0031,  ...,  0.0012,  0.0183,  0.0110],\n",
       "          [-0.0371, -0.0222, -0.0120,  ...,  0.0139, -0.0001, -0.0193],\n",
       "          [ 0.0111, -0.0116, -0.0250,  ..., -0.0151,  0.0047,  0.0152]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-3.4879e-04, -1.9896e-04,  2.0370e-02,  ...,  2.7564e-02,\n",
       "           -2.3010e-02,  1.5727e-02],\n",
       "          [ 1.0601e-03,  4.6988e-03,  9.1498e-03,  ..., -2.3482e-02,\n",
       "           -2.0868e-02, -1.9817e-02],\n",
       "          [-1.1565e-02,  4.3699e-03,  1.1575e-02,  ..., -2.6393e-02,\n",
       "            1.8291e-02,  1.9321e-02],\n",
       "          ...,\n",
       "          [-1.4555e-02, -2.4173e-03, -1.4565e-02,  ...,  7.9102e-05,\n",
       "           -1.0107e-02,  2.6739e-02],\n",
       "          [ 1.2524e-02, -2.5236e-02, -2.1428e-02,  ..., -1.1523e-02,\n",
       "            2.3118e-02, -1.4744e-02],\n",
       "          [ 2.5848e-02,  3.3316e-04,  1.6527e-02,  ...,  2.3316e-02,\n",
       "            2.4569e-02, -1.5105e-02]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.3270e-02,  2.6580e-02, -2.7999e-03, -2.6369e-02,  6.8176e-03,\n",
       "           1.4806e-02,  1.7351e-02, -2.2651e-02,  2.1735e-02, -2.0079e-02,\n",
       "           3.7459e-03,  8.3533e-03, -2.1961e-02, -2.2895e-02,  1.6410e-02,\n",
       "          -4.6493e-03,  2.5887e-02, -1.2520e-02, -7.0792e-04,  1.1060e-02,\n",
       "          -7.2421e-03, -2.3190e-02, -2.0327e-02,  1.4727e-02,  1.1839e-02,\n",
       "          -7.8950e-03,  9.5413e-03, -2.2255e-02, -2.3990e-02, -1.5559e-02,\n",
       "          -2.0750e-02,  2.2723e-02, -2.6432e-02,  6.3948e-03,  6.3954e-03,\n",
       "          -6.1432e-05, -1.9583e-02, -1.5831e-02,  1.1844e-02, -9.8584e-03,\n",
       "          -8.4670e-03, -1.8126e-02, -2.7111e-02, -7.0938e-03, -1.1323e-02,\n",
       "          -2.3689e-02, -7.0423e-03,  1.0762e-02,  2.6121e-04, -8.3391e-03,\n",
       "           4.1147e-03, -2.2243e-03,  1.5027e-02, -1.7489e-02, -9.5862e-06,\n",
       "          -1.9052e-02,  2.0293e-02, -1.8654e-02, -1.4925e-02, -9.7647e-03,\n",
       "          -5.4362e-03, -1.4338e-02,  2.4303e-02, -1.0171e-02,  3.9461e-03,\n",
       "           2.4073e-03, -2.2484e-04,  2.3760e-02,  1.9376e-02, -2.0961e-02,\n",
       "          -2.5325e-02,  1.0458e-02,  2.3783e-02,  2.1787e-02,  2.1837e-02,\n",
       "          -1.8314e-02, -1.7256e-02,  2.5815e-02,  2.3938e-02, -3.6963e-03,\n",
       "          -1.4377e-02,  5.1454e-03, -1.7659e-02,  1.3216e-02,  2.4579e-02,\n",
       "          -7.9472e-03,  2.3124e-02, -1.4603e-02,  1.3478e-02, -1.7472e-02,\n",
       "           1.4666e-03, -1.3273e-02, -1.9963e-02,  2.1723e-02,  5.8342e-03,\n",
       "          -2.2542e-02, -2.0306e-04,  8.7542e-03, -2.0895e-03, -9.6346e-03,\n",
       "          -8.9405e-03,  2.7829e-02,  5.6948e-03, -1.8966e-02,  1.6168e-02,\n",
       "          -1.2123e-02,  1.9918e-02, -1.1645e-02, -1.3908e-02, -1.0457e-02,\n",
       "          -1.8930e-02,  2.2735e-02, -1.8973e-02, -1.5703e-02, -6.5797e-03,\n",
       "          -2.3781e-02,  2.0147e-02, -7.1572e-03,  1.5671e-02, -9.6251e-03,\n",
       "           6.2975e-03,  2.0237e-02,  1.2252e-02,  6.2221e-03, -1.1094e-02,\n",
       "          -1.8505e-02,  3.2652e-03, -2.6687e-02,  3.8300e-03, -1.0342e-02,\n",
       "          -4.4866e-03, -5.9033e-03,  2.0673e-02, -2.0079e-02, -1.7853e-02,\n",
       "           6.2882e-03, -2.0366e-02,  2.2705e-02,  1.0179e-02,  9.4480e-03,\n",
       "          -2.4401e-02, -8.9943e-03,  2.2062e-02,  3.6828e-03, -2.6627e-02,\n",
       "          -2.3440e-02, -1.6085e-02,  6.1977e-03,  5.2614e-03,  2.0616e-02,\n",
       "          -1.5615e-02,  2.4925e-02,  1.6218e-02,  1.2123e-02,  1.2349e-02,\n",
       "           3.3412e-03,  2.2837e-02, -1.3502e-02, -1.2422e-03, -6.6708e-03,\n",
       "          -1.4348e-02,  5.6381e-03, -1.1080e-02,  3.5975e-03, -1.8135e-02,\n",
       "          -2.1179e-02,  1.6037e-02, -9.4094e-03,  2.6878e-02,  6.8376e-03,\n",
       "           1.6019e-02,  1.4008e-02,  8.9760e-03, -2.2849e-02,  5.9529e-03,\n",
       "           1.8241e-02, -2.5321e-02, -1.0367e-02, -7.0168e-03, -9.6943e-03,\n",
       "           9.9690e-04, -1.6923e-02,  2.1452e-02, -6.5017e-03,  9.3280e-03,\n",
       "           1.5074e-02,  2.6125e-02, -1.6575e-02, -1.8623e-02,  2.3819e-03,\n",
       "          -2.6761e-03, -1.2694e-02,  2.3176e-02,  1.6586e-02, -7.1382e-03,\n",
       "           2.6059e-02, -1.9983e-02, -2.6834e-02,  1.9434e-02, -9.5239e-03,\n",
       "          -1.7150e-02, -2.4934e-02,  1.2749e-02,  2.3772e-03,  2.2961e-02,\n",
       "           9.9625e-03, -1.1179e-02,  1.6574e-02, -5.7744e-03, -5.2899e-03,\n",
       "          -2.5068e-03, -1.4114e-02, -9.5749e-04, -6.6800e-03, -1.4610e-02,\n",
       "          -1.7913e-02, -1.2209e-02,  9.8782e-03, -7.7250e-03, -1.5512e-02,\n",
       "          -2.0620e-02, -2.3980e-03,  2.4340e-02,  1.0395e-03,  2.6391e-02,\n",
       "          -1.1776e-02,  2.2655e-02, -1.6740e-02, -2.7322e-02,  2.4725e-03,\n",
       "          -2.6440e-02,  8.2151e-03, -1.0520e-02,  2.3067e-03,  1.7988e-03,\n",
       "           1.2170e-02, -1.2508e-02, -8.2742e-03, -2.4171e-02, -1.8608e-02,\n",
       "           2.7415e-02,  1.2250e-02, -4.1548e-03,  1.2822e-02, -2.3851e-03,\n",
       "           1.0733e-02,  7.3621e-03,  5.8068e-03,  2.2848e-02, -7.5870e-03,\n",
       "           3.8427e-03, -1.5776e-02, -2.7112e-02,  2.1565e-02,  1.8920e-02,\n",
       "          -1.8996e-02,  2.3178e-02,  1.1761e-02,  2.4167e-02,  2.4726e-02,\n",
       "          -9.2271e-03, -5.8018e-03, -2.6743e-02, -1.8036e-03,  8.6842e-03,\n",
       "           1.0924e-02, -1.8590e-02,  2.5015e-02,  1.8641e-02, -2.3879e-02,\n",
       "           1.2075e-03,  1.0889e-02, -5.2906e-03, -2.3752e-02,  7.7556e-03,\n",
       "           1.1436e-02,  2.0828e-02,  1.7856e-02, -1.6546e-02,  2.1093e-02,\n",
       "          -1.1399e-02, -1.5342e-02, -2.2441e-02, -1.0725e-02, -1.5407e-02,\n",
       "          -2.5849e-02,  1.5245e-02,  2.6447e-03,  4.9072e-03, -1.8900e-02,\n",
       "          -1.9646e-03, -9.3618e-03,  2.1597e-04, -2.0048e-02, -7.7198e-03,\n",
       "          -8.0211e-03,  1.7619e-02, -1.0609e-03,  2.4165e-02,  1.9917e-02,\n",
       "          -9.0932e-03,  1.2547e-02,  1.2653e-02, -2.4455e-02,  1.2758e-02,\n",
       "          -4.0110e-04,  1.0355e-02, -1.5781e-02,  2.3913e-02, -8.8267e-03,\n",
       "          -9.7851e-03, -9.7652e-03, -1.9157e-02,  1.6952e-02,  2.4276e-02,\n",
       "          -4.1755e-03, -2.0611e-02, -5.8055e-03, -2.3245e-02,  2.0096e-02,\n",
       "          -1.9918e-02,  1.6538e-02, -9.6503e-03,  8.5501e-03, -1.5504e-02,\n",
       "           2.5620e-04,  1.1648e-02, -4.6001e-03,  9.0867e-03,  2.3354e-02,\n",
       "           9.7692e-03,  1.2344e-02,  3.7145e-03,  1.7044e-02, -1.4485e-02,\n",
       "           2.1841e-03,  2.2434e-02,  4.1130e-03, -1.6651e-02,  1.6752e-02,\n",
       "          -2.5988e-02,  2.6512e-02,  8.1270e-03,  5.5913e-03, -4.2347e-03,\n",
       "          -6.1301e-04,  1.3294e-02,  1.4778e-02, -2.6433e-02,  1.3621e-02,\n",
       "          -1.9069e-02,  1.3587e-02, -9.4016e-03, -6.3964e-04,  2.4600e-03,\n",
       "          -2.0353e-03,  2.1807e-02, -2.4765e-02, -2.1904e-02, -1.0031e-02,\n",
       "          -5.0251e-03, -1.4668e-02,  8.7475e-03, -1.7006e-02, -1.4899e-02,\n",
       "           5.7291e-03, -1.4105e-03,  1.5106e-02,  1.9145e-02, -1.2597e-02,\n",
       "           2.6896e-04, -1.5452e-02,  2.0427e-02, -2.4163e-02, -2.5061e-02,\n",
       "           4.5704e-03,  1.2489e-02, -3.3972e-03,  2.0115e-02,  2.5493e-02,\n",
       "          -2.1930e-02,  3.2417e-03, -1.7145e-02, -2.6777e-03, -1.3326e-02,\n",
       "          -8.4170e-03,  1.8226e-02, -1.1557e-02, -1.7681e-02, -2.1810e-02,\n",
       "           5.4249e-03,  1.3326e-02,  2.1952e-02, -1.1791e-02,  3.1145e-03,\n",
       "           1.7901e-02, -2.0398e-02, -2.2015e-02, -2.0228e-02, -2.6051e-02,\n",
       "          -4.5250e-04, -6.9843e-03, -1.0293e-02, -2.5484e-02,  2.2177e-02,\n",
       "          -7.2334e-03, -2.6847e-02, -1.8864e-02, -2.4034e-02,  1.5231e-02,\n",
       "          -1.6136e-03,  8.1020e-03, -1.5270e-02,  2.6283e-02, -2.1762e-02,\n",
       "           2.4657e-02,  1.9124e-02,  2.5566e-02,  3.2549e-03, -1.7841e-02,\n",
       "           1.7878e-02,  2.5342e-02,  1.6567e-02, -7.8496e-03,  5.3903e-03,\n",
       "          -6.2192e-03,  1.9229e-02,  1.3970e-02, -2.5892e-02, -1.1459e-02,\n",
       "          -6.1223e-03,  1.5449e-02,  2.6515e-02,  2.5928e-02, -8.3325e-03,\n",
       "          -2.6190e-02,  1.2418e-02, -1.2528e-02,  1.9394e-02,  6.7961e-03,\n",
       "           1.5442e-02, -1.0037e-02, -3.0728e-03,  4.0883e-03,  3.2865e-03,\n",
       "           1.2439e-02,  2.7090e-02,  1.9995e-02, -8.0011e-03,  5.3383e-03,\n",
       "           6.3236e-03,  1.1012e-02, -1.6712e-02,  2.6684e-02,  1.8700e-03,\n",
       "          -1.6705e-02, -5.2767e-03,  1.6842e-02,  2.1909e-02,  9.5517e-03,\n",
       "           1.2827e-02, -6.6081e-03,  1.0828e-02,  2.0793e-02,  1.2931e-02,\n",
       "          -1.1805e-02, -1.1374e-02, -2.3520e-02,  2.4680e-02, -1.1716e-02,\n",
       "           2.6875e-02, -1.8259e-02,  2.4569e-02, -7.3212e-03, -1.1395e-02,\n",
       "           2.4664e-02, -1.9335e-02,  1.4559e-02,  1.2694e-02,  2.5325e-02,\n",
       "           7.0257e-03, -1.7732e-02,  1.3752e-03,  1.9402e-03, -1.8405e-02,\n",
       "          -1.4917e-02,  3.4519e-03, -1.4020e-02, -5.5202e-03, -1.1204e-02,\n",
       "          -6.1636e-03,  2.6608e-02,  2.0362e-02,  7.5710e-03, -1.5809e-04,\n",
       "           1.8031e-02,  2.0028e-02,  6.7437e-03, -7.7646e-04, -2.5597e-02,\n",
       "           2.4600e-02, -5.1570e-03,  2.6309e-03, -1.8467e-02,  3.3457e-03,\n",
       "          -1.5146e-02, -1.6132e-02,  6.2238e-03,  8.7171e-03,  1.8351e-02,\n",
       "          -2.6891e-02, -1.2270e-02,  1.4135e-02,  1.8413e-03,  1.7821e-02,\n",
       "           1.3391e-02, -8.7516e-03,  1.0097e-02, -1.0500e-04, -2.6542e-02,\n",
       "           2.6307e-02, -1.9349e-02,  2.8119e-03,  2.6554e-02, -2.1749e-02,\n",
       "           1.6178e-02,  1.2324e-02, -6.9859e-03,  1.5099e-02,  1.0212e-02,\n",
       "           2.7695e-02, -2.5471e-02,  1.1082e-02,  2.3971e-02, -3.2848e-03,\n",
       "          -6.8055e-03,  2.0061e-02,  1.7495e-02, -1.8828e-02,  2.6207e-02,\n",
       "           2.0617e-02,  1.6741e-02,  2.5495e-02,  1.2336e-03, -1.3299e-03,\n",
       "          -8.6316e-03,  1.9433e-02, -2.1263e-02, -3.4016e-03,  7.4004e-04,\n",
       "           1.7034e-02,  2.7487e-02, -3.9480e-03, -1.2757e-02, -2.9951e-03,\n",
       "          -1.3891e-02,  1.5805e-02, -9.4609e-04,  1.8943e-02, -4.6483e-03,\n",
       "           3.7168e-03, -1.9881e-02,  2.5175e-02,  8.1018e-03, -8.7816e-03,\n",
       "          -2.5748e-02, -2.2135e-02, -6.0609e-03, -1.5045e-02, -9.1784e-03,\n",
       "           8.1178e-04,  1.1518e-02,  2.3981e-02,  6.3590e-03, -2.5239e-02,\n",
       "          -1.2777e-02, -8.8744e-03,  4.5016e-03,  2.2607e-02, -9.7306e-03,\n",
       "          -1.4150e-03, -8.5694e-03, -1.9132e-02, -5.1371e-03, -1.4528e-02,\n",
       "          -5.1435e-03,  5.0629e-04,  2.4609e-02, -2.0357e-02, -1.5638e-02,\n",
       "           1.6513e-02,  2.2981e-02, -2.1279e-02, -1.3870e-03,  2.0108e-02,\n",
       "          -6.5855e-03, -1.1364e-03,  2.1240e-02,  1.6841e-02, -1.5896e-02,\n",
       "          -2.4006e-03, -1.4373e-02,  2.0055e-02, -6.9233e-03,  1.7447e-02,\n",
       "          -2.0260e-02,  2.5519e-02, -1.0701e-02,  8.3879e-03,  4.6225e-03,\n",
       "           2.5773e-02,  7.5079e-03, -2.3315e-02,  2.5970e-02,  2.1378e-03,\n",
       "           1.2275e-02,  5.6687e-03, -7.2750e-03, -9.0761e-03, -1.3257e-02,\n",
       "          -1.8570e-03,  2.6753e-02, -7.4426e-03, -2.1703e-02,  1.8413e-02,\n",
       "          -2.7691e-02,  2.6240e-02, -1.1400e-02, -8.1619e-03,  3.8713e-03,\n",
       "          -1.4170e-02,  2.8686e-03, -1.8715e-02,  1.9297e-02,  2.0628e-02,\n",
       "           2.1798e-02,  9.9498e-03,  9.0304e-04,  2.1730e-02,  4.9593e-03],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0005, -0.0166,  0.0219,  ...,  0.0243, -0.0232,  0.0034],\n",
       "          [-0.0066,  0.0334,  0.0178,  ..., -0.0246,  0.0372,  0.0231],\n",
       "          [ 0.0017, -0.0117,  0.0102,  ...,  0.0376, -0.0272,  0.0199],\n",
       "          ...,\n",
       "          [-0.0343,  0.0391, -0.0265,  ...,  0.0192, -0.0175,  0.0349],\n",
       "          [ 0.0049, -0.0156,  0.0195,  ..., -0.0361,  0.0262,  0.0227],\n",
       "          [-0.0074, -0.0268, -0.0367,  ..., -0.0044, -0.0016, -0.0157]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0141,  0.0350,  0.0027, -0.0046, -0.0319,  0.0114, -0.0242, -0.0278],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0009, -0.0281,  0.0115,  ...,  0.0046, -0.0078,  0.0187],\n",
       "          [ 0.0099,  0.0133,  0.0030,  ...,  0.0143, -0.0077,  0.0225],\n",
       "          [-0.0151, -0.0016,  0.0166,  ..., -0.0107,  0.0191, -0.0220],\n",
       "          ...,\n",
       "          [-0.0088, -0.0206, -0.0143,  ...,  0.0333,  0.0105, -0.0056],\n",
       "          [ 0.0004,  0.0317, -0.0051,  ..., -0.0470,  0.0122,  0.0056],\n",
       "          [ 0.0063, -0.0078,  0.0040,  ..., -0.0164, -0.0233, -0.0201]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0045,  0.0087,  0.0156,  ...,  0.0088,  0.0098, -0.0056],\n",
       "          [ 0.0065, -0.0210, -0.0131,  ..., -0.0046, -0.0052, -0.0168],\n",
       "          [ 0.0014,  0.0003, -0.0055,  ...,  0.0285, -0.0095, -0.0003],\n",
       "          ...,\n",
       "          [ 0.0032, -0.0033, -0.0137,  ...,  0.0060,  0.0094,  0.0206],\n",
       "          [ 0.0108,  0.0179,  0.0092,  ..., -0.0134, -0.0104,  0.0013],\n",
       "          [-0.0253,  0.0114,  0.0118,  ..., -0.0022, -0.0052, -0.0037]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0309,  0.0131, -0.0112,  ...,  0.0029,  0.0187, -0.0040],\n",
       "          [-0.0375,  0.0100,  0.0091,  ..., -0.0025, -0.0069,  0.0182],\n",
       "          [ 0.0240, -0.0076,  0.0068,  ..., -0.0420, -0.0125,  0.0049],\n",
       "          ...,\n",
       "          [ 0.0143, -0.0179, -0.0065,  ..., -0.0060, -0.0229, -0.0170],\n",
       "          [ 0.0102, -0.0010, -0.0088,  ..., -0.0015,  0.0113,  0.0004],\n",
       "          [ 0.0064,  0.0025, -0.0117,  ..., -0.0081, -0.0068,  0.0155]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0115, -0.0125, -0.0304,  ...,  0.0275, -0.0190, -0.0088],\n",
       "          [-0.0190, -0.0004,  0.0001,  ..., -0.0195,  0.0001,  0.0285],\n",
       "          [-0.0026, -0.0084,  0.0022,  ..., -0.0034,  0.0183, -0.0307],\n",
       "          ...,\n",
       "          [ 0.0199,  0.0033, -0.0119,  ...,  0.0042,  0.0034, -0.0196],\n",
       "          [-0.0106,  0.0016,  0.0346,  ...,  0.0108, -0.0258,  0.0175],\n",
       "          [-0.0036,  0.0109, -0.0138,  ..., -0.0241, -0.0152, -0.0020]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_self_attention_with_cross_frame_attention(\n",
    "                unet=pipeline.unet,\n",
    "                n_input_images=runfig.n_input_images,\n",
    "                to_k_other_frames=runfig.cross_frame_attention.to_k_other_frames,\n",
    "                with_self_attention=runfig.cross_frame_attention.with_self_attention,\n",
    "                random_others=runfig.cross_frame_attention.random_others,\n",
    "                use_lora_in_cfa=\"cfa\" in runfig.model.pose_cond_mode or \"sa\" in runfig.model.pose_cond_mode,\n",
    "                use_temb_in_lora=runfig.cross_frame_attention.use_temb_cond,\n",
    "                temb_out_size=8,\n",
    "                pose_cond_dim=runfig.model.pose_cond_dim,\n",
    "                rank=runfig.model.pose_cond_lora_rank,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zero-conv', 'no_residual_connection')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runfig.cross_frame_attention.last_layer_mode,finetune_config.training.changed_cfa_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runfig.cross_frame_attention.with_self_attention,runfig.cross_frame_attention.random_others,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cfa_config(\n",
    "   \n",
    "    pipeline: CustomInstructPix2pixDiffusionPipeline,\n",
    "):\n",
    "    if runfig.cross_frame_attention.mode == \"add_in_existing_block\":\n",
    "        update_cross_frame_attention_config(\n",
    "            pipeline.unet,\n",
    "            runfig.n_input_images,\n",
    "            runfig.cross_frame_attention.to_k_other_frames,\n",
    "            runfig.cross_frame_attention.with_self_attention,\n",
    "            runfig.cross_frame_attention.random_others,\n",
    "            change_self_attention_layers=False,  # should have custom cfa layers\n",
    "        )\n",
    "    elif runfig.cross_frame_attention.mode == \"pretrained\":\n",
    "        update_cross_frame_attention_config(\n",
    "            pipeline.unet,\n",
    "            3,\n",
    "            2,\n",
    "            runfig.cross_frame_attention.with_self_attention,\n",
    "            runfig.cross_frame_attention.random_others,\n",
    "            change_self_attention_layers=True,  # should have cfa is sa layers\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"did not implement different n_input_images for cfa.mode={runfig.cross_frame_attention.mode}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change last-layer-mode to no_residual_connection\n"
     ]
    }
   ],
   "source": [
    "if finetune_config.training.changed_cfa_last_layer != runfig.cross_frame_attention.last_layer_mode:\n",
    "        print(\"Change last-layer-mode to\", finetune_config.training.changed_cfa_last_layer)\n",
    "        update_last_layer_mode(\n",
    "            pipeline.unet,\n",
    "            finetune_config.training.changed_cfa_last_layer,\n",
    "        )\n",
    "update_vol_rend_inject_noise_sigma(\n",
    "        pipeline.unet, 0.0\n",
    "    )\n",
    "    # disable n_novel_images\n",
    "update_n_novel_images(\n",
    "        pipeline.unet, 0\n",
    "\n",
    "    )\n",
    "update_cfa_config(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LoRA weights into model\n"
     ]
    }
   ],
   "source": [
    "if runfig.model.pose_cond_mode != \"none\":\n",
    "        # Set correct lora layers\n",
    "        unet_lora_attn_procs, unet_lora_parameters = add_pose_cond_to_attention_layers(\n",
    "            pipeline.unet,\n",
    "            rank=runfig.model.pose_cond_lora_rank,\n",
    "            pose_cond_dim=runfig.model.pose_cond_dim,\n",
    "            only_cross_attention=\"sa\" not in runfig.model.pose_cond_mode,\n",
    "        )\n",
    "\n",
    "        if unet_lora_parameters is not None:\n",
    "            in_dir = os.path.join(runfig.pretrained_model_name_or_path, \"unet\")\n",
    "            try:\n",
    "                lora_state_dict, network_alpha = LoraLoaderMixin.lora_state_dict(in_dir, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "            except:\n",
    "                lora_state_dict, network_alpha = LoraLoaderMixin.lora_state_dict(in_dir, weight_name=\"pytorch_lora_weights.bin\")\n",
    "            lora_state_dict = {k.replace(\"unet.\", \"\"): v for k, v in lora_state_dict.items()}\n",
    "            pipeline.unet.load_state_dict(lora_state_dict, strict=False)\n",
    "            print(\"Loaded LoRA weights into model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sa-ca'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runfig.model.pose_cond_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.no_grad()\n",
    "def process_batch(\n",
    "    \n",
    "   \n",
    "    pipeline,\n",
    "   \n",
    "    batch,\n",
    "    guidance_scale=16,\n",
    "    image_guidance_scale: float = 1.0,\n",
    "    \n",
    "):\n",
    "    \n",
    "    model_config=runfig.model\n",
    "    cfa_config=runfig.cross_frame_attention\n",
    "    io_config=runfig\n",
    "    orig_hw=(512, 640)\n",
    "    num_inference_steps=50\n",
    "    n_repeat_generation=1\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(42)\n",
    "\n",
    "    # combine\n",
    "    \n",
    "    batch[\"images\"] = batch[\"images\"].to(\"cuda\").unsqueeze(0)\n",
    "    batch[\"target_imgs\"] = batch[\"target_imgs\"].to(\"cuda\").unsqueeze(0) \n",
    "    batch[\"pose\"] = batch[\"pose\"].to(\"cuda\").unsqueeze(0)\n",
    "    batch[\"K\"] = batch[\"K\"].to(\"cuda\").unsqueeze(0)\n",
    "    batch[\"intensity_stats\"] = batch[\"intensity_stats\"].to(\"cuda\").unsqueeze(0)\n",
    "    batch[\"bbox\"] = batch[\"bbox\"].to(\"cuda\").unsqueeze(0)\n",
    "\n",
    "    # check if need to change n_input_images\n",
    "    if runfig.n_input_images != batch[\"pose\"].shape[1]:\n",
    "        runfig.n_input_images = batch[\"pose\"].shape[1]\n",
    "        runfig.cross_frame_attention.to_k_other_frames = batch[\"pose\"].shape[1] - 1\n",
    "        runfig.model.n_input_images = batch[\"pose\"].shape[1]\n",
    "        update_cfa_config(runfig, pipeline)\n",
    "\n",
    "    # alwasy set to 0\n",
    "    batch[\"intensity_stats\"] *= 0\n",
    "\n",
    "    # create images\n",
    "    batch_size = len(batch[\"prompt\"])\n",
    "\n",
    "    batch[\"images\"] = 2*batch[\"images\"]-1\n",
    "    batch[\"target_imgs\"] = 2*batch[\"target_imgs\"]-1\n",
    "    # parse batch\n",
    "    # collapse K dimension into batch dimension (no concatenation happening)\n",
    "    batch[\"prompt\"] = [cap for cap in batch[\"prompt\"]]\n",
    "    prompt = collapse_prompt_to_batch_dim(batch[\"prompt\"],3)\n",
    "   \n",
    "    \n",
    "    _, pose = collapse_tensor_to_batch_dim(batch[\"pose\"])\n",
    "    _, K = collapse_tensor_to_batch_dim(batch[\"K\"])\n",
    "    _, intensity_stats = collapse_tensor_to_batch_dim(batch[\"intensity_stats\"])\n",
    "    bbox = batch[\"bbox\"]\n",
    "\n",
    "    _, known_images = collapse_tensor_to_batch_dim(batch[\"images\"])\n",
    "    known_images = known_images.to(pipeline.device)\n",
    "    known_images = known_images.squeeze(1)\n",
    "    print(known_images.shape)\n",
    "\n",
    "    K = K.squeeze(1)[..., :3, :3]\n",
    "    pose = pose.squeeze(1)\n",
    "    intensity_stats = intensity_stats.squeeze(1)\n",
    "\n",
    "    # build cross_attention_kwargs\n",
    "    cross_attention_kwargs = build_cross_attention_kwargs(\n",
    "        model_config=runfig.model,\n",
    "        cfa_config=runfig.cross_frame_attention,\n",
    "        pose=pose,\n",
    "        K=K,\n",
    "        intensity_stats=intensity_stats,\n",
    "        bbox=bbox,\n",
    "        orig_hw=orig_hw,\n",
    "    )\n",
    "    if \"pose_cond\" in cross_attention_kwargs:\n",
    "            cross_attention_kwargs[\"pose_cond\"] = torch.cat([cross_attention_kwargs[\"pose_cond\"]] * 3)\n",
    "    if \"unproj_reproj_kwargs\" in cross_attention_kwargs:\n",
    "        proj_kwargs = cross_attention_kwargs[\"unproj_reproj_kwargs\"]\n",
    "        proj_kwargs[\"pose\"] = torch.cat([proj_kwargs[\"pose\"]] * 3)\n",
    "        proj_kwargs[\"K\"] = torch.cat([proj_kwargs[\"K\"]] * 3)\n",
    "        proj_kwargs[\"bbox\"] = torch.cat([proj_kwargs[\"bbox\"]] * 3)\n",
    "\n",
    "    outputs = []\n",
    "    all_psnrs = []\n",
    "    all_lpipses = []\n",
    "    all_ssims = []\n",
    "    for _ in range(n_repeat_generation):\n",
    "        output = pipeline(\n",
    "            prompt=prompt,\n",
    "            height=orig_hw[0],\n",
    "            width=orig_hw[1],\n",
    "            known_images=known_images,\n",
    "            output_type=\"pt\",  # return tensor normalized to [0, 1]\n",
    "            generator=generator,\n",
    "            cross_attention_kwargs=cross_attention_kwargs,\n",
    "            guidance_scale=guidance_scale,\n",
    "            image_guidance_scale=image_guidance_scale,\n",
    "            decode_all_timesteps=True,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            n_images_per_batch=model_config.n_input_images,\n",
    "        )\n",
    "\n",
    "        # re-create K dimension from batch dimension\n",
    "        output.images = output.images.unsqueeze(1)\n",
    "        expand_output_to_k(output, batch_size, model_config.n_input_images)\n",
    "\n",
    "        outputs.append(output)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "# batch = val_data[0]\n",
    "# save_image(outputs[0].images[0], \"output.png\")\n",
    "# save_image(batch[\"images\"], \"input.png\")\n",
    "# save_image(batch[\"target_imgs\"], \"target.png\")\n",
    "\n",
    "# plt.imshow(plt.imread(\"output.png\"))\n",
    "# plt.show()\n",
    "# plt.imshow(plt.imread(\"input.png\"))\n",
    "# plt.show()\n",
    "# plt.imshow(plt.imread(\"target.png\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scan</th>\n",
       "      <th>ref_view</th>\n",
       "      <th>light_idx</th>\n",
       "      <th>src_views</th>\n",
       "      <th>target_light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scan3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[10, 1, 9, 12, 11, 13, 2, 8, 14, 27]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scan3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[9, 10, 2, 0, 8, 13, 14, 12, 7, 15]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scan3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, 1, 7, 9, 3, 15, 14, 16, 6, 10]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scan3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 6, 2, 4, 8, 5, 17, 16, 1, 15]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scan3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[5, 6, 3, 7, 18, 2, 17, 8, 16, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    scan  ref_view  light_idx                             src_views  \\\n",
       "0  scan3         0          0  [10, 1, 9, 12, 11, 13, 2, 8, 14, 27]   \n",
       "1  scan3         1          0   [9, 10, 2, 0, 8, 13, 14, 12, 7, 15]   \n",
       "2  scan3         2          0    [8, 1, 7, 9, 3, 15, 14, 16, 6, 10]   \n",
       "3  scan3         3          0     [7, 6, 2, 4, 8, 5, 17, 16, 1, 15]   \n",
       "4  scan3         4          0     [5, 6, 3, 7, 18, 2, 17, 8, 16, 1]   \n",
       "\n",
       "   target_light  \n",
       "0             6  \n",
       "1             0  \n",
       "2             5  \n",
       "3             2  \n",
       "4             1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame(val_data.metas,columns=[\"scan\",\"ref_view\",\"light_idx\",\"src_views\",\"target_light\"])    \n",
    "df.head()                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['scan3', 'scan5', 'scan17', 'scan21', 'scan28', 'scan35', 'scan37',\n",
       "       'scan38', 'scan40', 'scan43', 'scan56', 'scan59', 'scan66',\n",
       "       'scan67', 'scan82', 'scan86', 'scan106', 'scan117'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"scan\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/project/dp_simple/')\n",
    "#import ViT\n",
    "from torchvision import transforms as T\n",
    "from CasMVSNet_pl.models.mvsnet import CascadeMVSNet\n",
    "from CasMVSNet_pl.utils import load_ckpt\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset  \n",
    "from CasMVSNet_pl.utils import *\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset \n",
    "from CasMVSNet_pl.metrics import *  \n",
    "from inplace_abn import ABN\n",
    "\n",
    "import pytorch_ssim\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_ssim\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/D3Dnet/code')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from einops import rearrange\n",
    "from torchvision import models\n",
    "import sys\n",
    "\n",
    "from CasMVSNet_pl.datasets.utils import save_pfm, read_pfm\n",
    "import cv2\n",
    "import torch\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# for depth prediction\n",
    "from CasMVSNet_pl.models.mvsnet import CascadeMVSNet\n",
    "from CasMVSNet_pl.utils import load_ckpt\n",
    "from inplace_abn import ABN\n",
    "\n",
    "# for point cloud fusion\n",
    "from numba import jit\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # this increases inference speed a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True # this increases inference speed a little\n",
    "\n",
    "def get_opts():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--root_dir', type=str,\n",
    "                        default='/root/autodl-tmp/mvs_training/dtu',\n",
    "                        help='root directory of dtu dataset')\n",
    "    parser.add_argument('--dataset_name', type=str, default='dtu',\n",
    "                        choices=['dtu', 'tanks', 'blendedmvs'],\n",
    "                        help='which dataset to train/val')\n",
    "    parser.add_argument('--split', type=str, default='train',\n",
    "                        help='which split to evaluate')\n",
    "    parser.add_argument('--scan', type=str, default='scan7',\n",
    "                        help='specify scan to evaluate (must be in the split)')\n",
    "    parser.add_argument('--cpu', default=False, action='store_true',\n",
    "                        help='''use cpu to do depth inference.\n",
    "                                WARNING: It is going to be EXTREMELY SLOW!\n",
    "                                about 37s/view, so in total 30min/scan. \n",
    "                             ''')\n",
    "    # for depth prediction\n",
    "    parser.add_argument('--n_views', type=int, default=3,\n",
    "                        help='number of views (including ref) to be used in testing')\n",
    "    parser.add_argument('--depth_interval', type=float, default=2.65,\n",
    "                        help='depth interval unit in mm')\n",
    "    parser.add_argument('--n_depths', nargs='+', type=int, default=[8,32,48],\n",
    "                        help='number of depths in each level')\n",
    "    parser.add_argument('--interval_ratios', nargs='+', type=float, default=[1.0,2.0,4.0],\n",
    "                        help='depth interval ratio to multiply with --depth_interval in each level')\n",
    "    parser.add_argument('--num_groups', type=int, default=1, choices=[1, 2, 4, 8],\n",
    "                        help='number of groups in groupwise correlation, must be a divisor of 8')\n",
    "    parser.add_argument('--img_wh', nargs=\"+\", type=int, default=[640,512],\n",
    "                        help='resolution (img_w, img_h) of the image, must be multiples of 32')\n",
    "    parser.add_argument('--ckpt_path', type=str, default='/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/ckpts/_ckpt_epoch_10.ckpt',\n",
    "                        help='pretrained checkpoint path to load')\n",
    "    parser.add_argument('--save_visual', default=False, action='store_true',\n",
    "                        help='save depth and proba visualization or not')\n",
    "\n",
    "    # for point cloud fusion\n",
    "    parser.add_argument('--conf', type=float, default=0.999,\n",
    "                        help='min confidence for pixel to be valid')\n",
    "    parser.add_argument('--min_geo_consistent', type=int, default=5,\n",
    "                        help='min number of consistent views for pixel to be valid')\n",
    "    parser.add_argument('--max_ref_views', type=int, default=400,\n",
    "                        help='max number of ref views (to limit RAM usage)')\n",
    "    parser.add_argument('--skip', type=int, default=1,\n",
    "                        help='''how many points to skip when creating the point cloud.\n",
    "                                Larger = fewer points and smaller file size.\n",
    "                                Ref: skip=10 creates ~= 3M points = 50MB file\n",
    "                                     skip=1 creates ~= 30M points = 500MB file\n",
    "                             ''')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(batch):\n",
    "    imgs = batch['images']\n",
    "    proj_mats = batch['proj_mats']\n",
    "    init_depth_min = batch['init_depth_min'].item()\n",
    "    depth_interval = batch['depth_interval'].item()\n",
    "    scan, vid = batch['scan_vid']\n",
    "    return imgs, proj_mats, init_depth_min, depth_interval, \\\n",
    "           scan, vid\n",
    "\n",
    "\n",
    "# define read_image and read_proj_mat for each dataset\n",
    "\n",
    "def read_image(dataset_name, root_dir, scan, vid):\n",
    "    if dataset_name == 'dtu':\n",
    "        return cv2.imread(os.path.join(root_dir,\n",
    "                    f'Rectified/{scan}_train/rect_{vid+1:03d}_3_r5000.png'))\n",
    "    if dataset_name == 'tanks':\n",
    "        return cv2.imread(os.path.join(root_dir, scan,\n",
    "                    f'images/{vid:08d}.jpg'))\n",
    "    if dataset_name == 'blendedmvs':\n",
    "        return cv2.imread(os.path.join(root_dir, scan,\n",
    "                    f'blended_images/{vid:08d}.jpg'))\n",
    "\n",
    "\n",
    "def read_refined_image(dataset_name, scan, vid):\n",
    "    return cv2.imread(f'results/{dataset_name}/image_refined/{scan}/{vid:08d}.png')\n",
    "\n",
    "\n",
    "def save_refined_image(image_refined, dataset_name, scan, vid):\n",
    "    cv2.imwrite(f'results/{dataset_name}/image_refined/{scan}/{vid:08d}.png',\n",
    "                image_refined)\n",
    "\n",
    "\n",
    "def read_proj_mat(dataset_name, dataset, scan, vid):\n",
    "    if dataset_name == 'dtu':\n",
    "        return dataset.proj_mats[vid][0][0].numpy()\n",
    "    if dataset_name in ['tanks', 'blendedmvs']:\n",
    "        return dataset.proj_mats[scan][vid][0][0].numpy()\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def xy_ref2src(xy_ref, depth_ref, P_world2ref,\n",
    "               depth_src, P_world2src, img_wh):\n",
    "    # create ref grid and project to ref 3d coordinate using depth_ref\n",
    "    xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_ref\n",
    "    xyz_ref_h = np.vstack((xyz_ref, np.ones_like(xy_ref[:1])))\n",
    "\n",
    "    P = (P_world2src @ np.ascontiguousarray(np.linalg.inv(P_world2ref)))[:3]\n",
    "    # project to src 3d coordinate using P_world2ref and P_world2src\n",
    "    xyz_src_h = P @ xyz_ref_h.reshape(4,-1)\n",
    "    xy_src = xyz_src_h[:2]/xyz_src_h[2:3]\n",
    "    xy_src = xy_src.reshape(2, img_wh[1], img_wh[0])\n",
    "\n",
    "    return xy_src\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def xy_src2ref(xy_ref, xy_src, depth_ref, P_world2ref,\n",
    "               depth_src2ref, P_world2src, img_wh):\n",
    "    # project xy_src back to ref view using the sampled depth\n",
    "    xyz_src = np.vstack((xy_src, np.ones_like(xy_src[:1]))) * depth_src2ref\n",
    "    xyz_src_h = np.vstack((xyz_src, np.ones_like(xy_src[:1])))\n",
    "    P = (P_world2ref @ np.ascontiguousarray(np.linalg.inv(P_world2src)))[:3]\n",
    "    xyz_ref_h = P @ xyz_src_h.reshape(4,-1)\n",
    "    depth_ref_reproj = xyz_ref_h[2].reshape(img_wh[1], img_wh[0])\n",
    "    xy_ref_reproj = xyz_ref_h[:2]/xyz_ref_h[2:3]\n",
    "    xy_ref_reproj = xy_ref_reproj.reshape(2, img_wh[1], img_wh[0])\n",
    "\n",
    "    # check |p_reproj-p_1| < 1\n",
    "    pixel_diff = xy_ref_reproj - xy_ref\n",
    "    mask_pixel_reproj = (pixel_diff[0]**2+pixel_diff[1]**2)<1\n",
    "\n",
    "    # check |d_reproj-d_1| / d_1 < 0.01\n",
    "    mask_depth_reproj = np.abs((depth_ref_reproj-depth_ref)/depth_ref)<0.01\n",
    "\n",
    "    mask_geo = mask_pixel_reproj & mask_depth_reproj\n",
    "\n",
    "    return depth_ref_reproj, mask_geo\n",
    "\n",
    "\n",
    "def check_geo_consistency(depth_ref, P_world2ref,\n",
    "                          depth_src, P_world2src,\n",
    "                          image_ref, image_src,\n",
    "                          img_wh):\n",
    "    \"\"\"\n",
    "    Check the geometric consistency between ref and src views.\n",
    "    \"\"\"\n",
    "    xy_ref = np.mgrid[:img_wh[1],:img_wh[0]][::-1].astype(np.float32)\n",
    "    xy_src = xy_ref2src(xy_ref, depth_ref, P_world2ref,\n",
    "                        depth_src, P_world2src, img_wh)\n",
    "\n",
    "    # Sample the depth of xy_src using bilinear interpolation\n",
    "    depth_src2ref = cv2.remap(depth_src,\n",
    "                              xy_src[0].astype(np.float32),\n",
    "                              xy_src[1].astype(np.float32),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    image_src2ref = cv2.remap(image_src,\n",
    "                              xy_src[0].astype(np.float32),\n",
    "                              xy_src[1].astype(np.float32),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    depth_ref_reproj, mask_geo = \\\n",
    "        xy_src2ref(xy_ref, xy_src, depth_ref, P_world2ref, \n",
    "                   depth_src2ref, P_world2src, img_wh)\n",
    "\n",
    "    depth_ref_reproj[~mask_geo] = 0\n",
    "    image_src2ref[~mask_geo] = 0\n",
    "    \n",
    "    return depth_ref_reproj, mask_geo, image_src2ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_error(depth_pred, depth_gt, mask):\n",
    "    depth_pred, depth_gt = depth_pred[mask], depth_gt[mask]\n",
    "    return np.abs(depth_pred - depth_gt)\n",
    "\n",
    "def acc_threshold(depth_pred, depth_gt, mask, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    errors = abs_error(depth_pred, depth_gt, mask)\n",
    "    acc_mask = errors < threshold\n",
    "    return acc_mask.mean()\n",
    "\n",
    "def return_log(result1,result2,gt_depth,mask):\n",
    "    depth_pred = result1[\"depth_0\"][0].cpu().numpy()\n",
    "    ori_pred = result2[\"depth_0\"][0].cpu().numpy()\n",
    "\n",
    "    print(depth_pred.shape, ori_pred.shape, gt_depth.shape, mask.shape)\n",
    "    \n",
    "\n",
    "    abs_error1 = abs_error(depth_pred, gt_depth, mask).mean()\n",
    "    abs_error2 = abs_error(ori_pred, gt_depth, mask).mean()\n",
    "    print(f\"depth modified is {abs_error1},original error is {abs_error2} \")\n",
    "    abs_diff = abs_error1 - abs_error2\n",
    "    abs_ratio = abs_error1 / abs_error2\n",
    "\n",
    "    acc1mm1 = acc_threshold(depth_pred, gt_depth, mask, 1)\n",
    "    acc1mm2 = acc_threshold(ori_pred, gt_depth, mask, 1)\n",
    "    acc_diff = acc1mm1 - acc1mm2\n",
    "    acc_ratio = acc1mm1 / (acc1mm2+1e-7)\n",
    "\n",
    "    acc2mm1 = acc_threshold(depth_pred, gt_depth, mask, 2)\n",
    "    acc2mm2 = acc_threshold(ori_pred, gt_depth, mask, 2)\n",
    "    acc_diff2 = acc2mm1 - acc2mm2\n",
    "    acc_ratio2 = acc2mm1 / (acc2mm2+1e-7)\n",
    "\n",
    "    acc3mm1 = acc_threshold(depth_pred, gt_depth, mask, 3)\n",
    "    acc3mm2 = acc_threshold(ori_pred, gt_depth, mask, 3)\n",
    "    acc_diff3 = acc3mm1 - acc3mm2\n",
    "    acc_ratio3 = acc3mm1 / (acc3mm2+1e-7)\n",
    "\n",
    "    acc4mm1 = acc_threshold(depth_pred, gt_depth, mask, 4)\n",
    "    acc4mm2 = acc_threshold(ori_pred, gt_depth, mask, 4)\n",
    "    acc_diff4 = acc4mm1 - acc4mm2\n",
    "    acc_ratio4 = acc4mm1 / (acc4mm2+1e-7)\n",
    "\n",
    "    return {\"abs_diff\":abs_diff,\"abs_ratio\":abs_ratio,\"acc_diff1\":acc_diff,\"acc_ratio1\":acc_ratio,\n",
    "            \"acc_diff2\":acc_diff2,\"acc_ratio2\":acc_ratio2,\"acc_diff3\":acc_diff3,\"acc_ratio3\":acc_ratio3,\n",
    "            \"acc_diff4\":acc_diff4,\"acc_ratio4\":acc_ratio4}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_opts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CascadeMVSNet(\n",
       "  (feature): FeatureNet(\n",
       "    (conv0): Sequential(\n",
       "      (0): ConvBnReLU(\n",
       "        (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "      (1): ConvBnReLU(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "    )\n",
       "    (conv1): Sequential(\n",
       "      (0): ConvBnReLU(\n",
       "        (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "      (1): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "      (2): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "      (1): ConvBnReLU(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "      (2): ConvBnReLU(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "      )\n",
       "    )\n",
       "    (toplayer): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (lat1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (lat0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (smooth1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (smooth0): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (cost_reg_0): CostRegNet(\n",
       "    (conv0): ConvBnReLU3D(\n",
       "      (conv): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv1): ConvBnReLU3D(\n",
       "      (conv): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv2): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv3): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv4): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv5): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv6): ConvBnReLU3D(\n",
       "      (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv7): Sequential(\n",
       "      (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv9): Sequential(\n",
       "      (0): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv11): Sequential(\n",
       "      (0): ConvTranspose3d(16, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (prob): Conv3d(8, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  )\n",
       "  (cost_reg_1): CostRegNet(\n",
       "    (conv0): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv1): ConvBnReLU3D(\n",
       "      (conv): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv2): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv3): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv4): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv5): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv6): ConvBnReLU3D(\n",
       "      (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv7): Sequential(\n",
       "      (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv9): Sequential(\n",
       "      (0): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv11): Sequential(\n",
       "      (0): ConvTranspose3d(16, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (prob): Conv3d(8, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  )\n",
       "  (cost_reg_2): CostRegNet(\n",
       "    (conv0): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv1): ConvBnReLU3D(\n",
       "      (conv): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv2): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv3): ConvBnReLU3D(\n",
       "      (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv4): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv5): ConvBnReLU3D(\n",
       "      (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv6): ConvBnReLU3D(\n",
       "      (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (bn): ABN(64, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv7): Sequential(\n",
       "      (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(32, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv9): Sequential(\n",
       "      (0): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(16, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (conv11): Sequential(\n",
       "      (0): ConvTranspose3d(16, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "      (1): ABN(8, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
       "    )\n",
       "    (prob): Conv3d(8, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CascadeMVSNet(n_depths=args.n_depths,\n",
    "                        interval_ratios=args.interval_ratios,\n",
    "                        num_groups=args.num_groups,\n",
    "                        norm_act=ABN)\n",
    "device = 'cpu' if args.cpu else 'cuda:0'\n",
    "model.to(device)\n",
    "load_ckpt(model, args.ckpt_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 640])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0][\"depths\"][\"level_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine = True\n",
    "read_gt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save_visual=True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating depth and confidence predictions...\n",
      "Processing scan106 with 49 views\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 12.391034126281738,original error is 3.7918593883514404 \n",
      "scan106 0000 abs_diff: 8.599174499511719 abs_ratio: 3.267798900604248 acc_diff1: -0.2826278077156093 acc_ratio1: 0.5943789475340991 acc_diff2: -0.19801939607067864 acc_ratio2: 0.7561123831708602 acc_diff3: -0.1485042978376433 acc_ratio3: 0.8257505952749943 acc_diff4: -0.12468967653476304 acc_ratio4: 0.8576279551688266 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/49 [00:03<02:56,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.114622116088867,original error is 1.9095314741134644 \n",
      "scan106 0001 abs_diff: 4.902132511138916 abs_ratio: 2.4494457244873047 acc_diff1: -0.2844683343831733 acc_ratio1: 0.6065192344131805 acc_diff2: -0.18622738480272183 acc_ratio2: 0.7780471944761118 acc_diff3: -0.12643567254952282 acc_ratio3: 0.8557358625133635 acc_diff4: -0.09665383079381018 acc_ratio4: 0.8919993651788007 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/49 [00:05<01:53,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.692985773086548,original error is 1.7375462055206299 \n",
      "scan106 0002 abs_diff: 3.9199016094207764 abs_ratio: 2.3414313793182373 acc_diff1: -0.2857470917445683 acc_ratio1: 0.6085831449312121 acc_diff2: -0.1807002984306126 acc_ratio2: 0.7871294096041369 acc_diff3: -0.1199816726901232 acc_ratio3: 0.8646265015750637 acc_diff4: -0.09139592380356105 acc_ratio4: 0.899108557576854 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/49 [00:06<01:32,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 4.639885902404785,original error is 3.7749674320220947 \n",
      "scan106 0003 abs_diff: 3.156155824661255 abs_ratio: 2.0633535385131836 acc_diff1: -0.27953153877258996 acc_ratio1: 0.6236442933521972 acc_diff2: -0.17112123129650342 acc_ratio2: 0.799736161232665 acc_diff3: -0.1119216037679408 acc_ratio3: 0.8744136457263529 acc_diff4: -0.08297620453391188 acc_ratio4: 0.9088173454256536 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/49 [00:08<01:22,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 13.902609825134277,original error is 11.890796661376953 \n",
      "scan106 0004 abs_diff: 2.9272873401641846 abs_ratio: 1.8845208883285522 acc_diff1: -0.2650882055955809 acc_ratio1: 0.6461344634561652 acc_diff2: -0.15162467484672737 acc_ratio2: 0.8222298803460811 acc_diff3: -0.09925006642916281 acc_ratio3: 0.8882525679098107 acc_diff4: -0.07427662745571026 acc_ratio4: 0.9180192052823408 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/49 [00:09<01:16,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 11.308504104614258,original error is 8.902156829833984 \n",
      "scan106 0005 abs_diff: 2.840463876724243 abs_ratio: 1.782152533531189 acc_diff1: -0.2672670994942361 acc_ratio1: 0.6476739346816544 acc_diff2: -0.14749502836297648 acc_ratio2: 0.8278028203333562 acc_diff3: -0.09813164540829612 acc_ratio3: 0.8898070746102805 acc_diff4: -0.07546574152303577 acc_ratio4: 0.9168837631177023 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/49 [00:11<01:12,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 4.116981506347656,original error is 2.5493357181549072 \n",
      "scan106 0006 abs_diff: 2.658632755279541 abs_ratio: 1.7582626342773438 acc_diff1: -0.26855513923401336 acc_ratio1: 0.6479139175906019 acc_diff2: -0.1460204231775369 acc_ratio2: 0.829894267545983 acc_diff3: -0.09447041578798922 acc_ratio3: 0.8939675077422543 acc_diff4: -0.07144080623899714 acc_ratio4: 0.9213096519169565 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/49 [00:12<01:08,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.099046230316162,original error is 1.9533439874649048 \n",
      "scan106 0007 abs_diff: 2.4695165157318115 abs_ratio: 1.7367966175079346 acc_diff1: -0.27251239867183785 acc_ratio1: 0.6445507033387343 acc_diff2: -0.14604941290231394 acc_ratio2: 0.8304418251217239 acc_diff3: -0.09291518296815393 acc_ratio3: 0.895936204103272 acc_diff4: -0.06919988872081212 acc_ratio4: 0.9238856513031894 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 8/49 [00:14<01:05,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.558220624923706,original error is 1.588063359260559 \n",
      "scan106 0008 abs_diff: 2.3029210567474365 abs_ratio: 1.7228087186813354 acc_diff1: -0.27506128461033214 acc_ratio1: 0.6431079936547399 acc_diff2: -0.14645608766823415 acc_ratio2: 0.8308539629147247 acc_diff3: -0.09257598621244664 acc_ratio3: 0.8968334454763904 acc_diff4: -0.06827841425425107 acc_ratio4: 0.9252475103985852 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/49 [00:16<01:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.8737165927886963,original error is 1.5703096389770508 \n",
      "scan106 0009 abs_diff: 2.202969789505005 abs_ratio: 1.7335309982299805 acc_diff1: -0.27613201191919035 acc_ratio1: 0.6419066972812588 acc_diff2: -0.14971243944798734 acc_ratio2: 0.8279572116778373 acc_diff3: -0.09492364319781554 acc_ratio3: 0.8947742700135761 acc_diff4: -0.06970944095771846 acc_ratio4: 0.9240710734345173 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/49 [00:17<01:05,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.7178595066070557,original error is 2.0933971405029297 \n",
      "scan106 0010 abs_diff: 2.1503782272338867 abs_ratio: 1.7373912334442139 acc_diff1: -0.27454696276100976 acc_ratio1: 0.6414694456891956 acc_diff2: -0.14963935672865283 acc_ratio2: 0.8276423146488354 acc_diff3: -0.09429123864307355 acc_ratio3: 0.8953874054668951 acc_diff4: -0.0690162694942284 acc_ratio4: 0.9248199177239038 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/49 [00:19<01:02,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.4005861282348633,original error is 1.8767772912979126 \n",
      "scan106 0011 abs_diff: 2.0981640815734863 abs_ratio: 1.7436026334762573 acc_diff1: -0.2729927895168346 acc_ratio1: 0.6445995019993563 acc_diff2: -0.14680210864354967 acc_ratio2: 0.8311140475411957 acc_diff3: -0.09222679112874538 acc_ratio3: 0.8977952535930606 acc_diff4: -0.06771801417274288 acc_ratio4: 0.9263275467342419 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/49 [00:21<00:59,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 4.231178283691406,original error is 1.9870718717575073 \n",
      "scan106 0012 abs_diff: 2.1093904972076416 abs_ratio: 1.7732758522033691 acc_diff1: -0.27246817552906577 acc_ratio1: 0.645428339352783 acc_diff2: -0.14740220334509424 acc_ratio2: 0.8304287141972349 acc_diff3: -0.09408478255322364 acc_ratio3: 0.8957918945324918 acc_diff4: -0.07008362858366465 acc_ratio4: 0.9238385176787934 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 13/49 [00:22<00:57,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 4.697841167449951,original error is 2.120145559310913 \n",
      "scan106 0013 abs_diff: 2.14284086227417 abs_ratio: 1.8048855066299438 acc_diff1: -0.27112133379711456 acc_ratio1: 0.646386845984965 acc_diff2: -0.14682023538675473 acc_ratio2: 0.8306227726942542 acc_diff3: -0.09421672272612469 acc_ratio3: 0.8954393396598684 acc_diff4: -0.07086441141126491 acc_ratio4: 0.9228975989577375 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 14/49 [00:24<00:55,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.9092884063720703,original error is 1.477778673171997 \n",
      "scan106 0014 abs_diff: 2.095418691635132 abs_ratio: 1.8158057928085327 acc_diff1: -0.27164022643464486 acc_ratio1: 0.6467669690195093 acc_diff2: -0.14709732907752315 acc_ratio2: 0.8308694663763115 acc_diff3: -0.09379681505855367 acc_ratio3: 0.8962126887674361 acc_diff4: -0.06980221351772513 acc_ratio4: 0.9242172047650211 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 15/49 [00:25<00:53,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.707930564880371,original error is 1.5906322002410889 \n",
      "scan106 0015 abs_diff: 2.0967860221862793 abs_ratio: 1.8480119705200195 acc_diff1: -0.2730913808228551 acc_ratio1: 0.645564594951272 acc_diff2: -0.1476597199478685 acc_ratio2: 0.8303786755724939 acc_diff3: -0.0939770263713704 acc_ratio3: 0.8960770750094519 acc_diff4: -0.06992176765073461 acc_ratio4: 0.9241307747221046 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16/49 [00:27<00:51,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.3369882106781006,original error is 2.547083854675293 \n",
      "scan106 0016 abs_diff: 2.0199108123779297 abs_ratio: 1.8163713216781616 acc_diff1: -0.2715553132486029 acc_ratio1: 0.6477116646281695 acc_diff2: -0.14600358742198088 acc_ratio2: 0.8323161734217219 acc_diff3: -0.0932677329282698 acc_ratio3: 0.8968785079545742 acc_diff4: -0.06953024906020178 acc_ratio4: 0.9245569884163233 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 17/49 [00:28<00:49,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.8527886867523193,original error is 1.354295015335083 \n",
      "scan106 0017 abs_diff: 1.9909430742263794 abs_ratio: 1.8324881792068481 acc_diff1: -0.27066804079234175 acc_ratio1: 0.6492135970955057 acc_diff2: -0.14471626517692038 acc_ratio2: 0.8339792476206137 acc_diff3: -0.09249942073706709 acc_ratio3: 0.8978277486659513 acc_diff4: -0.06942447939306656 acc_ratio4: 0.9247658321633251 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 18/49 [00:30<00:48,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.642282247543335,original error is 1.6392204761505127 \n",
      "scan106 0018 abs_diff: 1.9389493465423584 abs_ratio: 1.8208791017532349 acc_diff1: -0.27093244642653963 acc_ratio1: 0.6502052014539965 acc_diff2: -0.1440567466323344 acc_ratio2: 0.8351737057436912 acc_diff3: -0.09154333384146557 acc_ratio3: 0.8990526371319045 acc_diff4: -0.0681917498631067 acc_ratio4: 0.9261811624358406 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 19/49 [00:31<00:46,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.3543238639831543,original error is 2.218082904815674 \n",
      "scan106 0019 abs_diff: 1.8988139629364014 abs_ratio: 1.805448293685913 acc_diff1: -0.27095501342913797 acc_ratio1: 0.6513309909011923 acc_diff2: -0.1423666695875605 acc_ratio2: 0.8373987567697407 acc_diff3: -0.08976149444931177 acc_ratio3: 0.9011248514235921 acc_diff4: -0.06663532799483152 acc_ratio4: 0.9279203527282502 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 20/49 [00:33<00:45,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.3832926750183105,original error is 1.399966835975647 \n",
      "scan106 0020 abs_diff: 1.8552193641662598 abs_ratio: 1.8005409240722656 acc_diff1: -0.27336089761043236 acc_ratio1: 0.6490278509796626 acc_diff2: -0.14422977599946382 acc_ratio2: 0.835899531230907 acc_diff3: -0.08952741213594673 acc_ratio3: 0.9016266497555518 acc_diff4: -0.06548677815756085 acc_ratio4: 0.9292661317812452 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 21/49 [00:35<00:43,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.6448371410369873,original error is 1.190735936164856 \n",
      "scan106 0021 abs_diff: 1.8369866609573364 abs_ratio: 1.8196609020233154 acc_diff1: -0.275537074146736 acc_ratio1: 0.6473996564897176 acc_diff2: -0.14525055777513315 acc_ratio2: 0.8350518870130318 acc_diff3: -0.08969505452193567 acc_ratio3: 0.9015851323140005 acc_diff4: -0.06535341394897003 acc_ratio4: 0.9295047170131336 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 22/49 [00:36<00:41,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.8815035820007324,original error is 1.1774959564208984 \n",
      "scan106 0022 abs_diff: 1.8312050104141235 abs_ratio: 1.8469427824020386 acc_diff1: -0.2771992434362513 acc_ratio1: 0.645871767606137 acc_diff2: -0.14581522839208863 acc_ratio2: 0.8345444615200573 acc_diff3: -0.08980187190284125 acc_ratio3: 0.9015581875640113 acc_diff4: -0.06552922738361404 acc_ratio4: 0.9293966037858329 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 23/49 [00:38<00:40,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.8576388359069824,original error is 1.415492057800293 \n",
      "scan106 0023 abs_diff: 1.814994215965271 abs_ratio: 1.8541046380996704 acc_diff1: -0.2779072792594192 acc_ratio1: 0.6452863660081755 acc_diff2: -0.14555901015762618 acc_ratio2: 0.8348686034746104 acc_diff3: -0.08911695155713888 acc_ratio3: 0.9023081479261156 acc_diff4: -0.06477346120527512 acc_ratio4: 0.9302078375518498 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 24/49 [00:39<00:38,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.5508604049682617,original error is 1.5944589376449585 \n",
      "scan106 0024 abs_diff: 1.8206504583358765 abs_ratio: 1.8690205812454224 acc_diff1: -0.2777380563658615 acc_ratio1: 0.6456454450796109 acc_diff2: -0.14493474827544178 acc_ratio2: 0.835616185703397 acc_diff3: -0.08870953331120145 acc_ratio3: 0.9027953570616951 acc_diff4: -0.06462491537724556 acc_ratio4: 0.9304056034888315 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 25/49 [00:41<00:37,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.9640355110168457,original error is 1.9551533460617065 \n",
      "scan106 0025 abs_diff: 1.827890157699585 abs_ratio: 1.875115156173706 acc_diff1: -0.276254342547963 acc_ratio1: 0.6472068017099856 acc_diff2: -0.14470074502075517 acc_ratio2: 0.8357020890871529 acc_diff3: -0.08956376735917676 acc_ratio3: 0.9018062017267243 acc_diff4: -0.06608064623537033 acc_ratio4: 0.9288427162640264 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 26/49 [00:42<00:36,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.025616407394409,original error is 1.3655098676681519 \n",
      "scan106 0026 abs_diff: 1.8216758966445923 abs_ratio: 1.8877309560775757 acc_diff1: -0.2751330649586915 acc_ratio1: 0.6489180960843032 acc_diff2: -0.14370248692684418 acc_ratio2: 0.8368671245460865 acc_diff3: -0.08943418469637963 acc_ratio3: 0.9019742159202114 acc_diff4: -0.06612446191021676 acc_ratio4: 0.9288263241384064 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 27/49 [00:44<00:36,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.35486102104187,original error is 0.9622334837913513 \n",
      "scan106 0027 abs_diff: 1.8063527345657349 abs_ratio: 1.907715082168579 acc_diff1: -0.2760961430809126 acc_ratio1: 0.6487027133819384 acc_diff2: -0.14411640810007875 acc_ratio2: 0.8367268413722772 acc_diff3: -0.08973164111694909 acc_ratio3: 0.9018168917303335 acc_diff4: -0.06621774272699847 acc_ratio4: 0.9288298483261109 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 28/49 [00:46<00:34,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.6259214878082275,original error is 1.0121707916259766 \n",
      "scan106 0028 abs_diff: 1.7997113466262817 abs_ratio: 1.931391954421997 acc_diff1: -0.27615236203710974 acc_ratio1: 0.6494046851428491 acc_diff2: -0.14399849618985414 acc_ratio2: 0.8370510631952306 acc_diff3: -0.08938953479536159 acc_ratio3: 0.9022761544548235 acc_diff4: -0.06581523115832709 acc_ratio4: 0.9293135965130748 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 29/49 [00:47<00:32,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.0011484622955322,original error is 0.9604710936546326 \n",
      "scan106 0029 abs_diff: 1.7744101285934448 abs_ratio: 1.9364625215530396 acc_diff1: -0.27515047823990063 acc_ratio1: 0.6515327062379898 acc_diff2: -0.1430473966982804 acc_ratio2: 0.8383100382645935 acc_diff3: -0.08889853469728821 acc_ratio3: 0.9029033833942314 acc_diff4: -0.06545412024004088 acc_ratio4: 0.9297581682576384 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 30/49 [00:49<00:30,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.9931753873825073,original error is 1.1177018880844116 \n",
      "scan106 0030 abs_diff: 1.7454121112823486 abs_ratio: 1.9315211772918701 acc_diff1: -0.2737692254167603 acc_ratio1: 0.6538703363340604 acc_diff2: -0.1417994945756243 acc_ratio2: 0.8398449924756705 acc_diff3: -0.08827587786104063 acc_ratio3: 0.9036506479446792 acc_diff4: -0.06496901864950567 acc_ratio4: 0.9303252477696715 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 31/49 [00:51<00:28,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.8970789909362793,original error is 1.2286248207092285 \n",
      "scan106 0031 abs_diff: 1.7430074214935303 abs_ratio: 1.9448480606079102 acc_diff1: -0.27268146543561067 acc_ratio1: 0.6556433260954716 acc_diff2: -0.14086537903865143 acc_ratio2: 0.8409503727302825 acc_diff3: -0.08813845988096522 acc_ratio3: 0.9038436316797144 acc_diff4: -0.06512314552142391 acc_ratio4: 0.9301934189572089 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 32/49 [00:52<00:27,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.469144105911255,original error is 1.5264010429382324 \n",
      "scan106 0032 abs_diff: 1.718756914138794 abs_ratio: 1.9349322319030762 acc_diff1: -0.2729019697859609 acc_ratio1: 0.6558474461918669 acc_diff2: -0.1403702848759474 acc_ratio2: 0.8415830001957528 acc_diff3: -0.08773832091420565 acc_ratio3: 0.9043144865912381 acc_diff4: -0.06487971438330357 acc_ratio4: 0.9304791564576569 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 33/49 [00:54<00:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.3133723735809326,original error is 1.2273889780044556 \n",
      "scan106 0033 abs_diff: 1.700145959854126 abs_ratio: 1.9334574937820435 acc_diff1: -0.273326299931867 acc_ratio1: 0.6559880454593374 acc_diff2: -0.1402330851213879 acc_ratio2: 0.8418877378892394 acc_diff3: -0.08756081552236923 acc_ratio3: 0.9045706762981494 acc_diff4: -0.06454494566475723 acc_ratio4: 0.9308665936020137 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 34/49 [00:55<00:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.9062126874923706,original error is 1.3557846546173096 \n",
      "scan106 0034 abs_diff: 1.6672967672348022 abs_ratio: 1.9183868169784546 acc_diff1: -0.2744448835762567 acc_ratio1: 0.6546092914708812 acc_diff2: -0.14108889290164425 acc_ratio2: 0.841085153830168 acc_diff3: -0.08750981619716953 acc_ratio3: 0.9047108078969425 acc_diff4: -0.0638865310577719 acc_ratio4: 0.9316080464360923 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 35/49 [00:57<00:21,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.2034316062927246,original error is 0.9404058456420898 \n",
      "scan106 0035 abs_diff: 1.656067132949829 abs_ratio: 1.9301834106445312 acc_diff1: -0.2763370920628094 acc_ratio1: 0.6529335126150309 acc_diff2: -0.1422511432782051 acc_ratio2: 0.840095197164763 acc_diff3: -0.0881137013050867 acc_ratio3: 0.9042079553338086 acc_diff4: -0.06401564147463715 acc_ratio4: 0.9315535660119012 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 36/49 [00:58<00:20,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.067241668701172,original error is 1.398384928703308 \n",
      "scan106 0036 abs_diff: 1.6293857097625732 abs_ratio: 1.9179706573486328 acc_diff1: -0.2780562393811271 acc_ratio1: 0.6514316482141282 acc_diff2: -0.14297711066521696 acc_ratio2: 0.8394882028013863 acc_diff3: -0.08813431794515063 acc_ratio3: 0.9042767657862145 acc_diff4: -0.06366689623462661 acc_ratio4: 0.931971172991173 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 37/49 [01:00<00:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.5924768447875977,original error is 1.5159491300582886 \n",
      "scan106 0037 abs_diff: 1.6148368120193481 abs_ratio: 1.912501335144043 acc_diff1: -0.27969862712572485 acc_ratio1: 0.6500053575932782 acc_diff2: -0.1437523375205951 acc_ratio2: 0.8388357740647652 acc_diff3: -0.08812490339880417 acc_ratio3: 0.9043681618750699 acc_diff4: -0.06350726538539549 acc_ratio4: 0.9321838010574744 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 38/49 [01:02<00:17,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 3.152177572250366,original error is 1.894791841506958 \n",
      "scan106 0038 abs_diff: 1.6056714057922363 abs_ratio: 1.9061193466186523 acc_diff1: -0.28115067512329095 acc_ratio1: 0.6485729708897299 acc_diff2: -0.14489409561845942 acc_ratio2: 0.8376993407503255 acc_diff3: -0.08890072707243492 acc_ratio3: 0.9036006800313894 acc_diff4: -0.06392265874567753 acc_ratio4: 0.9317798017534918 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 39/49 [01:03<00:15,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.5411415100097656,original error is 1.3978984355926514 \n",
      "scan106 0039 abs_diff: 1.5941107273101807 abs_ratio: 1.9039119482040405 acc_diff1: -0.282757019337033 acc_ratio1: 0.6472826907017468 acc_diff2: -0.1453701527923097 acc_ratio2: 0.837350133680987 acc_diff3: -0.08893571084841004 acc_ratio3: 0.903632273156024 acc_diff4: -0.06379460616351364 acc_ratio4: 0.9319534519098129 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 40/49 [01:05<00:13,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.2010130882263184,original error is 1.1197839975357056 \n",
      "scan106 0040 abs_diff: 1.5816013813018799 abs_ratio: 1.9054157733917236 acc_diff1: -0.28419178751905666 acc_ratio1: 0.6458885952763603 acc_diff2: -0.14615226060820574 acc_ratio2: 0.8367050127609943 acc_diff3: -0.08909522949153907 acc_ratio3: 0.9035565417009962 acc_diff4: -0.06379429263969302 acc_ratio4: 0.9320091390362661 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 41/49 [01:06<00:12,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.9479936361312866,original error is 1.0129083395004272 \n",
      "scan106 0041 abs_diff: 1.5662082433700562 abs_ratio: 1.9058386087417603 acc_diff1: -0.28579709771333456 acc_ratio1: 0.6442470125876273 acc_diff2: -0.1472941874447334 acc_ratio2: 0.8357069776229201 acc_diff3: -0.08927801718095844 acc_ratio3: 0.9034712260759468 acc_diff4: -0.06367353179662084 acc_ratio4: 0.9321969724551105 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 42/49 [01:08<00:11,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.773020625114441,original error is 0.9461953043937683 \n",
      "scan106 0042 abs_diff: 1.549013376235962 abs_ratio: 1.9050943851470947 acc_diff1: -0.28683679122556427 acc_ratio1: 0.6433321711322406 acc_diff2: -0.1477376915144674 acc_ratio2: 0.8354208935096615 acc_diff3: -0.08909299564479692 acc_ratio3: 0.9037569736563558 acc_diff4: -0.06322164996111951 acc_ratio4: 0.9327192925210551 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 43/49 [01:10<00:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.5927387475967407,original error is 0.7932613492012024 \n",
      "scan106 0043 abs_diff: 1.5319784879684448 abs_ratio: 1.9074294567108154 acc_diff1: -0.28673270613940216 acc_ratio1: 0.643887656211172 acc_diff2: -0.14735154246068516 acc_ratio2: 0.8359975827847562 acc_diff3: -0.08862838898127792 acc_ratio3: 0.9043263307267836 acc_diff4: -0.06278981140379986 acc_ratio4: 0.9332195383768727 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 44/49 [01:11<00:08,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.7515149116516113,original error is 0.9110279679298401 \n",
      "scan106 0044 abs_diff: 1.516611933708191 abs_ratio: 1.9077658653259277 acc_diff1: -0.28673026245117755 acc_ratio1: 0.6444257135030433 acc_diff2: -0.14669764666427856 acc_ratio2: 0.8368344054957261 acc_diff3: -0.08801866267259681 acc_ratio3: 0.9050334243090907 acc_diff4: -0.062245603741967255 acc_ratio4: 0.9338259108813228 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 45/49 [01:13<00:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.009265422821045,original error is 0.858924388885498 \n",
      "scan106 0045 abs_diff: 1.508649468421936 abs_ratio: 1.9171465635299683 acc_diff1: -0.28638426219333807 acc_ratio1: 0.6454096193077409 acc_diff2: -0.14594920215522772 acc_ratio2: 0.8377655383977151 acc_diff3: -0.08762411247355882 acc_ratio3: 0.9055059026178093 acc_diff4: -0.06208624085745344 acc_ratio4: 0.9340299532384335 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 46/49 [01:14<00:04,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 2.0982420444488525,original error is 1.070530891418457 \n",
      "scan106 0046 abs_diff: 1.4984166622161865 abs_ratio: 1.9180583953857422 acc_diff1: -0.28569730001169086 acc_ratio1: 0.6467408514632952 acc_diff2: -0.14488360626400756 acc_ratio2: 0.8390290550129754 acc_diff3: -0.08679950015442671 acc_ratio3: 0.9064231476114548 acc_diff4: -0.06151223849058253 acc_ratio4: 0.9346551053286685 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 47/49 [01:16<00:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 640) (512, 640) (512, 640) (512, 640)\n",
      "depth modified is 1.6372215747833252,original error is 0.9751362204551697 \n",
      "scan106 0047 abs_diff: 1.4809931516647339 abs_ratio: 1.9130773544311523 acc_diff1: -0.28473383560977816 acc_ratio1: 0.6484306366913034 acc_diff2: -0.14406806400934802 acc_ratio2: 0.8400614693316054 acc_diff3: -0.0863861215958589 acc_ratio3: 0.906931827248536 acc_diff4: -0.061219782074734996 acc_ratio4: 0.93500383231739 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 48/49 [01:17<00:01,  1.58s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Creating depth and confidence predictions...')\n",
    "for scan in [\"scan106\"]:\n",
    "    depth_dir = f'./results/{args.dataset_name}/depth'\n",
    "    depth_dir = os.path.join(depth_dir, scan)\n",
    "\n",
    "    img_dir = f'./results/{args.dataset_name}/image_modified'\n",
    "    img_dir = os.path.join(img_dir, scan)\n",
    "\n",
    "    os.makedirs(depth_dir, exist_ok=True)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    abs_ratio = []\n",
    "    acc_ratio1 = []\n",
    "    acc_ratio2 = []\n",
    "    acc_ratio3 = []\n",
    "    acc_ratio4 = []\n",
    "    acc_diff1 = []\n",
    "    acc_diff2 = []\n",
    "    acc_diff3 = []\n",
    "    acc_diff4 = []\n",
    "    abs_diff = []\n",
    "\n",
    "    data_range = [i for i, x in enumerate(val_data.metas) if x[0] == scan]\n",
    "    print(f'Processing {scan} with {len(data_range)} views')\n",
    "    for i in tqdm(data_range):\n",
    "        batch =  val_data[i]\n",
    "       \n",
    "        imgs, proj_mats, init_depth_min, depth_interval, \\\n",
    "            scan, vid = decode_batch(batch)\n",
    "        proj_mats = proj_mats.unsqueeze(0).to(\"cuda\")\n",
    "        imgs = imgs.unsqueeze(0).to(device)\n",
    "           \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        os.makedirs(os.path.join(depth_dir, scan), exist_ok=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "           \n",
    "            \n",
    "            \n",
    "            if refine == True:\n",
    "                # whether image exist or not\n",
    "                modified_path = os.path.join(img_dir, f'{vid:04d}_class6.npy')\n",
    "                if os.path.exists(modified_path):\n",
    "                    np_array = np.load(modified_path)\n",
    "                    modified_imgs = torch.tensor(np_array).unsqueeze(0).cuda()\n",
    "                    results_modified = model(transform(modified_imgs), proj_mats, init_depth_min, depth_interval)\n",
    "                else:\n",
    "                    modified_imgs = process_batch(pipeline,batch)[0].images\n",
    "                    results_modified = model(transform(modified_imgs), proj_mats, init_depth_min, depth_interval)\n",
    "                imgs_original = imgs[0]\n",
    "                pred_imgs = modified_imgs[0]\n",
    "                torch.stack([imgs_original, pred_imgs], dim=0)\n",
    "                save_image(torch.cat([imgs_original, pred_imgs], dim=0), \n",
    "                           os.path.join(img_dir, f'{vid:04d}_class6.png'))\n",
    "                np.save(os.path.join(img_dir, f'{vid:04d}_class6.npy'), pred_imgs.cpu().numpy())\n",
    "            \n",
    "                results_ori = model(transform(imgs), proj_mats, init_depth_min, depth_interval)\n",
    "\n",
    "                metric_logs = return_log(results_modified,\n",
    "                                        results_ori,\n",
    "                                        val_data[i][\"depths\"][\"level_0\"].numpy(),\n",
    "                                        val_data[i][\"masks\"][\"level_0\"].numpy())\n",
    "                abs_ratio.append(metric_logs[\"abs_ratio\"])\n",
    "                acc_ratio1.append(metric_logs[\"acc_ratio1\"])\n",
    "                acc_ratio2.append(metric_logs[\"acc_ratio2\"])\n",
    "                acc_ratio3.append(metric_logs[\"acc_ratio3\"])\n",
    "                acc_ratio4.append(metric_logs[\"acc_ratio4\"])\n",
    "                acc_diff1.append(metric_logs[\"acc_diff1\"])\n",
    "                acc_diff2.append(metric_logs[\"acc_diff2\"])\n",
    "                acc_diff3.append(metric_logs[\"acc_diff3\"])\n",
    "                acc_diff4.append(metric_logs[\"acc_diff4\"])\n",
    "                abs_diff.append(metric_logs[\"abs_diff\"])\n",
    "\n",
    "                # print output\n",
    "                sys.stdout.write(f'\\r{scan} {vid:04d} '\n",
    "                                f'abs_diff: {np.mean(abs_diff)} '\n",
    "                                f'abs_ratio: {np.mean(abs_ratio)} '\n",
    "                                f'acc_diff1: {np.mean(acc_diff1)} '\n",
    "                                f'acc_ratio1: {np.mean(acc_ratio1)} '\n",
    "                                f'acc_diff2: {np.mean(acc_diff2)} '\n",
    "                                f'acc_ratio2: {np.mean(acc_ratio2)} '\n",
    "                                f'acc_diff3: {np.mean(acc_diff3)} '\n",
    "                                f'acc_ratio3: {np.mean(acc_ratio3)} '\n",
    "                                f'acc_diff4: {np.mean(acc_diff4)} '\n",
    "                                f'acc_ratio4: {np.mean(acc_ratio4)} ')\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                results_ori = model(transform(imgs), proj_mats, init_depth_min, depth_interval)\n",
    "\n",
    "\n",
    "            \n",
    "        if refine == True:\n",
    "            depth = results_modified['depth_0'][0].cpu().numpy()\n",
    "            depth = np.nan_to_num(depth)\n",
    "            proba = results_modified['confidence_2'][0].cpu().numpy()\n",
    "            proba = np.nan_to_num(proba)\n",
    "            save_pfm(os.path.join(depth_dir, f'{scan}/depth_refined_{vid:04d}.pfm'), depth)\n",
    "            save_pfm(os.path.join(depth_dir, f'{scan}/proba_refined_{vid:04d}.pfm'), proba)\n",
    "        else:   \n",
    "            depth = results_ori['depth_0'][0].cpu().numpy()\n",
    "            depth = np.nan_to_num(depth) # change nan to 0\n",
    "            proba = results_ori['confidence_2'][0].cpu().numpy() # NOTE: this is 1/4 scale!\n",
    "            proba = np.nan_to_num(proba) # change nan to 0\n",
    "            save_pfm(os.path.join(depth_dir, f'{scan}/depth_{vid:04d}.pfm'), depth)\n",
    "            save_pfm(os.path.join(depth_dir, f'{scan}/proba_{vid:04d}.pfm'), proba)\n",
    "        if args.save_visual:\n",
    "            mi = np.min(depth[depth>0])\n",
    "            ma = np.max(depth)\n",
    "            depth = (depth-mi)/(ma-mi+1e-8)\n",
    "            depth = (255*depth).astype(np.uint8)\n",
    "            depth_img = cv2.applyColorMap(depth, cv2.COLORMAP_JET)\n",
    "            \n",
    "            cv2.imwrite(os.path.join(depth_dir, f'{scan}/depth_visual_{vid:04d}.jpg'),\n",
    "                        depth_img)\n",
    "            cv2.imwrite(os.path.join(depth_dir, f'{scan}/proba_visual_{vid:04d}.jpg'),\n",
    "                        (255*(proba>args.conf)).astype(np.uint8))\n",
    "        del imgs, proj_mats, results_ori\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Step 2. Perform depth filtering and fusion\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m point_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m/points\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(point_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFusing point clouds...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2. Perform depth filtering and fusion\n",
    "point_dir = f'results/{args.dataset_name}/points'\n",
    "os.makedirs(point_dir, exist_ok=True)\n",
    "print('Fusing point clouds...')\n",
    "\n",
    "for scan in [\"scan106\"]:\n",
    "    print(f'Processing {scan} ...')\n",
    "    \n",
    "    # buffers for the final vertices of this scan\n",
    "    vs = []\n",
    "    v_colors = []\n",
    "    # buffers storing the refined data of each ref view\n",
    "    os.makedirs(f'results/{args.dataset_name}/image_refined/{scan}', exist_ok=True)\n",
    "    image_refined = set()\n",
    "    depth_refined = {}\n",
    "    for meta in tqdm(list(filter(lambda x: x[0]==scan and x[2]==0, val_data.metas))[:args.max_ref_views]):\n",
    "       \n",
    "        try:\n",
    "            ref_vid = meta[1]\n",
    "            if ref_vid in image_refined: # not yet refined actually\n",
    "                image_ref = read_refined_image(args.dataset_name, scan, ref_vid)\n",
    "                depth_ref = depth_refined[ref_vid]\n",
    "            else:\n",
    "                if refine:\n",
    "                    img_dir = f'./results/{args.dataset_name}/image_modified/{scan}'\n",
    "                    image_ref = np.load(os.path.join(img_dir, f'{ref_vid:04d}_class6.npy'))[0]\n",
    "                    print(image_ref.shape)\n",
    "                    image_ref *= 255\n",
    "                    image_ref = image_ref.transpose(1,2,0)\n",
    "                    image_ref = image_ref.astype(np.uint8)\n",
    "                    image_ref = cv2.resize(image_ref, tuple(args.img_wh))\n",
    "                    plt.imshow(image_ref)\n",
    "                    plt.show()\n",
    "                    print(image_ref.shape)\n",
    "\n",
    "                else:\n",
    "                    image_ref = read_image(args.dataset_name, args.root_dir, scan, ref_vid)\n",
    "                    image_ref = cv2.resize(image_ref, tuple(args.img_wh),\n",
    "                                            interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB\n",
    "                \n",
    "                if read_gt:\n",
    "                    depth_ref = read_pfm(f'{args.root_dir}/Depths/{scan}/depth_map_{ref_vid:04d}.pfm')[0]\n",
    "                    depth_ref = cv2.resize(depth_ref, tuple(args.img_wh),\n",
    "                                            interpolation=cv2.INTER_LINEAR)\n",
    "                else:\n",
    "                    if refine:\n",
    "                        depth_ref = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                                f'{scan}/{scan}/depth_refined_{ref_vid:04d}.pfm')[0]\n",
    "                        print(depth_ref.shape)\n",
    "                    else:\n",
    "                        depth_ref = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                            f'{scan}/{scan}/depth_{ref_vid:04d}.pfm')[0]\n",
    "            if read_gt:\n",
    "                proba_ref = np.ones_like(depth_ref)\n",
    "            proba_ref = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                    f'{scan}/{scan}/proba_{ref_vid:04d}.pfm')[0]\n",
    "            proba_ref = cv2.resize(proba_ref, None, fx=4, fy=4,\n",
    "                                    interpolation=cv2.INTER_LINEAR)\n",
    "            mask_conf = proba_ref > args.conf # confidence mask\n",
    "            P_world2ref = read_proj_mat(args.dataset_name, val_data, scan, ref_vid)\n",
    "            \n",
    "            src_vids = meta[3]\n",
    "            mask_geos = []\n",
    "            depth_ref_reprojs = [depth_ref]\n",
    "            image_src2refs = [image_ref]\n",
    "            # for each src view, check the consistency and refine depth\n",
    "            for src_vid in src_vids:\n",
    "                if src_vid in image_refined: # use refined data of previous runs\n",
    "                    image_src = read_refined_image(args.dataset_name, scan, src_vid)\n",
    "                    depth_src = depth_refined[src_vid]\n",
    "                else:\n",
    "                    if refine:\n",
    "                        img_dir = f'./results/{args.dataset_name}/image_modified/{scan}'\n",
    "                        image_src = np.load(os.path.join(img_dir, f'{src_vid:04d}_class6.npy'))[0]\n",
    "                        print(image_src.shape)\n",
    "                        image_src *= 255\n",
    "                        image_src = image_src.transpose(1,2,0)\n",
    "                        image_src= image_src.astype(np.uint8)\n",
    "                        image_src = cv2.resize(image_src, tuple(args.img_wh))\n",
    "                    else: \n",
    "                        image_src = read_image(args.dataset_name, args.root_dir, scan, src_vid)\n",
    "                        image_src = cv2.resize(image_src, tuple(args.img_wh),\n",
    "                                                interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if read_gt:\n",
    "                        depth_src = read_pfm(f'{args.root_dir}/Depths/{scan}/depth_map_{src_vid:04d}.pfm')[0]\n",
    "                        depth_src = cv2.resize(depth_src, tuple(args.img_wh),\n",
    "                                                interpolation=cv2.INTER_LINEAR)\n",
    "                    else:\n",
    "                        if refine:\n",
    "                            depth_src = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                                f'{scan}/{scan}/depth_refined_{src_vid:04d}.pfm')[0]\n",
    "                        else:\n",
    "                            depth_src = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                                f'{scan}/{scan}/depth_{src_vid:04d}.pfm')[0]\n",
    "                        \n",
    "\n",
    "                   \n",
    "                    depth_refined[src_vid] = depth_src\n",
    "                P_world2src = read_proj_mat(args.dataset_name, val_data, scan, src_vid)\n",
    "                depth_ref_reproj, mask_geo, image_src2ref = \\\n",
    "                    check_geo_consistency(depth_ref, P_world2ref,\n",
    "                                            depth_src, P_world2src,\n",
    "                                            image_ref, image_src, tuple(args.img_wh))\n",
    "                depth_ref_reprojs += [depth_ref_reproj]\n",
    "                image_src2refs += [image_src2ref]\n",
    "                mask_geos += [mask_geo]\n",
    "            mask_geo_sum = np.sum(mask_geos, 0)\n",
    "            mask_geo_final = mask_geo_sum >= args.min_geo_consistent\n",
    "            depth_refined[ref_vid] = \\\n",
    "                (np.sum(depth_ref_reprojs, 0)/(mask_geo_sum+1)).astype(np.float32)\n",
    "            image_refined_ = \\\n",
    "                np.sum(image_src2refs, 0)/np.expand_dims((mask_geo_sum+1), -1)\n",
    "\n",
    "            image_refined.add(ref_vid)\n",
    "            save_refined_image(image_refined_, args.dataset_name, scan, ref_vid)\n",
    "            mask_final = mask_conf & mask_geo_final\n",
    "            \n",
    "            # create the final points\n",
    "            xy_ref = np.mgrid[:args.img_wh[1],:args.img_wh[0]][::-1]\n",
    "            xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_refined[ref_vid]\n",
    "            xyz_ref = xyz_ref.transpose(1,2,0)[mask_final].T # (3, N)\n",
    "            color = image_refined_[mask_final] # (N, 3)\n",
    "            xyz_ref_h = np.vstack((xyz_ref, np.ones_like(xyz_ref[:1])))\n",
    "            xyz_world = (np.linalg.inv(P_world2ref) @ xyz_ref_h).T # (N, 4)\n",
    "            xyz_world = xyz_world[::args.skip, :3]\n",
    "            color = color[::args.skip]\n",
    "            \n",
    "            # append to buffers\n",
    "            vs += [xyz_world]\n",
    "            v_colors += [color]\n",
    "\n",
    "        except Exception as e:\n",
    "            # some scenes might not have depth prediction due to too few valid src views\n",
    "            \n",
    "            print(f'Error: {e}')\n",
    "    # clear refined buffer\n",
    "    image_refined.clear()\n",
    "    depth_refined.clear()\n",
    "    shutil.rmtree(f'results/{args.dataset_name}/image_refined/{scan}')\n",
    "\n",
    "    # process all points in the buffers\n",
    "    vs = np.ascontiguousarray(np.vstack(vs).astype(np.float32))\n",
    "    v_colors = np.vstack(v_colors).astype(np.uint8)\n",
    "    print(f'{scan} contains {len(vs)/1e6:.2f} M points')\n",
    "    vs.dtype = [('x', 'f4'), ('y', 'f4'), ('z', 'f4')]\n",
    "    v_colors.dtype = [('red', 'u1'), ('green', 'u1'), ('blue', 'u1')]\n",
    "\n",
    "    vertex_all = np.empty(len(vs), vs.dtype.descr+v_colors.dtype.descr)\n",
    "    for prop in vs.dtype.names:\n",
    "        vertex_all[prop] = vs[prop][:, 0]\n",
    "    for prop in v_colors.dtype.names:\n",
    "        vertex_all[prop] = v_colors[prop][:, 0]\n",
    "    if read_gt:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}_gt.ply')\n",
    "    elif refine:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}_refine.ply')\n",
    "    \n",
    "    else:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}.ply')\n",
    "    del vertex_all, vs, v_colors\n",
    "shutil.rmtree(f'results/{args.dataset_name}/image_refined')\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
