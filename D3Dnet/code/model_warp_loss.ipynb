{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/project/dp_simple/')\n",
    "#import ViT\n",
    "from torchvision import transforms as T\n",
    "from CasMVSNet_pl.models.mvsnet import CascadeMVSNet\n",
    "from CasMVSNet_pl.utils import load_ckpt\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset  \n",
    "from CasMVSNet_pl.utils import *\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset \n",
    "from CasMVSNet_pl.metrics import *  \n",
    "from inplace_abn import ABN\n",
    "from utils.utils import *\n",
    "import pytorch_ssim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "sys.path.append('/root/autodl-tmp/D3Dnet/code')\n",
    "import matplotlib.pyplot as plt\n",
    "from dcn.modules.deform_conv import *\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from einops import rearrange\n",
    "from torchvision import models\n",
    "import sys\n",
    "sys.path.append(\"/root/autodl-tmp/Self-Supervised-MVS/jdacs-ms\")\n",
    "\n",
    "from Losses.unsup_seg_loss import UnSupSegLoss\n",
    "from Losses.unsup_loss import UnSupLoss\n",
    "def abs_error(depth_pred, depth_gt, mask):\n",
    "    depth_pred, depth_gt = depth_pred[mask], depth_gt[mask]\n",
    "    return (depth_pred - depth_gt).abs()\n",
    "\n",
    "def acc_threshold(depth_pred, depth_gt, mask, threshold):\n",
    "    \"\"\"\n",
    "    computes the percentage of pixels whose depth error is less than @threshold\n",
    "    \"\"\"\n",
    "    errors = abs_error(depth_pred, depth_gt, mask)\n",
    "    acc_mask = errors < threshold\n",
    "    return acc_mask.float()\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
    "        return out\n",
    "\n",
    "class SL1Loss(nn.Module):\n",
    "    def __init__(self, levels=3):\n",
    "        super(SL1Loss, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.loss = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, inputs, targets, masks):\n",
    "        loss = 0\n",
    "        for l in range(self.levels):\n",
    "            depth_pred_l = inputs[f'depth_{l}']\n",
    "            depth_gt_l = targets[f'level_{l}']\n",
    "            mask_l = masks[f'level_{l}']\n",
    "            loss += self.loss(depth_pred_l[mask_l], depth_gt_l[mask_l]) * 2**(1-l)\n",
    "        return loss\n",
    "\n",
    "loss_dict = {'sl1': SL1Loss}\n",
    "\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self,configs):\n",
    "        super(Net, self).__init__()\n",
    "        self.upscale_factor = configs.upscale_factor\n",
    "        self.in_channel = configs.in_channel\n",
    "        out_channel = configs.out_channel\n",
    "        nf = configs.nf\n",
    "        in_channel = configs.in_channel\n",
    "        upscale_factor = configs.upscale_factor\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channel, out_channels=nf, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "        self.vgg = VGG16()\n",
    "        self.residual_layer = self.make_layer(functools.partial(ResBlock_3d, nf,dropout = 0.1), configs.num_groups1)\n",
    "        self.TA = nn.Conv2d(3 * nf, nf, 1, 1, bias=True)\n",
    "        ### reconstruct\n",
    "        self.reconstruct = self.make_layer(functools.partial(ResBlock, nf,dropout = 0.1), configs.num_groups2)\n",
    "        ###upscale\n",
    "        self.upscale = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf * upscale_factor ** 2, 1, 1, 0, bias=False),\n",
    "            nn.PixelShuffle(upscale_factor),\n",
    "            nn.Conv2d(nf, out_channel, 3, 1, 1, bias=False),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False)\n",
    "        )\n",
    "        self.lambda_content = configs.lambda_content\n",
    "\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "        self.depthmodel = configs.model\n",
    "        for param in self.depthmodel.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.depth_loss = loss_dict['sl1'](levels=3)\n",
    "        self.lr = configs.lr\n",
    "        self.unpreprocess = T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n",
    "                                        std=[1/0.229, 1/0.224, 1/0.225])\n",
    "        self.unsup = UnSupLoss()\n",
    "        self.unsup_seg = UnSupSegLoss(configs)\n",
    "\n",
    "    def make_layer(self, block, num_of_layer):\n",
    "        layers = []\n",
    "        for _ in range(num_of_layer):\n",
    "            layers.append(block())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n, h, w = x.size()\n",
    "        residual = rearrange(x,'b c n h w -> b (c n) h w')\n",
    "        out = self.input(x)\n",
    "        out = self.residual_layer(out)\n",
    "        out = self.TA(out.permute(0,2,1,3,4).contiguous().view(b, -1, h, w))  # B, C, H, W\n",
    "        out = self.reconstruct(out)\n",
    "        ###upscale\n",
    "        out = self.upscale(out)\n",
    "        out = torch.add(out, residual)\n",
    "        out = out.reshape(b,c,n,h,w)\n",
    "        return out\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "        monitor = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor\n",
    "            }\n",
    "        }\n",
    "    def decode_batch(self, batch):\n",
    "        imgs = batch['imgs']\n",
    "        proj_mats = batch['proj_mats']\n",
    "        depths = batch['depths']\n",
    "        masks = batch['masks']\n",
    "        init_depth_min = batch['init_depth_min']\n",
    "        depth_interval = batch['depth_interval']\n",
    "        return imgs, proj_mats, depths, masks, init_depth_min, depth_interval\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, proj_mats, depths, masks, init_depth_min, depth_interval = self.decode_batch(batch)\n",
    "        imgs = imgs.transpose(1, 2)\n",
    "        new_imgs = self.forward(imgs)\n",
    "\n",
    "        results = self.depthmodel(new_imgs.transpose(1, 2), proj_mats, init_depth_min, depth_interval)\n",
    "        result_original = self.depthmodel(imgs.transpose(1,2), proj_mats, init_depth_min, depth_interval)\n",
    "        loss_original = self.calculate_depthloss(result_original, depths, masks)\n",
    "        loss_depth = self.calculate_depthloss(results, depths, masks)\n",
    "        content_loss = self.calculate_contentloss(new_imgs, imgs)*self.lambda_content\n",
    "\n",
    "        cams = batch[\"cams\"]\n",
    "        #depth = batch[\"depth\"]\n",
    "        imgs_new = new_imgs.transpose(1, 2)\n",
    "        depth = results['depth_0']\n",
    "\n",
    "        unsuploss = self.unsup(imgs_new, cams, depth)\n",
    "        unsupsegloss = self.unsup_seg(imgs_new, cams, depth)[0]\n",
    "        \n",
    "       \n",
    "        loss =  unsuploss + unsupsegloss  + content_loss\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log('train/content_loss', content_loss, on_step=True, on_epoch=True)\n",
    "        self.log('train/unsup_loss', unsuploss, on_step=True, on_epoch=True)\n",
    "        self.log('train/unsupsegloss',unsupsegloss , on_step=True, on_epoch=True)\n",
    "       \n",
    "        log ={}\n",
    "        with torch.no_grad():\n",
    "            if batch_idx%10 == 0:\n",
    "                try:\n",
    "                    imgs_new = new_imgs.transpose(1, 2)\n",
    "                    img_ = self.unpreprocess(imgs_new[0,0]).cpu() # batch 0, ref image\n",
    "                    depth_gt_ = visualize_depth(depths['level_0'][0])\n",
    "                    depth_pred_ = visualize_depth(results['depth_0'][0]*masks['level_0'][0])\n",
    "                    prob = visualize_prob(results['confidence_0'][0]*masks['level_0'][0])\n",
    "                    stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
    "                    self.logger.experiment.add_images('train/image_pred_prob',\n",
    "                                                    stack, self.global_step)\n",
    "                    \n",
    "                    imgs = imgs.transpose(1, 2)\n",
    "                    img_ = self.unpreprocess(imgs[0,0]).cpu() # batch 0, ref image\n",
    "                    depth_gt_ = visualize_depth(depths['level_0'][0])\n",
    "                    depth_pred_ = visualize_depth(result_original['depth_0'][0]*masks['level_0'][0])\n",
    "                    prob = visualize_prob(result_original['confidence_0'][0]*masks['level_0'][0])\n",
    "                    stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
    "                    self.logger.experiment.add_images('train/image_GT_prob_old',\n",
    "                                                    stack, self.global_step)\n",
    "                    log['error'] =0\n",
    "\n",
    "                    # visualize output\n",
    "                    log_imgs = new_imgs.transpose(1, 2)\n",
    "                    log_imgs = rearrange(log_imgs, 'b n c h w -> (b n) c h w')\n",
    "                    log_imgs = self.unpreprocess(log_imgs)\n",
    "                    self.logger.experiment.add_images('train/output', log_imgs, self.global_step)\n",
    "                except:\n",
    "                    log['error'] = 1\n",
    "\n",
    "            depth_pred = results['depth_0']\n",
    "            depth_old = result_original['depth_0']\n",
    "            depth_gt = depths['level_0']\n",
    "            mask = masks['level_0']\n",
    "            log['train/abs_err'] = abs_err = abs_error(depth_pred, depth_gt, mask).mean()\n",
    "            log['train/acc_1mm'] = acc_threshold(depth_pred, depth_gt, mask, 1).mean()\n",
    "            log['train/acc_2mm'] = acc_threshold(depth_pred, depth_gt, mask, 2).mean()\n",
    "            log['train/acc_4mm'] = acc_threshold(depth_pred, depth_gt, mask, 4).mean()\n",
    "            log['train/abs_err_old'] = abs_error(depth_old, depth_gt, mask).mean()\n",
    "            log['train/acc_1mm_old'] = acc_threshold(depth_old, depth_gt, mask, 1).mean()\n",
    "            log['train/acc_2mm_old'] = acc_threshold(depth_old, depth_gt, mask, 2).mean()\n",
    "            log['train/acc_4mm_old'] = acc_threshold(depth_old, depth_gt, mask, 4).mean()\n",
    "            # the ratio of the loss\n",
    "            log['train/abs_err_ratio'] = abs_err/(1e-10 + abs_error(depth_old, depth_gt, mask).mean())\n",
    "            log['train/acc_1mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 1).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 1).mean())\n",
    "            log['train/acc_2mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 2).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 2).mean())\n",
    "            log['train/acc_4mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 4).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 4).mean())\n",
    "            self.log_dict(log, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "    def predict_step(self, imgs):\n",
    "        transform = T.Compose([T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225]),\n",
    "                                ])\n",
    "        imgs = transform(imgs)\n",
    "        imgs = imgs.transpose(1, 2)\n",
    "        new_imgs = self.forward(imgs)\n",
    "        new_imgs = new_imgs.transpose(1, 2)\n",
    "        return new_imgs\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       \n",
    "        imgs, proj_mats, depths, masks, init_depth_min, depth_interval = self.decode_batch(batch)\n",
    "        imgs = imgs.transpose(1, 2)\n",
    "        new_imgs = self.forward(imgs)\n",
    "        results = self.depthmodel(new_imgs.transpose(1, 2), proj_mats, init_depth_min, depth_interval)\n",
    "        result_original = self.depthmodel(imgs.transpose(1,2), proj_mats, init_depth_min, depth_interval)\n",
    "        loss_original = self.calculate_depthloss(result_original, depths, masks)\n",
    "        loss_depth = self.calculate_depthloss(results, depths, masks)\n",
    "\n",
    "        new_imgs = new_imgs.transpose(1, 2) # b, n, c, h, w\n",
    "        imgs = imgs.transpose(1, 2)\n",
    "        if batch_idx%10 == 0:\n",
    "\n",
    "            denormalize = T.Compose([T.Normalize(mean=[0., 0., 0.],\n",
    "                                                std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "                                    T.Normalize(mean=[-0.485, -0.456, -0.406],\n",
    "                                                std=[1., 1., 1.]),\n",
    "                                    ])\n",
    "            new_img = denormalize(new_imgs[0])\n",
    "            \n",
    "            \n",
    "            new_img= rearrange(new_img, 'n c h w -> c h (n w) ')\n",
    "            self.logger.experiment.add_image('val_output', new_img, self.global_step)\n",
    "\n",
    "        \n",
    "        cams = batch[\"cams\"]\n",
    "        depth = batch[\"depth\"]\n",
    "        imgs_new = new_imgs\n",
    "\n",
    "        unsuploss = self.unsup(imgs_new, cams, depth)\n",
    "        unsupsegloss = self.unsup_seg(imgs_new, cams, depth)[0]\n",
    "        \n",
    "        self.log('val/unsup_loss', unsuploss, on_step=True, on_epoch=True)\n",
    "        self.log('val/unsupsegloss',unsupsegloss , on_step=True, on_epoch=True)\n",
    "        \n",
    "        log= {}\n",
    "        with torch.no_grad():\n",
    "            if batch_idx%10 == 0:\n",
    "                try:\n",
    "                    img_ = self.unpreprocess(new_imgs[0,0]).cpu() # batch 0, ref image\n",
    "                    depth_gt_ = visualize_depth(depths['level_0'][0])\n",
    "                    depth_pred_ = visualize_depth(results['depth_0'][0]*masks['level_0'][0])\n",
    "                    prob = visualize_prob(results['confidence_0'][0]*masks['level_0'][0])\n",
    "                    stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
    "                    self.logger.experiment.add_images('val/image_pred_prob',\n",
    "                                                    stack, self.global_step)\n",
    "                    \n",
    "                \n",
    "                    img_ = self.unpreprocess(imgs[0,0]).cpu() # batch 0, ref image\n",
    "                    depth_gt_ = visualize_depth(depths['level_0'][0])\n",
    "                    depth_pred_ = visualize_depth(result_original['depth_0'][0]*masks['level_0'][0])\n",
    "                    prob = visualize_prob(result_original['confidence_0'][0]*masks['level_0'][0])\n",
    "                    stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
    "                    self.logger.experiment.add_images('val/image_GT_prob_old',\n",
    "                                                    stack, self.global_step)\n",
    "                    log['error'] =0\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    log['error'] = 1\n",
    "\n",
    "            depth_pred = results['depth_0']\n",
    "            depth_old = result_original['depth_0']\n",
    "            depth_gt = depths['level_0']\n",
    "            mask = masks['level_0']\n",
    "            abs_err = abs_error(depth_pred, depth_gt, mask).mean()\n",
    "            log['val/abs_err'] = abs_err \n",
    "            log['val/acc_1mm'] = acc_threshold(depth_pred, depth_gt, mask, 1).mean()\n",
    "            log['val/acc_2mm'] = acc_threshold(depth_pred, depth_gt, mask, 2).mean()\n",
    "            log['val/acc_4mm'] = acc_threshold(depth_pred, depth_gt, mask, 4).mean()\n",
    "            log['val/abs_err_old'] = abs_error(depth_old, depth_gt, mask).mean()\n",
    "            log['val/acc_1mm_old'] = acc_threshold(depth_old, depth_gt, mask, 1).mean()\n",
    "            log['val/acc_2mm_old'] = acc_threshold(depth_old, depth_gt, mask, 2).mean()\n",
    "            log['val/acc_4mm_old'] = acc_threshold(depth_old, depth_gt, mask, 4).mean()\n",
    "            # the ratio of the loss\n",
    "            log['val/abs_err_ratio'] = abs_err/(1e-10 + abs_error(depth_old, depth_gt, mask).mean())\n",
    "            log['val/acc_1mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 1).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 1).mean())\n",
    "            log['val/acc_2mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 2).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 2).mean())\n",
    "            log['val/acc_4mm_ratio'] = acc_threshold(depth_pred, depth_gt, mask, 4).mean()/(1e-10 + acc_threshold(depth_old, depth_gt, mask, 4).mean())\n",
    "            self.log_dict(log, on_epoch=True, on_step=True)\n",
    "        self.log('val_loss', log['val/abs_err_ratio'], on_step=True, on_epoch=True)\n",
    "        return {'loss': log['val/acc_1mm'],\n",
    "                'progress_bar': {'train_abs_err': abs_err},\n",
    "                'log': log\n",
    "               }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calculate_depthloss(self, results, depths, masks):\n",
    "        depth_loss = self.depth_loss(results, depths, masks)\n",
    "        return depth_loss\n",
    "    \n",
    "    def calculate_contentloss(self, x_transform, x_original):\n",
    "\n",
    "        # what we wanted is : the transformed image smooth, don't have many high frequency noise, abrubt changes\n",
    "        # the changes on original images should be proportional to the difference of predicted depth and gt depth\n",
    "        # gradient smootheness\n",
    "        b,c,d,h,w = x_transform.shape\n",
    "        x_transform = rearrange(x_transform,'b c d h w -> (b d) c h w')\n",
    "        x_original = rearrange(x_original,'b c d h w -> (b d) c h w')\n",
    "\n",
    "        # ssim_loss = pytorch_ssim.SSIM(window_size=11)\n",
    "        # ssim_loss = 1 - ssim_loss(x_transform,x_original)\n",
    "\n",
    "        \n",
    "        features_original = self.vgg(x_original)\n",
    "        features_transformed = self.vgg(x_transform)\n",
    "        content_loss = self.l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n",
    "        return content_loss\n",
    "    \n",
    "\n",
    "\n",
    "class ResBlock_3d(nn.Module):\n",
    "    def __init__(self, nf,dropout = 0.1):\n",
    "        super(ResBlock_3d, self).__init__()\n",
    "        self.dcn0 = DeformConvPack_d(nf, nf, kernel_size=3, stride=1, padding=1, dimension='HW')\n",
    "        self.dcn1 = DeformConvPack_d(nf, nf, kernel_size=3, stride=1, padding=1, dimension='HW')\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.dcn1(self.lrelu(self.dcn0(x)))) + x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, nf,dropout = 0.1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.dcn0 = nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1)\n",
    "        self.dcn1 = nn.Conv2d(nf, nf, kernel_size=3, stride=1, padding=1)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.dropout(self.dcn1(self.lrelu(self.dcn0(x)))) + x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trial,\n",
    "                    lambda_content, upscale_factor,\n",
    "                    in_channel, out_channel, model,):\n",
    "    \n",
    "    nf = trial.nf\n",
    "    lr = trial.lr\n",
    "    num_groups2 =trial.num_groups2\n",
    "    num_groups1 = trial.num_groups1\n",
    "\n",
    "    print(f'nf: {nf}, lambda_content: {lambda_content}, \\\n",
    "            upscale_factor:{upscale_factor}, in_channel: {in_channel}, \\\n",
    "            out_channel: {out_channel}, lr: {lr}')\n",
    "\n",
    "    configs = namedtuple('configs', ['nf',\n",
    "                                        'lambda_content', \n",
    "                                        'upscale_factor',\n",
    "                                         'seg_clusters', 'in_channel', 'out_channel', 'model','lr'])\n",
    "    configs.nf = nf\n",
    "    configs.lambda_content = lambda_content\n",
    "    configs.upscale_factor = upscale_factor\n",
    "    configs.in_channel = in_channel\n",
    "    configs.out_channel = out_channel\n",
    "    configs.model = model\n",
    "    configs.num_groups1 = num_groups1\n",
    "    configs.num_groups2 = num_groups2\n",
    "    configs.lr = lr\n",
    "    configs.seg_clusters=4\n",
    "\n",
    "    return Net(configs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtudataset(DTUDataset):\n",
    "    def __len__(self):\n",
    "        return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CascadeMVSNet(n_depths=[8,32,48],\n",
    "                        interval_ratios=[1.0,2.0,4.0],\n",
    "                        num_groups=1,\n",
    "                        \n",
    "                        norm_act=ABN).cuda()\n",
    "load_ckpt(model, '/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/ckpts/_ckpt_epoch_10.ckpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf: 64, lambda_content: 100,             upscale_factor:1, in_channel: 3,             out_channel: 9, lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "trial = namedtuple('trial', ['nf', 'lr', 'num_groups1', 'num_groups2'])\n",
    "trial.nf = 64\n",
    "trial.lr = 1e-4\n",
    "trial.num_groups1 = 5\n",
    "trial.num_groups2 = 6\n",
    "model = build_model(trial, 100,1, 3,9, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = dtudataset('/root/autodl-tmp/mvs_training/dtu/', 'train', img_wh=(256,256))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "    \n",
    "val_dataset = dtudataset('/root/autodl-tmp/mvs_training/dtu/', 'val',img_wh=(256,256))\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "logger = TensorBoardLogger('/root/autodl-tmp/logs', name='d3c2_net')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val/abs_err_ratio',\n",
    "    dirpath='/root/autodl-tmp/project/dp_simple/ckpts/',\n",
    "    filename='d3c_net_warp_{epoch}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_last=True\n",
    "\n",
    "    )\n",
    "early_stop_callback = EarlyStopping(\n",
    "        monitor='val/abs_err_ratio',\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "    )\n",
    "trainer = Trainer(max_epochs=50, \n",
    "                    gpus=1,\n",
    "               \n",
    "                callbacks=[checkpoint_callback, \n",
    "                            #early_stop_callback\n",
    "                            ]\n",
    "                ,      \n",
    "                \n",
    "                val_check_interval=1.0,\n",
    "                logger=logger,\n",
    "                resume_from_checkpoint='/root/autodl-tmp/project/dp_simple/ckpts/d3c_net_warp_epoch=27.ckpt'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/autodl-tmp/project/dp_simple/ckpts/d3c_net_warp_epoch=27.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:247: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint file at /root/autodl-tmp/project/dp_simple/ckpts/d3c_net_warp_epoch=27.ckpt\n",
      "\n",
      "   | Name           | Type          | Params\n",
      "--------------------------------------------------\n",
      "0  | l2_loss        | MSELoss       | 0     \n",
      "1  | input          | Sequential    | 5.2 K \n",
      "2  | vgg            | VGG16         | 7.6 M \n",
      "3  | residual_layer | Sequential    | 2.0 M \n",
      "4  | TA             | Conv2d        | 12.4 K\n",
      "5  | reconstruct    | Sequential    | 443 K \n",
      "6  | upscale        | Sequential    | 10.0 K\n",
      "7  | depthmodel     | CascadeMVSNet | 934 K \n",
      "8  | depth_loss     | SL1Loss       | 0     \n",
      "9  | unpreprocess   | Normalize     | 0     \n",
      "10 | unsup          | UnSupLoss     | 0     \n",
      "11 | unsup_seg      | UnSupSegLoss  | 143 M \n",
      "--------------------------------------------------\n",
      "146 M     Trainable params\n",
      "8.6 M     Non-trainable params\n",
      "154 M     Total params\n",
      "618.991   Total estimated model params size (MB)\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /root/autodl-tmp/project/dp_simple/ckpts/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008838415145874023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation sanity check",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef5f44444754464b13adb25e5a4f4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:440: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006812572479248047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6671a3555e4324a67028f34ee69570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00659942626953125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b767b2d9604d9f8fac11710eba23cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006696939468383789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90cecb45eb840b28cff4c14b572c471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00685882568359375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cf7e441d9e438393bbeb506d7f3571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0066967010498046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec50420855a489184256a09db23bced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00658416748046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validating",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0f8b4982934b5884b0c1188f5f94e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
