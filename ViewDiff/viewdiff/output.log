The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/root/autodl-tmp/instructpix2pix/lib/python3.8/site-packages/tyro/_resolver.py:305: UserWarning: <class 'float'> does not match any type in Union: [<class 'int'>, <class 'NoneType'>]
  warnings.warn(
/root/autodl-tmp/instructpix2pix/lib/python3.8/site-packages/tyro/_fields.py:826: UserWarning: Mutable type <class 'io_util.SaveConfig'> is used as a default value for `save`. This is dangerous! Consider using `dataclasses.field(default_factory=...)` or marking <class 'io_util.SaveConfig'> as frozen.
  warnings.warn(
04/13/2024 17:27:53 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

{'variance_type', 'clip_sample_range', 'timestep_spacing'} was not found in config. Values will be initialized to default values.
{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.
{'attention_type', 'num_attention_heads', 'class_embeddings_concat', 'projection_class_embeddings_input_dim', 'dropout', 'addition_embed_type', 'time_cond_proj_dim', 'addition_embed_type_num_heads', 'timestep_post_act', 'addition_time_embed_dim', 'time_embedding_dim', 'conv_in_kernel', 'encoder_hid_dim', 'time_embedding_act_fn', 'transformer_layers_per_block', 'cross_attention_norm', 'conv_out_kernel', 'time_embedding_type', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'encoder_hid_dim_type', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.
04/13/2024 17:27:54 - INFO - __main__ - Initializing the StableDiffusion3D UNet from the pretrained UNet.
/root/autodl-tmp/ViewDiff/viewdiff
/root/autodl-tmp/ViewDiff/viewdiff
scan scan2 mean output: [1.00121434 0.97192338 0.96325176 0.9620754  0.96730371 0.97451435
 0.98707856]
scan scan6 mean output: [1.11856514 1.0349555  1.01148678 1.00809412 1.00456903 1.02157744
 1.05318288]
scan scan7 mean output: [2.80176354 2.98415808 3.08893367 3.06841932 3.10951912 3.1721381
 2.96063437]
scan scan8 mean output: [1.19103727 1.09251997 1.09845439 1.09647759 1.16365631 1.21807814
 1.26017903]
scan scan14 mean output: [1.97682371 1.88708503 1.85337303 1.86532249 1.89953903 2.03141878
 2.11037798]
scan scan16 mean output: [3.1915682  3.25915756 3.22970628 3.21195235 3.26540529 3.16314249
 3.15736253]
scan scan18 mean output: [2.05124783 2.013102   1.9762233  1.97305491 1.94199959 1.96142664
 2.00164497]
scan scan19 mean output: [1.42494544 1.44790176 1.42633028 1.43245512 1.41559309 1.43743257
 1.49461799]
scan scan20 mean output: [2.16098818 2.17821048 2.15450914 2.14886178 2.14548533 2.15932194
 2.23102642]
scan scan22 mean output: [1.84457468 1.76052218 1.72700193 1.70638893 1.6944377  1.70606693
 1.75517236]
scan scan30 mean output: [8.67458678 8.59489073 8.69847434 8.68814547 8.62376398 8.5837679
 8.51076824]
scan scan31 mean output: [2.31524642 2.25100664 2.26015923 2.24801694 2.2719446  2.35124722
 2.43751027]
scan scan36 mean output: [2.63409314 2.46368888 2.46062461 2.43784512 2.47157738 2.49926177
 2.58666817]
scan scan39 mean output: [2.37594525 2.31825419 2.30938649 2.28579744 2.29160664 2.37218753
 2.46592282]
scan scan41 mean output: [1.66476584 1.60040762 1.59964506 1.59753174 1.62585583 1.64422795
 1.66758125]
scan scan42 mean output: [2.95913485 2.90192703 2.82472118 2.82756103 2.83083466 2.84663147
 3.02270986]
scan scan44 mean output: [2.41167771 2.38853236 2.34733436 2.3156443  2.31195744 2.31628597
 2.38618758]
scan scan45 mean output: [1.66468076 1.67245606 1.68474703 1.67637421 1.65131788 1.71283245
 1.60885482]
scan scan46 mean output: [1.29634672 1.28409807 1.26796138 1.26884852 1.25513599 1.27678512
 1.28780049]
scan scan47 mean output: [11.23890356 10.62560907 10.38173617 10.33011346 10.41427073 10.89797371
 11.45373098]
scan scan50 mean output: [4.78126085 4.6839368  4.68366494 4.67832368 4.68632154 4.81935768
 4.82078136]
scan scan51 mean output: [1.38731192 1.30007031 1.28973332 1.29298134 1.31923183 1.32288761
 1.39191384]
scan scan52 mean output: [6.39765668 6.71479313 7.32903373 7.36236764 7.29158699 6.97721179
 6.38887133]
scan scan53 mean output: [6.57723425 7.27589331 7.38309274 7.2946379  7.4625387  7.67598391
 6.51365198]
scan scan55 mean output: [1.69197897 1.63865039 1.64135865 1.63935601 1.66539767 1.70896921
 1.78418155]
scan scan57 mean output: [1.71960834 1.59272069 1.58916454 1.58068859 1.57162419 1.61019183
 1.64975446]
scan scan58 mean output: [2.24748354 2.17271329 2.12357374 2.09704737 2.13028055 2.16428077
 2.24225527]
scan scan60 mean output: [10.0765839  10.713741   10.83717628 10.72482792 10.65179486 10.05052784
  8.91400745]
scan scan61 mean output: [1.34369714 1.28631924 1.34006934 1.33217524 1.37976855 1.46957162
 1.40087494]
scan scan63 mean output: [7.95549631 8.00294294 7.99245622 7.98088335 8.0994514  8.011342
 7.50288264]
scan scan64 mean output: [9.61688464 9.76545364 9.4716502  9.27641999 9.16097031 8.93420555
 8.70215934]
scan scan65 mean output: [2.47293989 2.1114776  2.03738769 2.03424759 2.05745113 2.09894038
 2.23646499]
scan scan68 mean output: [1.66418732 1.54708171 1.55255918 1.54454011 1.60591144 1.68637406
 1.66911975]
scan scan69 mean output: [2.07839942 1.71989811 1.57556223 1.53391165 1.5258197  1.58260642
 1.71275553]
scan scan70 mean output: [6.08255564 5.70765056 5.58918495 5.5997456  5.63302205 5.86396391
 6.33816176]
scan scan71 mean output: [2.4684212  2.04934821 1.8593077  1.81687432 1.82307946 1.87917591
 1.97769005]
scan scan72 mean output: [5.42979493 4.98342379 4.74852921 4.70286036 4.70392189 4.81169244
 5.190466  ]
scan scan74 mean output: [6.19517582 6.1203795  6.00283794 5.99464523 6.01317631 5.97329591
 5.93384791]
scan scan76 mean output: [2.52154994 2.46967198 2.42213043 2.42903319 2.45165442 2.4932581
 2.67943018]
scan scan83 mean output: [5.78417572 5.60813875 5.5824187  5.59089596 5.61494345 5.75872464
 6.14112679]
scan scan84 mean output: [4.81248867 4.73980808 4.64406028 4.62658724 4.64393221 4.72740636
 5.06771795]
scan scan85 mean output: [8.09187486 5.58834374 5.07499904 4.93354202 5.00643232 5.12534594
 5.44755901]
scan scan87 mean output: [10.27863458  9.28198363  8.86244189  8.76100429  8.95012405  9.33165627
 11.22065983]
scan scan88 mean output: [12.53427557 11.66079711 11.2207347  11.16631215 11.3067646  12.05679975
 13.51323437]
scan scan89 mean output: [3.10886091 1.84053157 1.57470665 1.4852684  1.43455578 1.45991223
 1.76265765]
scan scan90 mean output: [2.91044355 2.14173189 1.87538047 1.80021106 1.68886491 1.86459352
 2.09340844]
scan scan91 mean output: [4.1554668  3.15375396 2.79960795 2.60254759 2.62762953 2.77030982
 3.16996982]
scan scan92 mean output: [3.44822968 2.92558409 2.46401183 2.36850045 2.37061088 2.37754278
 2.6572957 ]
scan scan93 mean output: [2.47878795 2.42562657 2.3478494  2.33718573 2.31991502 2.36878802
 2.54261788]
scan scan94 mean output: [2.25050864 2.15269626 2.12538479 2.10822735 2.11139564 2.16234145
 2.31182622]
scan scan95 mean output: [3.02138372 2.92085865 2.87007221 2.83432269 2.89923202 2.94220613
 3.32199339]
scan scan96 mean output: [3.56508471 3.47703425 3.44893682 3.41940667 3.42401943 3.53237321
 3.7412994 ]
scan scan97 mean output: [2.74131069 2.66731166 2.65141736 2.64831129 2.63753103 2.69091483
 2.84743998]
scan scan98 mean output: [4.32465715 4.21557355 4.19174789 4.20941    4.23795416 4.30376435
 4.56633065]
scan scan99 mean output: [1.89460801 1.83698884 1.81319355 1.80834472 1.81198793 1.83644629
 1.96337358]
scan scan100 mean output: [4.37227584 4.20406946 4.1266844  4.09965451 4.13235198 4.19584484
 4.44129949]
scan scan101 mean output: [10.98790638 10.51371385 10.07423857 10.14579899 10.08343843 10.57270488
 11.77931405]
scan scan102 mean output: [14.89902821 14.11591781 13.57399777 13.26589634 13.29806149 13.27530462
 13.84596409]
scan scan103 mean output: [1.96966011 1.88718121 1.85632959 1.84724409 1.85666614 1.89407913
 2.0532192 ]
scan scan104 mean output: [8.07298178 7.76581244 7.65959314 7.45407124 7.64064978 7.87105531
 8.93807717]
scan scan105 mean output: [3.92992292 3.8236729  3.78583217 3.81376588 3.82348662 3.92253066
 4.54122754]
scan scan107 mean output: [2.95482818 2.72223734 2.62752161 2.57842151 2.58153874 2.7260572
 2.93490657]
scan scan108 mean output: [1.50086186 1.42920616 1.39831006 1.38961969 1.39941372 1.4443991
 1.54088434]
scan scan109 mean output: [2.30197851 2.20360578 2.13959853 2.1302663  2.15524475 2.31695009
 2.64638124]
scan scan111 mean output: [3.53442634 3.37920633 3.31997709 3.28886725 3.31675281 3.38871323
 3.67240878]
scan scan112 mean output: [5.59327961 5.28310663 5.14031178 5.14182902 5.1483838  5.27757189
 5.69128932]
scan scan113 mean output: [5.65039545 5.24060912 5.12833628 5.10867552 5.09432419 5.23226616
 5.79506215]
scan scan115 mean output: [1.10287945 1.05027054 1.00521698 1.00482054 1.01658114 1.06764755
 1.16337927]
scan scan116 mean output: [1.26797312 1.20659617 1.16379642 1.14963802 1.16327593 1.16962385
 1.19768231]
scan scan119 mean output: [2.96742071 2.68487256 2.58232683 2.5617083  2.58739374 2.68868351
 2.96048615]
scan scan120 mean output: [1.62734547 1.45432689 1.36843849 1.3507711  1.34210405 1.37754391
 1.58746313]
scan scan121 mean output: [2.45077215 2.2816408  2.19525332 2.18318168 2.20049704 2.27198017
 2.69546309]
scan scan122 mean output: [1.73261188 1.5833986  1.51542548 1.49187345 1.49378712 1.53200495
 1.73979831]
scan scan123 mean output: [1.42056491 1.26430274 1.21997888 1.23058494 1.21519014 1.27313657
 1.45864995]
scan scan124 mean output: [1.50513161 1.40515696 1.3793114  1.36679706 1.37046543 1.43902769
 1.61628655]
scan scan125 mean output: [1.84503571 1.68809586 1.65205791 1.62550835 1.64884693 1.7013044
 1.82643247]
04/13/2024 17:28:16 - INFO - __main__ - ***** Running training *****
04/13/2024 17:28:16 - INFO - __main__ -   Num examples = 27097
04/13/2024 17:28:16 - INFO - __main__ -   Num Epochs = 50
04/13/2024 17:28:16 - INFO - __main__ -   Instantaneous batch size per device = 4
04/13/2024 17:28:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
04/13/2024 17:28:16 - INFO - __main__ -   Gradient Accumulation steps = 1
04/13/2024 17:28:16 - INFO - __main__ -   Total optimization steps = 338750
04/13/2024 17:28:16 - INFO - accelerate.accelerator - Loading states from /root/autodl-tmp/output_var_unproj_dtu2/all/subset_all/input_3/train/class6/checkpoint-10000/
Some weights of the model checkpoint were not used when initializing UNet2DConditionCrossFrameInExistingAttnModel: 
 ['down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight']
04/13/2024 17:28:19 - INFO - __main__ - Loaded lora parameters into model
04/13/2024 17:28:19 - INFO - accelerate.checkpointing - All model weights loaded successfully
