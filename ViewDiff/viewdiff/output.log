The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/workspace/viewdiff/lib/python3.10/site-packages/tyro/_resolver.py:305: UserWarning: <class 'float'> does not match any type in Union: [<class 'int'>, <class 'NoneType'>]
  warnings.warn(
/workspace/viewdiff/lib/python3.10/site-packages/tyro/_fields.py:826: UserWarning: Mutable type <class 'viewdiff.io_util.SaveConfig'> is used as a default value for `save`. This is dangerous! Consider using `dataclasses.field(default_factory=...)` or marking <class 'viewdiff.io_util.SaveConfig'> as frozen.
  warnings.warn(
03/25/2024 14:48:21 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: bf16

{'clip_sample_range', 'variance_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.
{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.
{'resnet_out_scale_factor', 'time_cond_proj_dim', 'projection_class_embeddings_input_dim', 'addition_embed_type', 'time_embedding_dim', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'addition_embed_type_num_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'encoder_hid_dim', 'time_embedding_type', 'mid_block_only_cross_attention', 'dropout', 'resnet_skip_time_act', 'class_embeddings_concat', 'timestep_post_act', 'num_attention_heads', 'attention_type', 'addition_time_embed_dim', 'conv_out_kernel', 'conv_in_kernel'} was not found in config. Values will be initialized to default values.
03/25/2024 14:48:22 - INFO - __main__ - Initializing the StableDiffusion3D UNet from the pretrained UNet.
/workspace/ViewDiff/viewdiff
/workspace/ViewDiff/viewdiff
scan scan2 mean output: [1.00121434 0.97192338 0.96325176 0.9620754  0.96730371 0.97451435
 0.98707856]
scan scan6 mean output: [1.11856514 1.0349555  1.01148678 1.00809412 1.00456903 1.02157744
 1.05318288]
scan scan7 mean output: [2.80176354 2.98415808 3.08893367 3.06841932 3.10951912 3.1721381
 2.96063437]
scan scan8 mean output: [1.19103727 1.09251997 1.09845439 1.09647759 1.16365631 1.21807814
 1.26017903]
scan scan14 mean output: [1.97682371 1.88708503 1.85337303 1.86532249 1.89953903 2.03141878
 2.11037798]
scan scan16 mean output: [3.1915682  3.25915756 3.22970628 3.21195235 3.26540529 3.16314249
 3.15736253]
scan scan18 mean output: [2.05124783 2.013102   1.9762233  1.97305491 1.94199959 1.96142664
 2.00164497]
scan scan19 mean output: [1.42494544 1.44790176 1.42633028 1.43245512 1.41559309 1.43743257
 1.49461799]
scan scan20 mean output: [2.16098818 2.17821048 2.15450914 2.14886178 2.14548533 2.15932194
 2.23102642]
scan scan22 mean output: [1.84457468 1.76052218 1.72700193 1.70638893 1.6944377  1.70606693
 1.75517236]
scan scan30 mean output: [8.67458678 8.59489073 8.69847434 8.68814547 8.62376398 8.5837679
 8.51076824]
scan scan31 mean output: [2.31524642 2.25100664 2.26015923 2.24801694 2.2719446  2.35124722
 2.43751027]
scan scan36 mean output: [2.63409314 2.46368888 2.46062461 2.43784512 2.47157738 2.49926177
 2.58666817]
scan scan39 mean output: [2.37594525 2.31825419 2.30938649 2.28579744 2.29160664 2.37218753
 2.46592282]
scan scan41 mean output: [1.66476584 1.60040762 1.59964506 1.59753174 1.62585583 1.64422795
 1.66758125]
scan scan42 mean output: [2.95913485 2.90192703 2.82472118 2.82756103 2.83083466 2.84663147
 3.02270986]
scan scan44 mean output: [2.41167771 2.38853236 2.34733436 2.3156443  2.31195744 2.31628597
 2.38618758]
scan scan45 mean output: [1.66468076 1.67245606 1.68474703 1.67637421 1.65131788 1.71283245
 1.60885482]
scan scan46 mean output: [1.29634672 1.28409807 1.26796138 1.26884852 1.25513599 1.27678512
 1.28780049]
scan scan47 mean output: [11.23890356 10.62560907 10.38173617 10.33011346 10.41427073 10.89797371
 11.45373098]
scan scan50 mean output: [4.78126085 4.6839368  4.68366494 4.67832368 4.68632154 4.81935768
 4.82078136]
scan scan51 mean output: [1.38731192 1.30007031 1.28973332 1.29298134 1.31923183 1.32288761
 1.39191384]
scan scan52 mean output: [6.39765668 6.71479313 7.32903373 7.36236764 7.29158699 6.97721179
 6.38887133]
scan scan53 mean output: [6.57723425 7.27589331 7.38309274 7.2946379  7.4625387  7.67598391
 6.51365198]
scan scan55 mean output: [1.69197897 1.63865039 1.64135865 1.63935601 1.66539767 1.70896921
 1.78418155]
scan scan57 mean output: [1.71960834 1.59272069 1.58916454 1.58068859 1.57162419 1.61019183
 1.64975446]
scan scan58 mean output: [2.24748354 2.17271329 2.12357374 2.09704737 2.13028055 2.16428077
 2.24225527]
scan scan60 mean output: [10.0765839  10.713741   10.83717628 10.72482792 10.65179486 10.05052784
  8.91400745]
scan scan61 mean output: [1.34369714 1.28631924 1.34006934 1.33217524 1.37976855 1.46957162
 1.40087494]
scan scan63 mean output: [7.95549631 8.00294294 7.99245622 7.98088335 8.0994514  8.011342
 7.50288264]
scan scan64 mean output: [9.61688464 9.76545364 9.4716502  9.27641999 9.16097031 8.93420555
 8.70215934]
scan scan65 mean output: [2.47293989 2.1114776  2.03738769 2.03424759 2.05745113 2.09894038
 2.23646499]
scan scan68 mean output: [1.66418732 1.54708171 1.55255918 1.54454011 1.60591144 1.68637406
 1.66911975]
scan scan69 mean output: [2.07839942 1.71989811 1.57556223 1.53391165 1.5258197  1.58260642
 1.71275553]
scan scan70 mean output: [6.08255564 5.70765056 5.58918495 5.5997456  5.63302205 5.86396391
 6.33816176]
scan scan71 mean output: [2.4684212  2.04934821 1.8593077  1.81687432 1.82307946 1.87917591
 1.97769005]
scan scan72 mean output: [5.42979493 4.98342379 4.74852921 4.70286036 4.70392189 4.81169244
 5.190466  ]
scan scan74 mean output: [6.19517582 6.1203795  6.00283794 5.99464523 6.01317631 5.97329591
 5.93384791]
scan scan76 mean output: [2.52154994 2.46967198 2.42213043 2.42903319 2.45165442 2.4932581
 2.67943018]
scan scan83 mean output: [5.78417572 5.60813875 5.5824187  5.59089596 5.61494345 5.75872464
 6.14112679]
scan scan84 mean output: [4.81248867 4.73980808 4.64406028 4.62658724 4.64393221 4.72740636
 5.06771795]
scan scan85 mean output: [8.09187486 5.58834374 5.07499904 4.93354202 5.00643232 5.12534594
 5.44755901]
scan scan87 mean output: [10.27863458  9.28198363  8.86244189  8.76100429  8.95012405  9.33165627
 11.22065983]
scan scan88 mean output: [12.53427557 11.66079711 11.2207347  11.16631215 11.3067646  12.05679975
 13.51323437]
scan scan89 mean output: [3.10886091 1.84053157 1.57470665 1.4852684  1.43455578 1.45991223
 1.76265765]
scan scan90 mean output: [2.91044355 2.14173189 1.87538047 1.80021106 1.68886491 1.86459352
 2.09340844]
scan scan91 mean output: [4.1554668  3.15375396 2.79960795 2.60254759 2.62762953 2.77030982
 3.16996982]
scan scan92 mean output: [3.44822968 2.92558409 2.46401183 2.36850045 2.37061088 2.37754278
 2.6572957 ]
scan scan93 mean output: [2.47878795 2.42562657 2.3478494  2.33718573 2.31991502 2.36878802
 2.54261788]
scan scan94 mean output: [2.25050864 2.15269626 2.12538479 2.10822735 2.11139564 2.16234145
 2.31182622]
scan scan95 mean output: [3.02138372 2.92085865 2.87007221 2.83432269 2.89923202 2.94220613
 3.32199339]
scan scan96 mean output: [3.56508471 3.47703425 3.44893682 3.41940667 3.42401943 3.53237321
 3.7412994 ]
scan scan97 mean output: [2.74131069 2.66731166 2.65141736 2.64831129 2.63753103 2.69091483
 2.84743998]
scan scan98 mean output: [4.32465715 4.21557355 4.19174789 4.20941    4.23795416 4.30376435
 4.56633065]
scan scan99 mean output: [1.89460801 1.83698884 1.81319355 1.80834472 1.81198793 1.83644629
 1.96337358]
scan scan100 mean output: [4.37227584 4.20406946 4.1266844  4.09965451 4.13235198 4.19584484
 4.44129949]
scan scan101 mean output: [10.98790638 10.51371385 10.07423857 10.14579899 10.08343843 10.57270488
 11.77931405]
scan scan102 mean output: [14.89902821 14.11591781 13.57399777 13.26589634 13.29806149 13.27530462
 13.84596409]
scan scan103 mean output: [1.96966011 1.88718121 1.85632959 1.84724409 1.85666614 1.89407913
 2.0532192 ]
scan scan104 mean output: [8.07298178 7.76581244 7.65959314 7.45407124 7.64064978 7.87105531
 8.93807717]
scan scan105 mean output: [3.92992292 3.8236729  3.78583217 3.81376588 3.82348662 3.92253066
 4.54122754]
scan scan107 mean output: [2.95482818 2.72223734 2.62752161 2.57842151 2.58153874 2.7260572
 2.93490657]
scan scan108 mean output: [1.50086186 1.42920616 1.39831006 1.38961969 1.39941372 1.4443991
 1.54088434]
scan scan109 mean output: [2.30197851 2.20360578 2.13959853 2.1302663  2.15524475 2.31695009
 2.64638124]
scan scan111 mean output: [3.53442634 3.37920633 3.31997709 3.28886725 3.31675281 3.38871323
 3.67240878]
scan scan112 mean output: [5.59327961 5.28310663 5.14031178 5.14182902 5.1483838  5.27757189
 5.69128932]
scan scan113 mean output: [5.65039545 5.24060912 5.12833628 5.10867552 5.09432419 5.23226616
 5.79506215]
scan scan115 mean output: [1.10287945 1.05027054 1.00521698 1.00482054 1.01658114 1.06764755
 1.16337927]
scan scan116 mean output: [1.26797312 1.20659617 1.16379642 1.14963802 1.16327593 1.16962385
 1.19768231]
scan scan119 mean output: [2.96742071 2.68487256 2.58232683 2.5617083  2.58739374 2.68868351
 2.96048615]
scan scan120 mean output: [1.62734547 1.45432689 1.36843849 1.3507711  1.34210405 1.37754391
 1.58746313]
scan scan121 mean output: [2.45077215 2.2816408  2.19525332 2.18318168 2.20049704 2.27198017
 2.69546309]
scan scan122 mean output: [1.73261188 1.5833986  1.51542548 1.49187345 1.49378712 1.53200495
 1.73979831]
scan scan123 mean output: [1.42056491 1.26430274 1.21997888 1.23058494 1.21519014 1.27313657
 1.45864995]
scan scan124 mean output: [1.50513161 1.40515696 1.3793114  1.36679706 1.37046543 1.43902769
 1.61628655]
scan scan125 mean output: [1.84503571 1.68809586 1.65205791 1.62550835 1.64884693 1.7013044
 1.82643247]
03/25/2024 14:48:43 - INFO - __main__ - ***** Running training *****
03/25/2024 14:48:43 - INFO - __main__ -   Num examples = 27097
03/25/2024 14:48:43 - INFO - __main__ -   Num Epochs = 600
03/25/2024 14:48:43 - INFO - __main__ -   Instantaneous batch size per device = 3
03/25/2024 14:48:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 3
03/25/2024 14:48:43 - INFO - __main__ -   Gradient Accumulation steps = 1
03/25/2024 14:48:43 - INFO - __main__ -   Total optimization steps = 5419800
03/25/2024 14:48:43 - INFO - accelerate.accelerator - Loading states from /workspace/ViewDiff/output_var_debug/all/subset_all/input_3/train/class6/checkpoint-1500/
Some weights of the model checkpoint were not used when initializing UNet2DConditionCrossFrameInExistingAttnModel: 
 ['down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, mid_block.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.0.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.bias, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.temb_proj.2.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight']
03/25/2024 14:48:44 - INFO - __main__ - Loaded lora parameters into model
03/25/2024 14:48:44 - INFO - accelerate.checkpointing - All model weights loaded successfully
03/25/2024 14:48:51 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
03/25/2024 14:48:51 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
03/25/2024 14:48:51 - INFO - accelerate.checkpointing - All random states loaded successfully
03/25/2024 14:48:52 - INFO - accelerate.accelerator - Loading in 0 custom states
scan scan126 mean output: [1.97154763 1.61379367 1.60398592 1.57126795 1.57520988 1.70586573
 2.406283  ]
scan scan127 mean output: [2.47762656 2.09939229 2.21543788 2.23824336 2.264222   2.22630964
 2.38829782]
scan scan128 mean output: [3.36696636 2.89962806 2.82768071 2.79384071 2.75277496 2.82201681
 3.07775393]
scan scan3 mean output: [3.05263341 2.92222905 2.79727794 2.86347311 2.84872966 2.86011306
 2.89029897]
scan scan5 mean output: [1.27925969 1.21662743 1.19024779 1.17714057 1.17585228 1.13693983
 1.13944231]
scan scan17 mean output: [4.23918102 4.3646479  4.26873698 4.152941   4.16164234 4.44407791
 4.15546879]
scan scan21 mean output: [6.7044504  6.78296111 6.79752936 6.78882487 6.74292021 6.64370545
 6.71224226]
scan scan28 mean output: [8.33500304 8.26974845 8.08564879 7.80853114 7.63656337 7.53135754
 7.33638762]
scan scan35 mean output: [1.18667069 1.32775589 1.20760566 1.06743625 1.15983699 0.91010288
 0.87788923]
scan scan37 mean output: [21.1885944  21.17826699 21.25191359 21.18731575 20.97766127 20.14655309
 19.91200681]
scan scan38 mean output: [1.69360119 1.60652492 1.57134287 1.54112932 1.56236016 1.63116413
 1.62020702]
scan scan40 mean output: [3.64535844 3.69948315 3.71793726 3.69029421 3.68635351 3.71370282
 3.58273512]
scan scan43 mean output: [2.2589177  2.24700747 2.22983368 2.24613731 2.19530878 2.33009434
 2.42131581]
scan scan56 mean output: [2.31583979 2.17296925 2.15377764 2.14338661 2.1395326  2.20913124
 2.29832114]
scan scan59 mean output: [2.25470706 2.34287849 2.48965601 2.58392481 2.6572768  2.5697732
 2.31332584]
scan scan66 mean output: [1.92438559 2.08761549 1.80486628 1.61805749 1.56760059 1.44017948
 1.4255248 ]
scan scan67 mean output: [3.06216342 3.51998134 3.27188233 3.01847513 3.0300974  3.01376892
 2.77910937]
scan scan82 mean output: [8.87980272 9.1904472  9.19312354 9.1552566  9.26186729 9.03365678
 7.82553422]
scan scan86 mean output: [16.68869462 13.0623592  11.09205004 10.73961877 10.67555843 10.71248814
 11.94098382]
scan scan106 mean output: [1.91600927 1.62643204 1.56631822 1.54953243 1.58691446 1.61378269
 1.72092495]
scan scan117 mean output: [1.6601294  1.54156241 1.50466732 1.4715858  1.46958674 1.49934888
 1.62797041]
load from path: /workspace/ViewDiff/output_var_debug/all/subset_all/input_3/train/class6/checkpoint-1500
Resuming from checkpoint /workspace/ViewDiff/output_var_debug/all/subset_all/input_3/train/class6/checkpoint-1500
  0%|                                                                                          | 0/5418300 [00:00<?, ?it/s]Steps:   0%|                                                                                   | 0/5418300 [00:00<?, ?it/s]03/25/2024 14:48:52 - INFO - __main__ - Will skip the first 1500 steps in dataloader.
03/25/2024 14:48:52 - INFO - __main__ - Running validation...

Loading pipeline components...:   0%|                                                                | 0/7 [00:00<?, ?it/s][ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of timbrooks/instruct-pix2pix.
Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of timbrooks/instruct-pix2pix.
{'timestep_spacing'} was not found in config. Values will be initialized to default values.
Loaded scheduler as EulerAncestralDiscreteScheduler from `scheduler` subfolder of timbrooks/instruct-pix2pix.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of timbrooks/instruct-pix2pix.

Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  9.57it/s][ALoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  9.57it/s]
{'timestep_spacing'} was not found in config. Values will be initialized to default values.
conduct do_classifier_free_guidance, text_guidance: 16,image_guidance: 1

  0%|                                                                                               | 0/50 [00:00<?, ?it/s][A
  2%|â–ˆâ–‹                                                                                     | 1/50 [00:00<00:16,  2.90it/s][A
  4%|â–ˆâ–ˆâ–ˆâ–                                                                                   | 2/50 [00:00<00:15,  3.04it/s][A
  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                 | 3/50 [00:00<00:15,  3.10it/s][A
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                | 4/50 [00:01<00:14,  3.13it/s][A
 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                              | 5/50 [00:01<00:14,  3.15it/s][A
 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                            | 6/50 [00:01<00:13,  3.16it/s][A
 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 7/50 [00:02<00:13,  3.17it/s][A
 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                         | 8/50 [00:02<00:13,  3.17it/s][A
 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                       | 9/50 [00:02<00:12,  3.17it/s][A
 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                    | 10/50 [00:03<00:12,  3.16it/s][A
 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                   | 11/50 [00:03<00:12,  3.16it/s][A
 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                 | 12/50 [00:03<00:12,  3.16it/s][A
 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                               | 13/50 [00:04<00:11,  3.16it/s][A
 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 14/50 [00:04<00:11,  3.16it/s][A
 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                            | 15/50 [00:04<00:11,  3.17it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 16/50 [00:05<00:10,  3.17it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                        | 17/50 [00:05<00:10,  3.17it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                       | 18/50 [00:05<00:10,  3.17it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                     | 19/50 [00:06<00:09,  3.17it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 20/50 [00:06<00:09,  3.17it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                  | 21/50 [00:06<00:09,  3.17it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                | 22/50 [00:06<00:08,  3.14it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                              | 23/50 [00:07<00:08,  3.15it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                            | 24/50 [00:07<00:08,  3.15it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 25/50 [00:07<00:07,  3.16it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 26/50 [00:08<00:07,  3.16it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 27/50 [00:08<00:07,  3.16it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 28/50 [00:08<00:06,  3.16it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 29/50 [00:09<00:06,  3.16it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 30/50 [00:09<00:06,  3.16it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 31/50 [00:09<00:06,  3.16it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 32/50 [00:10<00:05,  3.16it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 33/50 [00:10<00:05,  3.16it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 34/50 [00:10<00:05,  3.16it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 35/50 [00:11<00:04,  3.16it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 36/50 [00:11<00:04,  3.16it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 37/50 [00:11<00:04,  3.16it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 38/50 [00:12<00:03,  3.16it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 39/50 [00:12<00:03,  3.17it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 40/50 [00:12<00:03,  3.17it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 41/50 [00:12<00:02,  3.17it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 42/50 [00:13<00:02,  3.17it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 43/50 [00:13<00:02,  3.17it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 44/50 [00:13<00:01,  3.17it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 45/50 [00:14<00:01,  3.17it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 46/50 [00:14<00:01,  3.17it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 47/50 [00:14<00:00,  3.17it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 48/50 [00:15<00:00,  3.17it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/50 [00:15<00:00,  3.17it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:15<00:00,  3.17it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:15<00:00,  3.16it/s]
03/25/2024 14:49:11 - INFO - __main__ - Running training...
save img to : /workspace/ViewDiff/output_var_debug/all/subset_all/input_3/train/class6/images/modify the lightness of image to light_class_5 style_val_step_1500_0.png modify the lightness of image to light_class_5 style
writing inference outputs failed [Errno 2] No such file or directory: '/usr/bin/ffmpeg'
Steps:   0%|                                                                     | 1501/5418300 [00:21<21:12:15, 70.96it/s]Steps:   0%|                                          | 1501/5418300 [00:21<21:12:15, 70.96it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                          | 1502/5418300 [00:22<21:12:15, 70.96it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                          | 1503/5418300 [00:22<21:12:15, 70.96it/s, lr=0.001, step_loss=0.895]Steps:   0%|                                          | 1504/5418300 [00:23<21:12:15, 70.96it/s, lr=0.001, step_loss=0.884]Steps:   0%|                                          | 1505/5418300 [00:24<21:12:15, 70.96it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                          | 1506/5418300 [00:25<21:12:15, 70.96it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                              | 1507/5418300 [00:26<21:12:15, 70.96it/s, lr=0.001, step_loss=1]Steps:   0%|                                          | 1508/5418300 [00:27<21:12:15, 70.96it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                          | 1509/5418300 [00:28<31:27:27, 47.83it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                          | 1509/5418300 [00:28<31:27:27, 47.83it/s, lr=0.001, step_loss=0.894]Steps:   0%|                                          | 1510/5418300 [00:29<33:18:38, 45.17it/s, lr=0.001, step_loss=0.894]Steps:   0%|                                          | 1510/5418300 [00:29<33:18:38, 45.17it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                          | 1511/5418300 [00:30<35:54:11, 41.91it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                          | 1511/5418300 [00:30<35:54:11, 41.91it/s, lr=0.001, step_loss=0.887]Steps:   0%|                                          | 1512/5418300 [00:31<39:35:47, 38.00it/s, lr=0.001, step_loss=0.887]Steps:   0%|                                          | 1512/5418300 [00:31<39:35:47, 38.00it/s, lr=0.001, step_loss=0.559]Steps:   0%|                                          | 1513/5418300 [00:32<44:47:48, 33.59it/s, lr=0.001, step_loss=0.559]Steps:   0%|                                          | 1513/5418300 [00:32<44:47:48, 33.59it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                          | 1514/5418300 [00:33<52:09:17, 28.85it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                          | 1514/5418300 [00:33<52:09:17, 28.85it/s, lr=0.001, step_loss=0.669]Steps:   0%|                                          | 1515/5418300 [00:33<62:34:13, 24.05it/s, lr=0.001, step_loss=0.669]Steps:   0%|                                          | 1515/5418300 [00:33<62:34:13, 24.05it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                          | 1516/5418300 [00:34<77:10:55, 19.49it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                          | 1516/5418300 [00:34<77:10:55, 19.49it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                          | 1517/5418300 [00:35<97:27:34, 15.44it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                          | 1517/5418300 [00:35<97:27:34, 15.44it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                         | 1518/5418300 [00:36<125:21:27, 12.00it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                         | 1518/5418300 [00:36<125:21:27, 12.00it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                         | 1519/5418300 [00:37<163:18:34,  9.21it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                         | 1519/5418300 [00:37<163:18:34,  9.21it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                         | 1520/5418300 [00:38<213:38:25,  7.04it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                         | 1520/5418300 [00:38<213:38:25,  7.04it/s, lr=0.001, step_loss=0.779]Steps:   0%|                                         | 1521/5418300 [00:39<278:04:41,  5.41it/s, lr=0.001, step_loss=0.779]Steps:   0%|                                         | 1521/5418300 [00:39<278:04:41,  5.41it/s, lr=0.001, step_loss=0.895]Steps:   0%|                                         | 1522/5418300 [00:40<360:35:31,  4.17it/s, lr=0.001, step_loss=0.895]Steps:   0%|                                         | 1522/5418300 [00:40<360:35:31,  4.17it/s, lr=0.001, step_loss=0.781]Steps:   0%|                                         | 1523/5418300 [00:41<458:18:17,  3.28it/s, lr=0.001, step_loss=0.781]Steps:   0%|                                         | 1523/5418300 [00:41<458:18:17,  3.28it/s, lr=0.001, step_loss=0.556]Steps:   0%|                                         | 1524/5418300 [00:42<568:38:59,  2.65it/s, lr=0.001, step_loss=0.556]Steps:   0%|                                         | 1524/5418300 [00:42<568:38:59,  2.65it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                         | 1525/5418300 [00:43<685:06:59,  2.20it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                         | 1525/5418300 [00:43<685:06:59,  2.20it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                         | 1526/5418300 [00:44<804:01:53,  1.87it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                         | 1526/5418300 [00:44<804:01:53,  1.87it/s, lr=0.001, step_loss=0.668]Steps:   0%|                                         | 1527/5418300 [00:44<917:17:48,  1.64it/s, lr=0.001, step_loss=0.668]Steps:   0%|                                         | 1527/5418300 [00:44<917:17:48,  1.64it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                        | 1528/5418300 [00:45<1021:50:08,  1.47it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                         | 1528/5418300 [00:45<1021:50:08,  1.47it/s, lr=0.001, step_loss=0.78]Steps:   0%|                                         | 1529/5418300 [00:46<1106:55:21,  1.36it/s, lr=0.001, step_loss=0.78]Steps:   0%|                                         | 1529/5418300 [00:46<1106:55:21,  1.36it/s, lr=0.001, step_loss=0.89]Steps:   0%|                                         | 1530/5418300 [00:47<1175:16:34,  1.28it/s, lr=0.001, step_loss=0.89]Steps:   0%|                                        | 1530/5418300 [00:47<1175:16:34,  1.28it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1531/5418300 [00:48<1227:04:46,  1.23it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1531/5418300 [00:48<1227:04:46,  1.23it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1532/5418300 [00:49<1272:32:34,  1.18it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1532/5418300 [00:49<1272:32:34,  1.18it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1533/5418300 [00:50<1303:29:44,  1.15it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1533/5418300 [00:50<1303:29:44,  1.15it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1534/5418300 [00:51<1333:03:47,  1.13it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1534/5418300 [00:51<1333:03:47,  1.13it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1535/5418300 [00:52<1355:11:30,  1.11it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1535/5418300 [00:52<1355:11:30,  1.11it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                        | 1536/5418300 [00:53<1363:58:55,  1.10it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                        | 1536/5418300 [00:53<1363:58:55,  1.10it/s, lr=0.001, step_loss=0.897]Steps:   0%|                                        | 1537/5418300 [00:54<1367:17:51,  1.10it/s, lr=0.001, step_loss=0.897]Steps:   0%|                                        | 1537/5418300 [00:54<1367:17:51,  1.10it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                        | 1538/5418300 [00:55<1368:49:03,  1.10it/s, lr=0.001, step_loss=0.775]Steps:   0%|                                        | 1538/5418300 [00:55<1368:49:03,  1.10it/s, lr=0.001, step_loss=0.662]Steps:   0%|                                        | 1539/5418300 [00:56<1374:03:45,  1.10it/s, lr=0.001, step_loss=0.662]Steps:   0%|                                            | 1539/5418300 [00:56<1374:03:45,  1.10it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1540/5418300 [00:56<1378:06:12,  1.09it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1540/5418300 [00:56<1378:06:12,  1.09it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                        | 1541/5418300 [00:57<1383:07:12,  1.09it/s, lr=0.001, step_loss=0.776]Steps:   0%|                                        | 1541/5418300 [00:57<1383:07:12,  1.09it/s, lr=0.001, step_loss=0.778]Steps:   0%|                                        | 1542/5418300 [00:58<1381:24:44,  1.09it/s, lr=0.001, step_loss=0.778]Steps:   0%|                                        | 1542/5418300 [00:58<1381:24:44,  1.09it/s, lr=0.001, step_loss=0.666]Steps:   0%|                                        | 1543/5418300 [00:59<1382:47:20,  1.09it/s, lr=0.001, step_loss=0.666]Steps:   0%|                                            | 1543/5418300 [00:59<1382:47:20,  1.09it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1544/5418300 [01:00<1383:41:08,  1.09it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1544/5418300 [01:00<1383:41:08,  1.09it/s, lr=0.001, step_loss=0.992]Steps:   0%|                                        | 1545/5418300 [01:01<1382:55:33,  1.09it/s, lr=0.001, step_loss=0.992]Steps:   0%|                                        | 1545/5418300 [01:01<1382:55:33,  1.09it/s, lr=0.001, step_loss=0.555]Steps:   0%|                                        | 1546/5418300 [01:02<1386:09:37,  1.09it/s, lr=0.001, step_loss=0.555]Steps:   0%|                                        | 1546/5418300 [01:02<1386:09:37,  1.09it/s, lr=0.001, step_loss=0.778]Steps:   0%|                                        | 1547/5418300 [01:03<1384:24:28,  1.09it/s, lr=0.001, step_loss=0.778]Steps:   0%|                                         | 1547/5418300 [01:03<1384:24:28,  1.09it/s, lr=0.001, step_loss=0.67]Steps:   0%|                                         | 1548/5418300 [01:04<1380:23:11,  1.09it/s, lr=0.001, step_loss=0.67]Steps:   0%|                                        | 1548/5418300 [01:04<1380:23:11,  1.09it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1549/5418300 [01:05<1378:41:17,  1.09it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1549/5418300 [01:05<1378:41:17,  1.09it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1550/5418300 [01:07<1935:21:13,  1.29s/it, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1550/5418300 [01:07<1935:21:13,  1.29s/it, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1551/5418300 [01:08<1775:19:52,  1.18s/it, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1551/5418300 [01:08<1775:19:52,  1.18s/it, lr=0.001, step_loss=0.893]Steps:   0%|                                        | 1552/5418300 [01:09<1658:30:03,  1.10s/it, lr=0.001, step_loss=0.893]Steps:   0%|                                        | 1552/5418300 [01:09<1658:30:03,  1.10s/it, lr=0.001, step_loss=0.557]Steps:   0%|                                        | 1553/5418300 [01:10<1578:31:46,  1.05s/it, lr=0.001, step_loss=0.557]Steps:   0%|                                         | 1553/5418300 [01:10<1578:31:46,  1.05s/it, lr=0.001, step_loss=0.89]Steps:   0%|                                         | 1554/5418300 [01:11<1519:13:59,  1.01s/it, lr=0.001, step_loss=0.89]Steps:   0%|                                        | 1554/5418300 [01:11<1519:13:59,  1.01s/it, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1555/5418300 [01:12<1479:35:40,  1.02it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1555/5418300 [01:12<1479:35:40,  1.02it/s, lr=0.001, step_loss=0.784]Steps:   0%|                                        | 1556/5418300 [01:12<1448:22:30,  1.04it/s, lr=0.001, step_loss=0.784]Steps:   0%|                                        | 1556/5418300 [01:12<1448:22:30,  1.04it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1557/5418300 [01:13<1428:42:49,  1.05it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1557/5418300 [01:13<1428:42:49,  1.05it/s, lr=0.001, step_loss=0.998]Steps:   0%|                                        | 1558/5418300 [01:14<1412:07:43,  1.07it/s, lr=0.001, step_loss=0.998]Steps:   0%|                                            | 1558/5418300 [01:14<1412:07:43,  1.07it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1559/5418300 [01:15<1400:13:12,  1.07it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1559/5418300 [01:15<1400:13:12,  1.07it/s, lr=0.001, step_loss=0.895]Steps:   0%|                                        | 1560/5418300 [01:16<1396:17:36,  1.08it/s, lr=0.001, step_loss=0.895]Steps:   0%|                                        | 1560/5418300 [01:16<1396:17:36,  1.08it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                        | 1561/5418300 [01:17<1393:37:20,  1.08it/s, lr=0.001, step_loss=0.782]Steps:   0%|                                            | 1561/5418300 [01:17<1393:37:20,  1.08it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1562/5418300 [01:18<1393:04:22,  1.08it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1562/5418300 [01:18<1393:04:22,  1.08it/s, lr=0.001, step_loss=0.668]Steps:   0%|                                        | 1563/5418300 [01:19<1392:23:48,  1.08it/s, lr=0.001, step_loss=0.668]Steps:   0%|                                         | 1563/5418300 [01:19<1392:23:48,  1.08it/s, lr=0.001, step_loss=0.89]Steps:   0%|                                         | 1564/5418300 [01:20<1390:26:00,  1.08it/s, lr=0.001, step_loss=0.89]Steps:   0%|                                        | 1564/5418300 [01:20<1390:26:00,  1.08it/s, lr=0.001, step_loss=0.666]Steps:   0%|                                        | 1565/5418300 [01:21<1386:55:40,  1.08it/s, lr=0.001, step_loss=0.666]Steps:   0%|                                        | 1565/5418300 [01:21<1386:55:40,  1.08it/s, lr=0.001, step_loss=0.885]Steps:   0%|                                        | 1566/5418300 [01:22<1394:19:11,  1.08it/s, lr=0.001, step_loss=0.885]Steps:   0%|                                        | 1566/5418300 [01:22<1394:19:11,  1.08it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1567/5418300 [01:23<1390:32:25,  1.08it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1567/5418300 [01:23<1390:32:25,  1.08it/s, lr=0.001, step_loss=0.998]Steps:   0%|                                        | 1568/5418300 [01:23<1391:01:23,  1.08it/s, lr=0.001, step_loss=0.998]Steps:   0%|                                        | 1568/5418300 [01:23<1391:01:23,  1.08it/s, lr=0.001, step_loss=0.774]Steps:   0%|                                        | 1569/5418300 [01:24<1387:58:50,  1.08it/s, lr=0.001, step_loss=0.774]Steps:   0%|                                        | 1569/5418300 [01:24<1387:58:50,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1570/5418300 [01:25<1390:11:49,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1570/5418300 [01:25<1390:11:49,  1.08it/s, lr=0.001, step_loss=0.892]Steps:   0%|                                        | 1571/5418300 [01:26<1395:50:35,  1.08it/s, lr=0.001, step_loss=0.892]Steps:   0%|                                        | 1571/5418300 [01:26<1395:50:35,  1.08it/s, lr=0.001, step_loss=0.996]Steps:   0%|                                        | 1572/5418300 [01:27<1394:16:17,  1.08it/s, lr=0.001, step_loss=0.996]Steps:   0%|                                        | 1572/5418300 [01:27<1394:16:17,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1573/5418300 [01:28<1397:04:35,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1573/5418300 [01:28<1397:04:35,  1.08it/s, lr=0.001, step_loss=0.557]Steps:   0%|                                        | 1574/5418300 [01:29<1393:59:03,  1.08it/s, lr=0.001, step_loss=0.557]Steps:   0%|                                        | 1574/5418300 [01:29<1393:59:03,  1.08it/s, lr=0.001, step_loss=0.887]Steps:   0%|                                        | 1575/5418300 [01:30<1388:14:04,  1.08it/s, lr=0.001, step_loss=0.887]Steps:   0%|                                        | 1575/5418300 [01:30<1388:14:04,  1.08it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                        | 1576/5418300 [01:31<1387:27:56,  1.08it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                        | 1576/5418300 [01:31<1387:27:56,  1.08it/s, lr=0.001, step_loss=0.773]Steps:   0%|                                        | 1577/5418300 [01:32<1393:14:22,  1.08it/s, lr=0.001, step_loss=0.773]Steps:   0%|                                        | 1577/5418300 [01:32<1393:14:22,  1.08it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1578/5418300 [01:33<1389:52:09,  1.08it/s, lr=0.001, step_loss=0.891]Steps:   0%|                                        | 1578/5418300 [01:33<1389:52:09,  1.08it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                        | 1579/5418300 [01:34<1391:28:09,  1.08it/s, lr=0.001, step_loss=0.888]Steps:   0%|                                        | 1579/5418300 [01:34<1391:28:09,  1.08it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1580/5418300 [01:35<1389:41:04,  1.08it/s, lr=0.001, step_loss=0.999]Steps:   0%|                                        | 1580/5418300 [01:35<1389:41:04,  1.08it/s, lr=0.001, step_loss=0.664]Steps:   0%|                                        | 1581/5418300 [01:35<1390:59:33,  1.08it/s, lr=0.001, step_loss=0.664]Steps:   0%|                                            | 1581/5418300 [01:35<1390:59:33,  1.08it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1582/5418300 [01:37<1457:34:36,  1.03it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1582/5418300 [01:37<1457:34:36,  1.03it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                        | 1583/5418300 [01:37<1433:15:52,  1.05it/s, lr=0.001, step_loss=0.997]Steps:   0%|                                        | 1583/5418300 [01:37<1433:15:52,  1.05it/s, lr=0.001, step_loss=0.783]Steps:   0%|                                        | 1584/5418300 [01:38<1416:27:43,  1.06it/s, lr=0.001, step_loss=0.783]Steps:   0%|                                        | 1584/5418300 [01:38<1416:27:43,  1.06it/s, lr=0.001, step_loss=0.781]Steps:   0%|                                        | 1585/5418300 [01:39<1408:13:35,  1.07it/s, lr=0.001, step_loss=0.781]Steps:   0%|                                        | 1585/5418300 [01:39<1408:13:35,  1.07it/s, lr=0.001, step_loss=0.995]Steps:   0%|                                        | 1586/5418300 [01:40<1399:28:19,  1.08it/s, lr=0.001, step_loss=0.995]Steps:   0%|                                        | 1586/5418300 [01:40<1399:28:19,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                        | 1587/5418300 [01:41<1394:59:08,  1.08it/s, lr=0.001, step_loss=0.889]Steps:   0%|                                         | 1587/5418300 [01:41<1394:59:08,  1.08it/s, lr=0.001, step_loss=0.77]Steps:   0%|                                         | 1588/5418300 [01:42<1389:45:29,  1.08it/s, lr=0.001, step_loss=0.77]Steps:   0%|                                        | 1588/5418300 [01:42<1389:45:29,  1.08it/s, lr=0.001, step_loss=0.894]Steps:   0%|                                        | 1589/5418300 [01:43<1388:24:12,  1.08it/s, lr=0.001, step_loss=0.894]Steps:   0%|                                            | 1589/5418300 [01:43<1388:24:12,  1.08it/s, lr=0.001, step_loss=1]Steps:   0%|                                            | 1590/5418300 [01:44<1387:02:36,  1.08it/s, lr=0.001, step_loss=1]Steps:   0%|                                        | 1590/5418300 [01:44<1387:02:36,  1.08it/s, lr=0.001, step_loss=0.664]Steps:   0%|                                        | 1591/5418300 [01:45<1400:58:11,  1.07it/s, lr=0.001, step_loss=0.664]Steps:   0%|                                        | 1591/5418300 [01:45<1400:58:11,  1.07it/s, lr=0.001, step_loss=0.886]Steps:   0%|                                        | 1592/5418300 [01:46<1402:48:22,  1.07it/s, lr=0.001, step_loss=0.886]Steps:   0%|                                        | 1592/5418300 [01:46<1402:48:22,  1.07it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1593/5418300 [01:47<1400:57:13,  1.07it/s, lr=0.001, step_loss=0.777]Steps:   0%|                                        | 1593/5418300 [01:47<1400:57:13,  1.07it/s, lr=0.001, step_loss=0.892]Steps:   0%|                                        | 1594/5418300 [01:48<1395:56:22,  1.08it/s, lr=0.001, step_loss=0.892]Steps:   0%|                                        | 1594/5418300 [01:48<1395:56:22,  1.08it/s, lr=0.001, step_loss=0.995]Steps:   0%|                                        | 1595/5418300 [01:49<1391:57:47,  1.08it/s, lr=0.001, step_loss=0.995]Steps:   0%|                                        | 1595/5418300 [01:49<1391:57:47,  1.08it/s, lr=0.001, step_loss=0.783]Steps:   0%|                                        | 1596/5418300 [01:49<1391:16:19,  1.08it/s, lr=0.001, step_loss=0.783]Steps:   0%|                                        | 1596/5418300 [01:49<1391:16:19,  1.08it/s, lr=0.001, step_loss=0.889]Traceback (most recent call last):
  File "/workspace/ViewDiff/viewdiff/train_pix2pix_debug.py", line 1043, in <module>
    tyro.cli(train_and_test)
  File "/workspace/viewdiff/lib/python3.10/site-packages/tyro/_cli.py", line 177, in cli
    output = _cli_impl(
  File "/workspace/viewdiff/lib/python3.10/site-packages/tyro/_cli.py", line 431, in _cli_impl
    out, consumed_keywords = _calling.call_from_args(
  File "/workspace/viewdiff/lib/python3.10/site-packages/tyro/_calling.py", line 217, in call_from_args
    return unwrapped_f(*positional_args, **kwargs), consumed_keywords  # type: ignore
  File "/workspace/ViewDiff/viewdiff/train_pix2pix_debug.py", line 333, in train_and_test
    avg_step_losses, acc_step, loss = train_step(
  File "/workspace/ViewDiff/viewdiff/train_pix2pix_debug.py", line 765, in train_step
    avg_losses, acc, loss = process_batch(batch)
  File "/workspace/ViewDiff/viewdiff/train_pix2pix_debug.py", line 665, in process_batch
    output = unet(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/utils/operations.py", line 636, in forward
    return model_forward(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/utils/operations.py", line 624, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/workspace/ViewDiff/viewdiff/../viewdiff/model/custom_unet_2d_condition.py", line 1522, in forward
    sample, res_samples = downsample_block(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py", line 1086, in forward
    hidden_states = attn(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/diffusers/models/transformer_2d.py", line 315, in forward
    hidden_states = block(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/diffusers/models/attention.py", line 197, in forward
    attn_output = self.attn1(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/viewdiff/lib/python3.10/site-packages/diffusers/models/attention_processor.py", line 420, in forward
    return self.processor(
  File "/workspace/ViewDiff/viewdiff/../viewdiff/model/custom_attention_processor.py", line 247, in __call__
    key = key.view(
  File "/workspace/viewdiff/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 5648) is killed by signal: Terminated. 
Steps:   0%|                                         | 1596/5418300 [01:50<104:21:31, 14.42it/s, lr=0.001, step_loss=0.889]
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/commands/launch.py", line 996, in <module>
    main()
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/commands/launch.py", line 992, in main
    launch_command(args)
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/workspace/viewdiff/lib/python3.10/site-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/workspace/viewdiff/bin/python', '/workspace/ViewDiff/viewdiff/train_pix2pix_debug.py', '--finetune-config.io.pretrained_model_name_or_path', 'timbrooks/instruct-pix2pix', '--finetune-config.io.output_dir', '/workspace/ViewDiff/output_var_debug', '--finetune-config.io.experiment_name', 'class6', '--finetune-config.training.mixed_precision', 'bf16', '--finetune-config.training.dataloader_num_workers', '4', '--finetune-config.training.num_train_epochs', '600', '--finetune-config.training.train_batch_size', '3', '--finetune-config.training.dreambooth_prior_preservation_loss_weight', '-1', '--finetune_config.training.noise_prediction_type', 'epsilon', '--finetune_config.training.prob_images_not_noisy', '0.25', '--finetune_config.training.max_num_images_not_noisy', '2', '--finetune_config.training.validation_epochs', '1', '--finetune_config.training.dreambooth_prior_preservation_every_nth', '-1', '--finetune-config.optimizer.learning_rate', '1e-5', '--finetune-config.optimizer.vol_rend_learning_rate', '1e-3', '--finetune-config.optimizer.vol_rend_adam_weight_decay', '0.0', '--finetune-config.optimizer.gradient_accumulation_steps', '1', '--finetune-config.optimizer.max_grad_norm', '5e-3', '--finetune-config.cross_frame_attention.to_k_other_frames', '2', '--finetune-config.cross_frame_attention.random_others', '--finetune-config.cross_frame_attention.with_self_attention', '--finetune-config.cross_frame_attention.use_temb_cond', '--finetune-config.cross_frame_attention.mode', 'pretrained', '--finetune-config.cross_frame_attention.n_cfa_down_blocks', '1', '--finetune-config.cross_frame_attention.n_cfa_up_blocks', '1', '--finetune-config.cross_frame_attention.unproj_reproj_mode', 'with_cfa', '--finetune-config.cross_frame_attention.num_3d_layers', '1', '--finetune-config.cross_frame_attention.dim_3d_latent', '16', '--finetune-config.cross_frame_attention.dim_3d_grid', '64', '--finetune-config.cross_frame_attention.n_novel_images', '1', '--finetune-config.cross_frame_attention.vol_rend_proj_in_mode', 'multiple', '--finetune-config.cross_frame_attention.vol_rend_proj_out_mode', 'multiple', '--finetune-config.cross_frame_attention.vol_rend_aggregator_mode', 'ibrnet', '--finetune-config.cross_frame_attention.last_layer_mode', 'zero-conv', '--finetune_config.cross_frame_attention.vol_rend_model_background', '--finetune_config.cross_frame_attention.vol_rend_background_grid_percentage', '0.5', '--finetune-config.model.pose_cond_mode', 'sa-ca', '--finetune-config.model.pose_cond_coord_space', 'absolute', '--finetune-config.model.pose_cond_lora_rank', '64', '--finetune-config.model.n_input_images', '3', '--dataset-config.root-dir', '/workspace/mvs_training/dtu', '--dataset-config.threshold', '0.8', '--dataset-config.split', 'train', '--dataset-config.img_wh', '512', '--dataset-config.debug', '0', '--validation-dataset-config.debug', '0', '--validation-dataset-config.root-dir', '/workspace/mvs_training/dtu', '--validation-dataset-config.split', 'val', '--validation-dataset-config.threshold', '0.8']' returned non-zero exit status 1.
