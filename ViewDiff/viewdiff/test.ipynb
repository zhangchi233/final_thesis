{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unet,textencoder,tokenizer,scheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPVisionModel\n",
    "from diffusers import AutoencoderKL,UNet2DConditionModel,DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(pretrained_model_name_or_path: str, revision: str = None):\n",
    "    noise_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\", \n",
    "                                                                      rescale_betas_zero_snr=True)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=revision)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=revision)\n",
    "    vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", revision=revision)\n",
    "    unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\", revision=revision)\n",
    "\n",
    "    return noise_scheduler, tokenizer, text_encoder, vae, unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "scheduelr, tokenizer, text_encoder, vae, unet = load_models(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import Literal, List, Optional, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Arguments for model.\"\"\"\n",
    "\n",
    "    n_input_images: int = 3\n",
    "    \"\"\"How many images are expected as input in parallel.\"\"\"\n",
    "\n",
    "    pose_cond_mode: Literal[\"none\", \"ca\", \"sa-ca\", \"sa-ca-cfa\", \"ca-cfa\"] = \"none\"\n",
    "    \"\"\"How to add the pose conditioning to the attention layers of the U-Net.\n",
    "        \"none\": do not add any pose-conditioning.\n",
    "        \"ca\": add it only to cross-attention (to text) layers.\n",
    "        \"sa-ca\": add it to self-attention and cross-attention layers.\n",
    "        \"sa-ca-cfa: add it to self-attention, cross-attention, and cross-frame-attention layers.\n",
    "        \"ca-cfa\": add it to cross-attention and cross-frame-attention layers.\n",
    "    \"\"\"\n",
    "\n",
    "    pose_cond_coord_space: Literal[\"absolute\", \"relative-first\"] = \"absolute\"\n",
    "    \"\"\"How to encode the pose conditioning.\n",
    "        \"absolute\": encode poses like in Zero123, but relative to the world-space-origin.\n",
    "        \"relative-first\": encode poses like in Zero123, where the first pose in each batch is the conditioning pose.\n",
    "    \"\"\"\n",
    "\n",
    "    pose_cond_lora_rank: int = 4\n",
    "    \"\"\"rank of the lora matrices used for the pose conditioning.\"\"\"\n",
    "\n",
    "    pose_cond_dim: int = 10\n",
    "    \"\"\"How many things we provide as pose conditioning. Currently it is 4 values for extrinsics (as in Zero123) + 4 values for intrinsics + 2 values for mean/var image intensity.\"\"\"\n",
    "\n",
    "    conditioning_dropout_prob: float = 0.1\n",
    "    \"\"\"Conditioning dropout probability. Drops out the conditionings (text prompt) used in training.\"\"\"\n",
    "\n",
    "    use_ema: bool = False\n",
    "    \"\"\"Whether to use EMA model.\"\"\"\n",
    "\n",
    "    enable_xformers_memory_efficient_attention: bool = True\n",
    "    \"\"\"Whether or not to use xformers.\"\"\"\n",
    "\n",
    "    gradient_checkpointing: bool = False\n",
    "    \"\"\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    \"\"\"Arguments for optimizer\"\"\"\n",
    "\n",
    "    learning_rate: float = 5e-5\n",
    "    \"\"\"Initial learning rate (after the potential warmup period) to use.\"\"\"\n",
    "\n",
    "    vol_rend_learning_rate: float = 1e-3\n",
    "    \"\"\"Initial learning rate (after the potential warmup period) to use for the volume-rendering components in the model.\"\"\"\n",
    "\n",
    "    vol_rend_adam_weight_decay: float = 0.0\n",
    "    \"\"\"Weight decay to use for the volume-rendering components in the model.\"\"\"\n",
    "\n",
    "    scale_lr: bool = False\n",
    "    \"\"\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\"\"\"\n",
    "\n",
    "    lr_scheduler: Literal[\n",
    "        \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"\n",
    "    ] = \"constant\"\n",
    "    \"\"\"The scheduler type to use.\"\"\"\n",
    "\n",
    "    lr_warmup_steps: int = 500\n",
    "    \"\"\"Number of steps for the warmup in the lr scheduler.\"\"\"\n",
    "\n",
    "    use_8bit_adam: bool = False\n",
    "    \"\"\"Whether or not to use 8-bit Adam from bitsandbytes.\"\"\"\n",
    "\n",
    "    allow_tf32: bool = False\n",
    "    \"\"\"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\n",
    "        https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\"\"\n",
    "\n",
    "    adam_beta1: float = 0.9\n",
    "    \"\"\"The beta1 parameter for the Adam optimizer.\"\"\"\n",
    "\n",
    "    adam_beta2: float = 0.999\n",
    "    \"\"\"The beta2 parameter for the Adam optimizer.\"\"\"\n",
    "\n",
    "    adam_weight_decay: float = 1e-2\n",
    "    \"\"\"Weight decay to use.\"\"\"\n",
    "\n",
    "    adam_epsilon: float = 1e-08\n",
    "    \"\"\"Epsilon value for the Adam optimizer\"\"\"\n",
    "\n",
    "    max_grad_norm: float = 0.1\n",
    "    \"\"\"Max gradient norm.\"\"\"\n",
    "\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \"\"\"Number of updates steps to accumulate before performing a backward/update pass.\"\"\"\n",
    "\n",
    "    only_train_new_layers: bool = False\n",
    "    \"\"\"If set, will only optimize over new layers (e.g. additional cross-frame attention layers).\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Arguments for training.\"\"\"\n",
    "\n",
    "    validation_epochs: int = 1\n",
    "    \"\"\"Run fine-tuning validation every X epochs.\"\"\"\n",
    "\n",
    "    train_batch_size: int = 1\n",
    "    \"\"\"Batch size (per device) for the training dataloader.\"\"\"\n",
    "\n",
    "    num_train_epochs: int = 500\n",
    "    \"\"\"epochs\"\"\"\n",
    "\n",
    "    max_train_steps: Optional[int] = None\n",
    "    \"\"\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\"\"\"\n",
    "\n",
    "    dataloader_num_workers: int = 4\n",
    "    \"\"\"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\"\"\n",
    "\n",
    "    local_rank: int = -1\n",
    "    \"\"\"For distributed training: local_rank\"\"\"\n",
    "\n",
    "    mixed_precision: Optional[Literal[\"no\", \"fp16\", \"bf16\"]] = None\n",
    "    \"\"\"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\n",
    "       and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\n",
    "        flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\"\"\n",
    "\n",
    "    noise_prediction_type: Literal[\"epsilon\", \"v_prediction\", \"sample\"] = \"epsilon\"\n",
    "    \"\"\"How to calculate the diffusion denoising MSE loss. The target is the applied noise ('epsilon'), the noise-free sample ('sample'), or the velocity ('v_prediction').\"\"\"\n",
    "\n",
    "    remove_cfa_skip_connections_at_iter: int = -1\n",
    "    \"\"\"If >-1, it will change the last_layer_mode in all cross-frame-attention transformer-blocks to --changed_cfa_last_layer after that many training iterations.\"\"\"\n",
    "\n",
    "    changed_cfa_last_layer: Literal[\"none\", \"no_residual_connection\"] = \"no_residual_connection\"\n",
    "    \"\"\"Change the last_layer_mode in all cross-frame-attention transformer-blocks to this value.\"\"\"\n",
    "\n",
    "    dreambooth_prior_preservation_loss_weight: float = 0.0\n",
    "    \"\"\"If >0, will use prior preservation loss during training similar to Dreambooth (see https://dreambooth.github.io/).\"\"\"\n",
    "\n",
    "    dreambooth_prior_preservation_every_nth: int = 5\n",
    "    \"\"\"Calculates the prior preservation loss every nth step.\"\"\"\n",
    "\n",
    "    prob_images_not_noisy: float = 0.25\n",
    "    \"\"\"With this probability, some of the images in a batch will be not noisy (e.g. input image conditioning).\"\"\"\n",
    "\n",
    "    max_num_images_not_noisy: int = 2\n",
    "    \"\"\"If some of the images in a batch should not be noisy: this defines the maximum number of images to which this applies.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaveConfig:\n",
    "    \"\"\"Which file types should be saved in #save_inference_outputs().\"\"\"\n",
    "\n",
    "    image_grids: bool = False\n",
    "    pred_files: bool = True\n",
    "    pred_video: bool = True\n",
    "    pred_gif: bool = False\n",
    "    denoise_files: bool = False\n",
    "    denoise_video: bool = False\n",
    "    cams: bool = True\n",
    "    prompts: bool = True\n",
    "    rendered_depth: bool = False\n",
    "    cond_files: bool = False\n",
    "    image_metrics: bool = True\n",
    "\n",
    "@dataclass\n",
    "class IOConfig:\n",
    "    \"\"\"Arguments for IO.\"\"\"\n",
    "\n",
    "    save: SaveConfig = SaveConfig()\n",
    "\n",
    "    pretrained_model_name_or_path: str = \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    \"\"\"Path to pretrained model or model identifier from huggingface.co/models\"\"\"\n",
    "\n",
    "    revision: Optional[str] = None\n",
    "    \"\"\"Revision of pretrained model identifier from huggingface.co/models.\"\"\"\n",
    "\n",
    "    output_dir: str = \"output\"\n",
    "    \"\"\"The output directory where the model predictions and checkpoints will be written.\"\"\"\n",
    "\n",
    "    experiment_name: Optional[str] = None\n",
    "    \"\"\"If this is set, will use this instead of the datetime string as identifier for the experiment.\"\"\"\n",
    "\n",
    "    logging_dir: str = \"logs\"\n",
    "    \"\"\"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
    "        *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\"\"\n",
    "\n",
    "    log_images_every_nth: int = 500\n",
    "    \"\"\"log images every nth step\"\"\"\n",
    "\n",
    "    report_to: Literal[\"tensorboard\", \"custom_tensorboard\"] = \"custom_tensorboard\"\n",
    "    \"\"\"The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`\"\"\"\n",
    "\n",
    "    checkpointing_steps: int = 500\n",
    "    \"\"\"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\n",
    "        training using `--resume_from_checkpoint`.\"\"\"\n",
    "\n",
    "    checkpoints_total_limit: int = 2\n",
    "    \"\"\"Max number of checkpoints to store.\"\"\"\n",
    "\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    \"\"\"Whether training should be resumed from a previous checkpoint. Use a path saved by\n",
    "        ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.\"\"\"\n",
    "\n",
    "    automatic_checkpoint_resume: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrossFrameAttentionConfig:\n",
    "    mode: Literal[\"none\", \"pretrained\", \"add_in_existing_block\"] = \"none\"\n",
    "    \"\"\"How to add cross-frame attention to the U-Net\n",
    "        \"none\": do not add cross-frame attention.\n",
    "        \"pretrained\": add cross-frame attention in the existing self-attention layers of the U-Net.\n",
    "        \"add_in_existing_block\": add cross-frame attention by inserting a new attention layer in each BasicTransformerBlock.\n",
    "    \"\"\"\n",
    "\n",
    "    n_cfa_down_blocks: int = 1\n",
    "    \"\"\"How many of the down_blocks in the U-Net should be replaced with blocks that contain cross-frame-attention.\"\"\"\n",
    "\n",
    "    n_cfa_up_blocks: int = 1\n",
    "    \"\"\"How many of the up_blocks in the U-Net should be replaced with blocks that contain cross-frame-attention.\"\"\"\n",
    "\n",
    "    no_cfa_in_mid_block: bool = False\n",
    "    \"\"\"If we should not use cross-frame-attention in the mid_block. Default behaviour: use it as soon as mode!=none. This forcibly overrides the setting.\"\"\"\n",
    "\n",
    "    to_k_other_frames: int = 2\n",
    "    \"\"\"How many of the other images in a batch to use as key/value.\"\"\"\n",
    "\n",
    "    with_self_attention: bool = False\n",
    "    \"\"\"If the key/value of the query image should be appended. Only relevant for mode='pretrained'.\"\"\"\n",
    "\n",
    "    random_others: bool = False\n",
    "    \"\"\"If True, will select the k_other_frames randomly, otherwise sequentially.\"\"\"\n",
    "\n",
    "    last_layer_mode: Literal[\"none\", \"zero-conv\", \"alpha\", \"no_residual_connection\"] = \"zero-conv\"\n",
    "    \"\"\"How to add the contributions of cross-frame-attention.\n",
    "        'none': directly add them to the residual connection in the ViT blocks.\n",
    "        'zero-conv': add them with the zero-conv idea from ControlNet.\n",
    "        'alpha': add them with the alpha idea from VideoLDM.\n",
    "        'no_residual_connection': do not use residual connection, instead use the output directly.\"\"\"\n",
    "\n",
    "    unproj_reproj_mode: Literal[\"none\", \"only_unproj_reproj\", \"with_cfa\"] = \"none\"\n",
    "    \"\"\"How to use unproj_reproj as layer in the model.\n",
    "        \"none\": do not use the layer.\n",
    "        \"only_unproj_reproj\": use the layer instead of cross-frame-attention.\n",
    "        \"with_cfa\": use the layer in addition to cross-frame-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    num_3d_layers: int = 1\n",
    "    \"\"\"how many 3D layers to use in the 3D CNN.\"\"\"\n",
    "\n",
    "    dim_3d_latent: int = 32\n",
    "    \"\"\"dimension of the 3D latent features to process with the 3D CNN.\"\"\"\n",
    "\n",
    "    dim_3d_grid: int = 64\n",
    "    \"\"\"dimension of the 3D voxel grid to process with the 3D CNN.\"\"\"\n",
    "\n",
    "    vol_rend_proj_in_mode: Literal[\"single\", \"multiple\", \"unet\"] = \"unet\"\n",
    "    \"\"\"How to convert hidden states into features for our proj-module.\n",
    "        'single': a single 1x1 convolution is used\n",
    "        'multiple': 2x{Conv_3x3, Relu} is used\n",
    "        'unet': a 3-layer unet is used\"\"\"\n",
    "\n",
    "    vol_rend_proj_out_mode: Literal[\"single\", \"multiple\"] = \"multiple\"\n",
    "    \"\"\"How to convert projected features into hidden states after our proj-module. We always use a linear combination of foreground and background.\n",
    "    For the background, we always use a single Conv_1x1. For the foreground we use either:\n",
    "    'single': a single 1x1 convolution\n",
    "    'multiple': 2x{Conv_1x1, Relu} followed by Conv_1x1 (==learned non-linear scale function)\"\"\"\n",
    "\n",
    "    vol_rend_aggregator_mode: Literal[\"mean\", \"ibrnet\"] = \"ibrnet\"\n",
    "    \"\"\"How to convert per-frame voxel grids into a joint voxel-grid across all frames.\n",
    "    'mean': mean across each voxel in each frame.\n",
    "    'ibrnet': use the aggregator as proposed in IBRNet (https://arxiv.org/abs/2102.13090)\"\"\"\n",
    "\n",
    "    vol_rend_model_background: bool = False\n",
    "    \"\"\"If the volume-rendering module should model the background.\"\"\"\n",
    "\n",
    "    vol_rend_background_grid_percentage: float = 0.5\n",
    "    \"\"\"How much of the voxel grid should be used for background, if we need to model it.\"\"\"\n",
    "\n",
    "    vol_rend_disparity_at_inf: float = 0.5\n",
    "    \"\"\"The value for disparity_at_inf argument for the volume-rendering module.\"\"\"\n",
    "\n",
    "    n_novel_images: int = 1\n",
    "    \"\"\"during unprojection, how many images to consider novel. They are not used for unprojection, but only for reprojection (novel-view-synthesis).\"\"\"\n",
    "\n",
    "    use_temb_cond: bool = True\n",
    "    \"\"\"If True, will use timestep embedding as additional input for cross-frame-attention and projection layers. Useful to process a batch with images of different timesteps.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    training: TrainingConfig\n",
    "    optimizer: OptimizerConfig\n",
    "    model: ModelConfig\n",
    "    cross_frame_attention: CrossFrameAttentionConfig\n",
    "    io: IOConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define transformerblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/ViewDiff/viewdiff\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import os\n",
    "import tyro\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from accelerate import Accelerator, skip_first_batches\n",
    "from accelerate.logging import get_logger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/root/autodl-tmp/ViewDiff/\")\n",
    "from viewdiff.model.custom_unet_2d_condition import (\n",
    "    UNet2DConditionCrossFrameInExistingAttnModel,\n",
    "    get_down_block_types,\n",
    "    get_mid_block_type,\n",
    "    get_up_block_types,\n",
    ")\n",
    "from viewdiff.model.util import (\n",
    "    replace_self_attention_with_cross_frame_attention,\n",
    "    update_last_layer_mode,\n",
    "    update_vol_rend_inject_noise_sigma,\n",
    "    update_n_novel_images,\n",
    "    update_cross_frame_attention_config,\n",
    "    add_pose_cond_to_attention_layers,\n",
    "    collapse_prompt_to_batch_dim,\n",
    "    collapse_tensor_to_batch_dim,\n",
    "    expand_output_to_k,\n",
    "    expand_tensor_to_k,\n",
    "    tokenize_captions,\n",
    "    ModelConfig,\n",
    "    CrossFrameAttentionConfig,\n",
    "    build_cross_attention_kwargs,\n",
    ")\n",
    "from viewdiff.model.custom_stable_diffusion_pipeline import CustomStableDiffusionPipeline\n",
    "\n",
    "from viewdiff.io_util import (\n",
    "    make_image_grid,\n",
    "    norm_0_1,\n",
    "    setup_output_directories,\n",
    "    make_output_directories,\n",
    "    save_inference_outputs,\n",
    "    IOConfig,\n",
    ")\n",
    "from viewdiff.train_util import (\n",
    "    check_local_rank,\n",
    "    FinetuneConfig,\n",
    "    load_models,\n",
    "    setup_accelerate,\n",
    "    setup_model_and_optimizer,\n",
    "    setup_train_val_dataloaders,\n",
    "    setup_training,\n",
    "    save_checkpoint,\n",
    "    maybe_continue_training,\n",
    ")\n",
    "\n",
    "from viewdiff.metrics.image_metrics import calc_psnr_ssim_lpips\n",
    "\n",
    "from viewdiff.data.co3d.co3d_dataset import CO3DConfig\n",
    "\n",
    "from viewdiff.scripts.misc.create_masked_images import remove_background\n",
    "\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuneconfig = FinetuneConfig(TrainingConfig(), OptimizerConfig(), ModelConfig(), CrossFrameAttentionConfig(), IOConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(finetune_config: FinetuneConfig, unet: UNet2DConditionModel):\n",
    "    if finetune_config.cross_frame_attention.mode != \"none\":\n",
    "        use_lora_in_cfa = \"cfa\" in finetune_config.model.pose_cond_mode\n",
    "        if (\n",
    "            finetune_config.cross_frame_attention.mode == \"pretrained\"\n",
    "            or finetune_config.cross_frame_attention.mode == \"add_in_existing_block\"\n",
    "        ):\n",
    "            # if finetune_config.cross_frame_attention.mode == \"pretrained\":\n",
    "            #     # overwrite the settings for cfa to not create the cfa layers\n",
    "            #     # instead we want to re-use the sa layers for it\n",
    "            #     if finetune_config.cross_frame_attention.unproj_reproj_mode == \"with_cfa\":\n",
    "            #         finetune_config.cross_frame_attention.unproj_reproj_mode = \"only_unproj_reproj\"\n",
    "\n",
    "            unet = UNet2DConditionCrossFrameInExistingAttnModel.from_source(\n",
    "                src=unet,\n",
    "                load_weights=True,\n",
    "                down_block_types=get_down_block_types(finetune_config.cross_frame_attention.n_cfa_down_blocks),\n",
    "                mid_block_type=get_mid_block_type(not finetune_config.cross_frame_attention.no_cfa_in_mid_block),\n",
    "                up_block_types=get_up_block_types(finetune_config.cross_frame_attention.n_cfa_up_blocks),\n",
    "                n_input_images=finetune_config.model.n_input_images,\n",
    "                to_k_other_frames=finetune_config.cross_frame_attention.to_k_other_frames,\n",
    "                random_others=finetune_config.cross_frame_attention.random_others,\n",
    "                last_layer_mode=finetune_config.cross_frame_attention.last_layer_mode,\n",
    "                use_lora_in_cfa=use_lora_in_cfa,\n",
    "                use_temb_in_lora=finetune_config.cross_frame_attention.use_temb_cond,\n",
    "                temb_out_size=8,\n",
    "                pose_cond_dim=finetune_config.model.pose_cond_dim,\n",
    "                rank=finetune_config.model.pose_cond_lora_rank,\n",
    "                unproj_reproj_mode=finetune_config.cross_frame_attention.unproj_reproj_mode,\n",
    "                dim_3d_grid=finetune_config.cross_frame_attention.dim_3d_grid,\n",
    "                dim_3d_latent=finetune_config.cross_frame_attention.dim_3d_latent,\n",
    "                n_novel_images=finetune_config.cross_frame_attention.n_novel_images,\n",
    "                num_3d_layers=finetune_config.cross_frame_attention.num_3d_layers,\n",
    "                vol_rend_proj_in_mode=finetune_config.cross_frame_attention.vol_rend_proj_in_mode,\n",
    "                vol_rend_aggregator_mode=finetune_config.cross_frame_attention.vol_rend_aggregator_mode,\n",
    "                vol_rend_proj_out_mode=finetune_config.cross_frame_attention.vol_rend_proj_out_mode,\n",
    "                vol_rend_model_background=finetune_config.cross_frame_attention.vol_rend_model_background,\n",
    "                vol_rend_background_grid_percentage=finetune_config.cross_frame_attention.vol_rend_background_grid_percentage,\n",
    "                vol_rend_disparity_at_inf=finetune_config.cross_frame_attention.vol_rend_disparity_at_inf,\n",
    "            )\n",
    "\n",
    "            if finetune_config.cross_frame_attention.mode == \"pretrained\":\n",
    "                # TODO: allow to only replace the layers as specified in finetune_config.cross_frame_attention.n_cfa_down_blocks\n",
    "                replace_self_attention_with_cross_frame_attention(\n",
    "                    unet=unet,\n",
    "                    n_input_images=finetune_config.model.n_input_images,\n",
    "                    to_k_other_frames=finetune_config.cross_frame_attention.to_k_other_frames,\n",
    "                    with_self_attention=finetune_config.cross_frame_attention.with_self_attention,\n",
    "                    random_others=finetune_config.cross_frame_attention.random_others,\n",
    "                    use_lora_in_cfa=use_lora_in_cfa or \"sa\" in finetune_config.model.pose_cond_mode,\n",
    "                    use_temb_in_lora=finetune_config.cross_frame_attention.use_temb_cond,\n",
    "                    temb_out_size=8,\n",
    "                    pose_cond_dim=finetune_config.model.pose_cond_dim,\n",
    "                    rank=finetune_config.model.pose_cond_lora_rank,\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"unsupported cross_frame_attention.mode\", finetune_config.cross_frame_attention.mode\n",
    "            )\n",
    "\n",
    "    unet_lora_parameters = None\n",
    "    if finetune_config.model.pose_cond_mode != \"none\":\n",
    "        # Set correct lora layers\n",
    "        unet_lora_attn_procs, unet_lora_parameters = add_pose_cond_to_attention_layers(\n",
    "            unet,\n",
    "            rank=finetune_config.model.pose_cond_lora_rank,\n",
    "            pose_cond_dim=finetune_config.model.pose_cond_dim,\n",
    "            only_cross_attention=\"sa\" not in finetune_config.model.pose_cond_mode,\n",
    "        )\n",
    "\n",
    "    return unet, unet_lora_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuneconfig.io.pretrained_model_name_or_path =  model_id\n",
    "finetuneconfig.io.output_dir= \"/root/autodl-tmp/ViewDiff/output\"\n",
    "finetuneconfig.io.experiment_name=\" train_teddybear\"\n",
    "finetuneconfig.training.mixed_precision = \"no\"\n",
    "finetuneconfig.training.dataloader_num_workers= 4\n",
    "finetuneconfig.training.num_train_epochs = 1000\n",
    "finetuneconfig.training.train_batch_size =  1\n",
    "finetuneconfig.training.dreambooth_prior_preservation_loss_weight=-1\n",
    "finetuneconfig.training.noise_prediction_type =  \"epsilon\"\n",
    "finetuneconfig.training.prob_images_not_noisy= 0.25\n",
    "finetuneconfig.training.max_num_images_not_noisy= 2,\n",
    "finetuneconfig.training.validation_epochs= 1\n",
    "finetuneconfig.training.dreambooth_prior_preservation_every_nth=-1\n",
    "finetuneconfig.optimizer.learning_rate=5e-5\n",
    "finetuneconfig.optimizer.vol_rend_learning_rate =1e-3\n",
    "finetuneconfig.optimizer.vol_rend_adam_weight_decay = 0.0\n",
    "finetuneconfig.optimizer.gradient_accumulation_steps =  1\n",
    "finetuneconfig.optimizer.max_grad_norm = 5e-3\n",
    "finetuneconfig.cross_frame_attention.to_k_other_frames =  2\n",
    "\n",
    "finetuneconfig.cross_frame_attention.mode =\"pretrained\"\n",
    "finetuneconfig.cross_frame_attention.n_cfa_down_blocks = 1\n",
    "finetuneconfig.cross_frame_attention.n_cfa_up_blocks= 1\n",
    "finetuneconfig.cross_frame_attention.unproj_reproj_mode = \"with_cfa\"\n",
    "\n",
    "finetuneconfig.model.pose_cond_mode =\"sa-ca\"\n",
    "finetuneconfig.model.pose_cond_coord_space = \"absolute\"\n",
    "finetuneconfig.model.pose_cond_lora_rank = 64\n",
    "finetuneconfig.model.n_input_images =3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet, unet_lora_parameters = update_model(finetuneconfig, unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _encode_prompt(\n",
    "        \n",
    "        prompt,\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "        negative_prompt=None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "        Args:\n",
    "             prompt (`str` or `List[str]`, *optional*):\n",
    "                prompt to be encoded\n",
    "            device: (`torch.device`):\n",
    "                torch device\n",
    "            num_images_per_prompt (`int`):\n",
    "                number of images that should be generated per prompt\n",
    "            do_classifier_free_guidance (`bool`):\n",
    "                whether to use classifier free guidance or not\n",
    "            negative_ prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
    "                less than `1`).\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "        \"\"\"\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        if prompt_embeds is None:\n",
    "            # textual inversion: process multi-vector tokens if necessary\n",
    "            \n",
    "            text_inputs =tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "            untruncated_ids =tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "                text_input_ids, untruncated_ids\n",
    "            ):\n",
    "                removed_text = tokenizer.batch_decode(\n",
    "                    untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "                )\n",
    "                logger.warning(\n",
    "                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                    f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "                )\n",
    "\n",
    "            if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                attention_mask = text_inputs.attention_mask.to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "            prompt_embeds = prompt_embeds[0]\n",
    "\n",
    "        if text_encoder is not None:\n",
    "            prompt_embeds_dtype = text_encoder.dtype\n",
    "        else:\n",
    "            prompt_embeds_dtype = unet.dtype\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "            uncond_tokens: List[str]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"] * batch_size\n",
    "            elif type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "\n",
    "            # textual inversion: process multi-vector tokens if necessary\n",
    "            \n",
    "\n",
    "            max_length = prompt_embeds.shape[1]\n",
    "            uncond_input = tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "                attention_mask = uncond_input.attention_mask.to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            negative_prompt_embeds = text_encoder(\n",
    "                uncond_input.input_ids.to(device),\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            # pix2pix has two negative embeddings, and unlike in other pipelines latents are ordered [prompt_embeds, negative_prompt_embeds, negative_prompt_embeds]\n",
    "            prompt_embeds = torch.cat([prompt_embeds, negative_prompt_embeds, negative_prompt_embeds])\n",
    "\n",
    "        return prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pose_cond cannot be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.bjb1.seetacloud.com/root/autodl-tmp/ViewDiff/viewdiff/test.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m unet(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m3\u001b[39;49m,\u001b[39m8\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m256\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda(),torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mcuda(), torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m, \u001b[39m77\u001b[39;49m,\u001b[39m768\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/autodl-tmp/ViewDiff/viewdiff/model/custom_unet_2d_condition.py:1522\u001b[0m, in \u001b[0;36mUNet2DConditionCrossFrameInExistingAttnModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, encoder_attention_mask, return_dict, n_images_per_batch, n_known_images)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(\n\u001b[1;32m   1512\u001b[0m             hidden_states\u001b[39m=\u001b[39msample,\n\u001b[1;32m   1513\u001b[0m             temb\u001b[39m=\u001b[39memb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39madditional_residuals,\n\u001b[1;32m   1520\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1522\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(\n\u001b[1;32m   1523\u001b[0m             hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m   1524\u001b[0m             temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m   1525\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1526\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1527\u001b[0m             cross_attention_kwargs\u001b[39m=\u001b[39;49mproc_ca_kwargs,\n\u001b[1;32m   1528\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1529\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madditional_residuals,\n\u001b[1;32m   1530\u001b[0m         )\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1532\u001b[0m     sample, res_samples \u001b[39m=\u001b[39m downsample_block(hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py:1086\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb, scale\u001b[39m=\u001b[39mlora_scale)\n\u001b[0;32m-> 1086\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn(\n\u001b[1;32m   1087\u001b[0m         hidden_states,\n\u001b[1;32m   1088\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1089\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m   1090\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1091\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1092\u001b[0m         return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1093\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1095\u001b[0m \u001b[39m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(blocks) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m additional_residuals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/diffusers/models/transformer_2d.py:315\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    303\u001b[0m         hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    304\u001b[0m             block,\n\u001b[1;32m    305\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m             use_reentrant\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m         hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    316\u001b[0m             hidden_states,\n\u001b[1;32m    317\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    318\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    319\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    320\u001b[0m             timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    321\u001b[0m             cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    322\u001b[0m             class_labels\u001b[39m=\u001b[39;49mclass_labels,\n\u001b[1;32m    323\u001b[0m         )\n\u001b[1;32m    325\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/diffusers/models/attention.py:218\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     norm_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states, timestep) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ada_layer_norm \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(hidden_states)\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 218\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn2(\n\u001b[1;32m    219\u001b[0m         norm_hidden_states,\n\u001b[1;32m    220\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    221\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    222\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn_output \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    226\u001b[0m \u001b[39m# 4. Feed-forward\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/diffusers/models/attention_processor.py:420\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    417\u001b[0m     \u001b[39m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessor(\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    422\u001b[0m         hidden_states,\n\u001b[1;32m    423\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    424\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    425\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    426\u001b[0m     )\n",
      "File \u001b[0;32m~/autodl-tmp/ViewDiff/viewdiff/model/custom_attention_processor.py:339\u001b[0m, in \u001b[0;36mPoseCondLoRAAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, scale, pose_cond)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    336\u001b[0m     \u001b[39mself\u001b[39m, attn: Attention, hidden_states, encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, temb\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, scale\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, pose_cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    337\u001b[0m ):\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m pose_cond \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpose_cond cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    341\u001b[0m     residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m attn\u001b[39m.\u001b[39mspatial_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: pose_cond cannot be None"
     ]
    }
   ],
   "source": [
    "unet(torch.rand(3,8, 256, 256).cuda(),torch.tensor([1,1,1]).cuda(), torch.rand(1, 77,768).cuda(),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
