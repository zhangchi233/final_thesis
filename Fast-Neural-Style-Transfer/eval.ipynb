{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from models import TransformerNet, VGG16\n",
    "from utils import *\n",
    "from deptLoss import SL1Loss\n",
    "import sys\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"/root/autodl-tmp/Fast-Neural-Style-Transfer/\")\n",
    "sys.path.append(\"/root/autodl-tmp/taming-transformers/\")\n",
    "\n",
    "\n",
    "from taming.data.dtu import DTUDataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Parser for Fast-Neural-Style\")\n",
    "    parser.add_argument(\"--dataset_path\", type=str, required=True,default = \"/root/autodl-tmp/mvs_training\", help=\"path to training dataset\")\n",
    "    parser.add_argument(\"--style_image\", type=str, default=\"style-images/mosaic.jpg\", help=\"path to style image\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1000, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--image_size\", type=int, default=256, help=\"Size of training images\")\n",
    "    parser.add_argument(\"--style_size\", type=int, help=\"Size of style image\")\n",
    "    parser.add_argument(\"--lambda_content\", type=float, default=1e5, help=\"Weight for content loss\")\n",
    "    parser.add_argument(\"--lambda_style\", type=float, default=1e10, help=\"Weight for style loss\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--checkpoint_model\", type=str, help=\"Optional path to checkpoint model\")\n",
    "    parser.add_argument(\"--checkpoint_interval\", type=int, default=2000, help=\"Batches between saving model\")\n",
    "    parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"Batches between saving image samples\")\n",
    "    args = parser.parse_args([\"--dataset_path\",\"/root/autodl-tmp/mvs_training/dtu\",\n",
    "                              \"--checkpoint_model\",\"/root/autodl-tmp/checkpoints/mosaic_60000.pth\"])\n",
    "\n",
    "    style_name = args.style_image.split(\"/\")[-1].split(\".\")[0]\n",
    "    os.makedirs(f\"images/outputs/{style_name}-training\", exist_ok=True)\n",
    "    os.makedirs(f\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create dataloader for the training data\n",
    "    train_dataset = DTUDataset(root_dir = args.dataset_path,split=\"val\")\n",
    "    dataloader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    # Defines networks\n",
    "    transformer = TransformerNet().to(device)\n",
    "    vgg = VGG16(requires_grad=False).to(device)\n",
    "\n",
    "    # Load checkpoint model if specified\n",
    "    if args.checkpoint_model:\n",
    "        transformer.load_state_dict(torch.load(args.checkpoint_model))\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer = Adam(transformer.parameters(), args.lr)\n",
    "    l2_loss = torch.nn.MSELoss().to(device)\n",
    "    cal_depthloss = SL1Loss().to(device)\n",
    "\n",
    "    # Load style image\n",
    "    # style = style_transform(args.style_size)(Image.open(args.style_image))\n",
    "    # style = style.repeat(args.batch_size, 1, 1, 1).to(device)\n",
    "\n",
    "    # # Extract style features\n",
    "    # features_style = vgg(style)\n",
    "    # gram_style = [gram_matrix(y) for y in features_style]\n",
    "\n",
    "    # Sample 8 images for visual evaluation of the model\n",
    "    # image_samples = []\n",
    "    # for path in random.sample(glob.glob(f\"{args.dataset_path}/*/*.png\"), 8):\n",
    "    #     image_samples += [style_transform(args.image_size)(Image.open(path))]\n",
    "    # image_samples = torch.stack(image_samples)\n",
    "\n",
    "    def save_sample(batches_done,image_samples):\n",
    "        \"\"\" Evaluates the model and saves image samples \"\"\"\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            output = transformer(image_samples.to(device))\n",
    "        image_grid = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2))\n",
    "        save_image(image_grid, f\"/root/autodl-tmp/images/outputs/{style_name}-training/{batches_done}.jpg\", nrow=4)\n",
    "        transformer.train()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_metrics = {\"content\": [], \"style\": [], \"total\": [],\"depth\":[]}\n",
    "        tqdm_bar = tqdm(dataloader)\n",
    "        \n",
    "        for batch_i, batch in enumerate(tqdm_bar):\n",
    "            target_imgs = batch['target_imgs']\n",
    "\n",
    "            images, proj_mats, depths, masks, init_depth_min, depth_interval = decode_batch(batch)\n",
    "\n",
    "            target_imgs = rearrange(target_imgs, 'b n c h w -> b c h (n w)', n=3)\n",
    "            images = rearrange(images, 'b n c h w -> b c h (n w)', n=3)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images_original = images.to(device)\n",
    "            images_transformed = transformer(images_original)\n",
    "\n",
    "            # Extract features\n",
    "            features_original = vgg(images_original)\n",
    "            features_transformed = vgg(images_transformed)\n",
    "\n",
    "            # Compute content loss as MSE between features\n",
    "            with torch.no_grad():\n",
    "              content_loss =  l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n",
    "\n",
    "            # Compute style loss as MSE between gram matrices\n",
    "            style_loss = 0\n",
    "            style_loss = l2_loss(images_original, images_transformed)*10\n",
    "            # for ft_y, gm_s in zip(features_transformed, gram_style):\n",
    "            #     gm_y = gram_matrix(ft_y)\n",
    "            #     style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n",
    "            \n",
    "            depth_loss,depth_ori,log = cal_depthloss(images_transformed, images_original, proj_mats, depths, masks, init_depth_min, depth_interval)\n",
    "            depthloss = (depth_loss-depth_ori)*10\n",
    "\n",
    "\n",
    "            total_loss = depthloss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_metrics[\"content\"] += [content_loss.item()]\n",
    "            epoch_metrics[\"depth\"] += [depth_loss.item()/depth_ori.item()]\n",
    "            epoch_metrics[\"total\"] += [total_loss.item()]\n",
    "            epoch_metrics[\"style\"] += [style_loss.item()]\n",
    "\n",
    "\n",
    "            # sys.stdout.write(\n",
    "            #     \"\\r[Epoch %d/%d] [Batch %d/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n",
    "            #     % (\n",
    "            #         epoch + 1,\n",
    "            #         args.epochs,\n",
    "            #         batch_i,\n",
    "            #         len(train_dataset),\n",
    "            #         content_loss.item(),\n",
    "            #         np.mean(epoch_metrics[\"content\"]),\n",
    "            #         depth_loss.item(),\n",
    "            #         np.mean(epoch_metrics[\"depth\"]),\n",
    "            #         total_loss.item(),\n",
    "            #         np.mean(epoch_metrics[\"total\"]),\n",
    "            #     )\n",
    "            # )\n",
    "            tqdm_bar.set_postfix_str(\n",
    "                f\"[Epoch {epoch + 1}/{args.epochs}] [Batch {batch_i}/{len(train_dataset)}] \"\n",
    "                f\"[Content: {np.mean(epoch_metrics['content']):.2f}] \"\n",
    "                f\"[Depth: {np.mean(epoch_metrics['depth']):.2f}] \"\n",
    "                f\"[Style: {np.mean(epoch_metrics['style']):.2f}] \"\n",
    "                f\"[Total: {np.mean(epoch_metrics['total']):.2f}]\"\n",
    "            )\n",
    "            \n",
    "            batches_done = epoch * len(dataloader) + batch_i + 1\n",
    "            if batches_done % args.sample_interval == 0:\n",
    "\n",
    "                save_sample(batches_done,images)\n",
    "\n",
    "            if args.checkpoint_interval > 0 and batches_done % args.checkpoint_interval == 0:\n",
    "                style_name = os.path.basename(args.style_image).split(\".\")[0]\n",
    "                torch.save(transformer.state_dict(), f\"checkpoints/{style_name}_{batches_done}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
