{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, datetime, glob, importlib\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "import sys\n",
    "\n",
    "from taming.data.dtu import DTUDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n",
    "\n",
    "\n",
    "def get_parser(**parser_kwargs):\n",
    "    def str2bool(v):\n",
    "        if isinstance(v, bool):\n",
    "            return v\n",
    "        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "            return True\n",
    "        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "            return False\n",
    "        else:\n",
    "            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(**parser_kwargs)\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        const=False,\n",
    "        default=\"\",\n",
    "        nargs=\"?\",\n",
    "        help=\"postfix for logdir\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-r\",\n",
    "        \"--resume\",\n",
    "        type=str,\n",
    "        const=True,\n",
    "        default=\"/root/autodl-tmp/taming-transformers/scripts/logs/transformers/checkpoints/last.ckpt\",\n",
    "        nargs=\"?\",\n",
    "        help=\"resume from logdir or checkpoint in logdir\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\",\n",
    "        \"--base\",\n",
    "        nargs=\"*\",\n",
    "        metavar=\"/root/autodl-tmp/taming-transformers/scripts/logs/vqgan_gumbel_f8/configs/model.yaml\",\n",
    "        help=\"paths to base configs. Loaded from left-to-right. \"\n",
    "        \"Parameters can be overwritten or added with command-line options of the form `--key value`.\",\n",
    "        default=list(\"/root/autodl-tmp/taming-transformers/scripts/logs/vqgan_gumbel_f8/configs/model.yaml\"),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\",\n",
    "        \"--train\",\n",
    "        type=str2bool,\n",
    "        const=True,\n",
    "        default=True,\n",
    "        nargs=\"?\",\n",
    "        help=\"train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-test\",\n",
    "        type=str2bool,\n",
    "        const=True,\n",
    "        default=False,\n",
    "        nargs=\"?\",\n",
    "        help=\"disable test\",\n",
    "    )\n",
    "    parser.add_argument(\"-p\", \"--project\", help=\"name of new or path to existing project\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--debug\",\n",
    "        type=str2bool,\n",
    "        nargs=\"?\",\n",
    "        const=True,\n",
    "        default=False,\n",
    "        help=\"enable post-mortem debugging\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=23,\n",
    "        help=\"seed for seed_everything\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--postfix\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"post-postfix for default name\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def nondefault_trainer_args(opt):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args([])\n",
    "    return sorted(k for k in vars(args) if getattr(opt, k) != getattr(args, k))\n",
    "\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "class WrappedDataset(Dataset):\n",
    "    \"\"\"Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class DataModuleFromConfig(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, root_dir = \"/root/autodl-tmp/mvs_training/dtu\",train=True,\n",
    "                  validation=True, test=None,\n",
    "                 wrap=False, num_workers=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.datasets = dict()\n",
    "        self.num_workers = num_workers if num_workers is not None else batch_size*2\n",
    "        if train is not None:\n",
    "            self.datasets[\"train\"] = DTUDataset(root_dir, split=\"train\")\n",
    "            self.train_dataloader = self._train_dataloader\n",
    "        if validation is not None:\n",
    "            self.datasets[\"validation\"] =  DTUDataset(root_dir, split=\"val\")\n",
    "            self.val_dataloader = self._val_dataloader\n",
    "        if test is not None:\n",
    "            self.datasets[\"test\"] =  DTUDataset(root_dir, split=\"test\")\n",
    "            self.test_dataloader = self._test_dataloader\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def _train_dataloader(self):\n",
    "        return DataLoader(self.datasets[\"train\"], batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def _val_dataloader(self):\n",
    "        return DataLoader(self.datasets[\"validation\"],\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def _test_dataloader(self):\n",
    "        return DataLoader(self.datasets[\"test\"], batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "class ImageLogger(Callback):\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        self.log_img(pl_module, batch, batch_idx, split=\"train\")\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        self.log_img(pl_module, batch, batch_idx, split=\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom parser to specify config files, train, test and debug mode,\n",
    "# postfix, resume.\n",
    "# `--key value` arguments are interpreted as arguments to the trainer.\n",
    "# `nested.key=value` arguments are interpreted as config parameters.\n",
    "# configs are merged from left-to-right followed by command line parameters.\n",
    "\n",
    "# model:\n",
    "#   base_learning_rate: float\n",
    "#   target: path to lightning module\n",
    "#   params:\n",
    "#       key: value\n",
    "# data:\n",
    "#   target: main.DataModuleFromConfig\n",
    "#   params:\n",
    "#      batch_size: int\n",
    "#      wrap: bool\n",
    "#      train:\n",
    "#          target: path to train dataset\n",
    "#          params:\n",
    "#              key: value\n",
    "#      validation:\n",
    "#          target: path to validation dataset\n",
    "#          params:\n",
    "#              key: value\n",
    "#      test:\n",
    "#          target: path to test dataset\n",
    "#          params:\n",
    "#              key: value\n",
    "# lightning: (optional, has sane defaults and can be specified on cmdline)\n",
    "#   trainer:\n",
    "#       additional arguments to trainer\n",
    "#   logger:\n",
    "#       logger to instantiate\n",
    "#   modelcheckpoint:\n",
    "#       modelcheckpoint to instantiate\n",
    "#   callbacks:\n",
    "#       callback1:\n",
    "#           target: importpath\n",
    "#           params:\n",
    "#               key: value\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "# add cwd for convenience and to make classes in this file available when\n",
    "# running as `python main.py`\n",
    "# (in particular `main.DataModuleFromConfig`)\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "parser = get_parser()\n",
    "# parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "# if opt.name and opt.resume:\n",
    "#     raise ValueError(\n",
    "#         \"-n/--name and -r/--resume cannot be specified both.\"\n",
    "#         \"If you want to resume training in a new log folder, \"\n",
    "#         \"use -n/--name in combination with --resume_from_checkpoint\"\n",
    "#     )\n",
    "# if opt.resume:\n",
    "#     if not os.path.exists(opt.resume):\n",
    "#         raise ValueError(\"Cannot find {}\".format(opt.resume))\n",
    "#     if os.path.isfile(opt.resume):\n",
    "#         paths = opt.resume.split(\"/\")\n",
    "#         # create new log dir\\\n",
    "#         print(paths)\n",
    "#         log_path = \"/\".join(paths[:-2])\n",
    "#         now = f\"/logs/\"\n",
    "#         log_path = os.path.join(log_path, now)\n",
    "#         os.makedirs(log_path, exist_ok=True)\n",
    "        \n",
    "#         logdir =log_path \n",
    "        \n",
    "#         print(f\"New logdir: {logdir}\")\n",
    "#         ckpt = opt.resume\n",
    "#         print(f\"Resuming from checkpoint: {ckpt}\")\n",
    "#     else:\n",
    "#         assert os.path.isdir(opt.resume), opt.resume\n",
    "#         logdir = opt.resume.rstrip(\"/\")\n",
    "#         ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
    "\n",
    "#     opt.resume_from_checkpoint = ckpt\n",
    "#     base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*.yaml\")))\n",
    "#     opt.base = base_configs+opt.base\n",
    "#     _tmp = logdir.split(\"/\")\n",
    "#     nowname = _tmp[_tmp.index(\"logs\")+1]\n",
    "# else:\n",
    "#     if opt.name:\n",
    "#         name = \"_\"+opt.name\n",
    "#     elif opt.base:\n",
    "#         cfg_fname = os.path.split(opt.base[0])[-1]\n",
    "#         cfg_name = os.path.splitext(cfg_fname)[0]\n",
    "#         name = \"_\"+cfg_name\n",
    "#     else:\n",
    "#         name = \"\"\n",
    "#     nowname = now+name+opt.postfix\n",
    "#     logdir = os.path.join(\"logs\", nowname)\n",
    "\n",
    "# ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "# cfgdir = os.path.join(logdir, \"configs\")\n",
    "# seed_everything(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Using first stage also as cond stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /root/autodl-tmp/taming-transformers/ckpts/logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /root/autodl-tmp/taming-transformers/scripts/logs/vqgan_gumbel_f8/logs/transformer test\n",
      "\n",
      "  | Name              | Type          | Params\n",
      "----------------------------------------------------\n",
      "0 | first_stage_model | VQModel       | 72.1 M\n",
      "1 | permuter          | Identity      | 0     \n",
      "2 | transformer       | GPT           | 308 M \n",
      "3 | depthmodel        | CascadeMVSNet | 934 K \n",
      "4 | depth_loss        | SL1Loss       | 0     \n",
      "----------------------------------------------------\n",
      "381 M     Trainable params\n",
      "0         Non-trainable params\n",
      "381 M     Total params\n",
      "1,525.667 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011955022811889648,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation sanity check",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8be549914f4814a9cd765a1073d40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 23.69 GiB total capacity; 4.67 GiB already allocated; 156.94 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/taming-transformers/transformer_test.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m checkpoins \u001b[39m=\u001b[39m ModelCheckpoint(dirpath\u001b[39m=\u001b[39mckptdir, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                                 \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m                                 save_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                                 monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval/loss_diff\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                                 filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{epoch:02d}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39m\u001b[39mval/rec_loss:.2f}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m                                 )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/autodl-tmp/taming-transformers/transformer_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1193\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   1195\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1272\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1304\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m-> 1304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m   1306\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1368\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1372\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:109\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fetcher \u001b[39m=\u001b[39m dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(\n\u001b[1;32m    105\u001b[0m     dataloader, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_batches[dataloader_idx]\n\u001b[0;32m--> 109\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(dataloader, dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders)\n\u001b[1;32m    111\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:123\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 123\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    124\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:215\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 215\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mvalidation_step(step_kwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:236\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, step_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:219\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/models/cond_transformer.py:403\u001b[0m, in \u001b[0;36mNet2NetTransformer.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 403\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared_step(batch, batch_idx)\n\u001b[1;32m    404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval/loss\u001b[39m\u001b[39m\"\u001b[39m, loss, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, logger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_step\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/models/cond_transformer.py:368\u001b[0m, in \u001b[0;36mNet2NetTransformer.shared_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    366\u001b[0m x \u001b[39m=\u001b[39m rearrange(x, \u001b[39m'\u001b[39m\u001b[39mb v c h w -> b c h (v w)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    367\u001b[0m c \u001b[39m=\u001b[39m rearrange(c, \u001b[39m'\u001b[39m\u001b[39mb v c h w -> b c h (v w)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m logits, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x, c)\n\u001b[1;32m    369\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    370\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/models/cond_transformer.py:111\u001b[0m, in \u001b[0;36mNet2NetTransformer.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, c\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    110\u001b[0m     \u001b[39m# one step to produce the logits\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     _, z_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_to_z(x)\n\u001b[1;32m    112\u001b[0m     \u001b[39m# if not unconditioned:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     _, c_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_to_c(c)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/models/cond_transformer.py:224\u001b[0m, in \u001b[0;36mNet2NetTransformer.encode_to_z\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_to_z\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 224\u001b[0m     quant_z, _, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_stage_model\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m    225\u001b[0m     indices \u001b[39m=\u001b[39m info[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mview(quant_z\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermuter(indices)\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/models/vqgan.py:57\u001b[0m, in \u001b[0;36mVQModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     58\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv(h)\n\u001b[1;32m     59\u001b[0m     quant, emb_loss, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantize(h)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/modules/diffusionmodules/model.py:416\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mfor\u001b[39;00m i_level \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_resolutions):\n\u001b[1;32m    415\u001b[0m     \u001b[39mfor\u001b[39;00m i_block \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_res_blocks):\n\u001b[0;32m--> 416\u001b[0m         h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown[i_level]\u001b[39m.\u001b[39;49mblock[i_block](hs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], temb)\n\u001b[1;32m    417\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown[i_level]\u001b[39m.\u001b[39mattn) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    418\u001b[0m             h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown[i_level]\u001b[39m.\u001b[39mattn[i_block](h)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/modules/diffusionmodules/model.py:120\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, temb)\u001b[0m\n\u001b[1;32m    118\u001b[0m h \u001b[39m=\u001b[39m x\n\u001b[1;32m    119\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(h)\n\u001b[0;32m--> 120\u001b[0m h \u001b[39m=\u001b[39m nonlinearity(h)\n\u001b[1;32m    121\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(h)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/autodl-tmp/taming-transformers/taming/modules/diffusionmodules/model.py:31\u001b[0m, in \u001b[0;36mnonlinearity\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnonlinearity\u001b[39m(x):\n\u001b[1;32m     30\u001b[0m     \u001b[39m# swish\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m*\u001b[39;49mtorch\u001b[39m.\u001b[39;49msigmoid(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 23.69 GiB total capacity; 4.67 GiB already allocated; 156.94 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# init and save configs\n",
    "nowname =\"transformer test\"\n",
    "cfg = \"/root/autodl-tmp/taming-transformers/ckpts/logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml\"\n",
    "ckptdir =\"/root/autodl-tmp/taming-transformers/ckpts/logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\"\n",
    "configs = [OmegaConf.load(cfg)]\n",
    "cli = OmegaConf.from_dotlist(unknown)\n",
    "config = OmegaConf.merge(*configs, cli)\n",
    "config.model.params.ckpt_path = ckptdir\n",
    "\n",
    "# model\n",
    "model = instantiate_from_config(config.model)\n",
    "model.learning_rate = config.model.base_learning_rate\n",
    "model.image_key = \"imgs\"\n",
    "\n",
    "# data\n",
    "\n",
    "data = DataModuleFromConfig(batch_size=1)\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"/root/autodl-tmp/taming-transformers/scripts/logs/vqgan_gumbel_f8/logs\", name=nowname, default_hp_metric=False)\n",
    "\n",
    "checkpoins = ModelCheckpoint(dirpath=ckptdir, \n",
    "                                \n",
    "                                save_last=True, save_top_k=1, \n",
    "                                monitor=\"val/loss_diff\", mode=\"min\",\n",
    "                                filename=\"{epoch:02d}-{val/rec_loss:.2f}\"\n",
    "                                )\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "    logger=logger,\n",
    "\n",
    "    callbacks=[checkpoins],\n",
    "    max_epochs=20,\n",
    "    #resume_from_checkpoint=ckptdir,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    val_check_interval=1.0,\n",
    "    \n",
    "    \n",
    "\n",
    ")\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
