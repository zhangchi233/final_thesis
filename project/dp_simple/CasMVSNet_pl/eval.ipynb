{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import pytorch_ssim\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/D3Dnet/code')\n",
    "from model_d3c import Net\n",
    "import matplotlib.pyplot as plt\n",
    "from dcn.modules.deform_conv import *\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from einops import rearrange\n",
    "from torchvision import models\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trial,\n",
    "                    lambda_content, upscale_factor,\n",
    "                    in_channel, out_channel, model,):\n",
    "    \n",
    "    nf = trial.nf\n",
    "    lr = trial.lr\n",
    "    num_groups2 =trial.num_groups2\n",
    "    num_groups1 = trial.num_groups1\n",
    "\n",
    "    print(f'nf: {nf}, lambda_content: {lambda_content}, \\\n",
    "            upscale_factor:{upscale_factor}, in_channel: {in_channel}, \\\n",
    "            out_channel: {out_channel}, lr: {lr}')\n",
    "\n",
    "    configs = namedtuple('configs', ['nf',\n",
    "                                        'lambda_content', \n",
    "                                        'upscale_factor',\n",
    "                                         'seg_clusters', 'in_channel', 'out_channel', 'model','lr'])\n",
    "    configs.nf = nf\n",
    "    configs.lambda_content = lambda_content\n",
    "    configs.upscale_factor = upscale_factor\n",
    "    configs.in_channel = in_channel\n",
    "    configs.out_channel = out_channel\n",
    "    configs.model = model\n",
    "    configs.num_groups1 = num_groups1\n",
    "    configs.num_groups2 = num_groups2\n",
    "    configs.lr = lr\n",
    "    configs.seg_clusters=4\n",
    "\n",
    "    return Net(configs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/root/autodl-tmp/project/dp_simple/')\n",
    "#import ViT\n",
    "from torchvision import transforms as T\n",
    "from CasMVSNet_pl.models.mvsnet import CascadeMVSNet\n",
    "from CasMVSNet_pl.utils import load_ckpt\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset  \n",
    "from CasMVSNet_pl.utils import *\n",
    "from CasMVSNet_pl.datasets.dtu import DTUDataset \n",
    "from CasMVSNet_pl.metrics import *  \n",
    "from inplace_abn import ABN\n",
    "\n",
    "import pytorch_ssim\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CascadeMVSNet(n_depths=[8,32,48],\n",
    "                        interval_ratios=[1.0,2.0,4.0],\n",
    "                        num_groups=1,\n",
    "                        \n",
    "                        norm_act=ABN).cuda()\n",
    "load_ckpt(model, '/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/ckpts/_ckpt_epoch_10.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nf: 64, lambda_content: 100,             upscale_factor:1, in_channel: 3,             out_channel: 9, lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "trial = namedtuple('trial', ['nf', 'lr', 'num_groups1', 'num_groups2'])\n",
    "trial.nf = 64\n",
    "trial.lr = 1e-4\n",
    "trial.num_groups1 = 5\n",
    "trial.num_groups2 = 6\n",
    "refine_model = build_model(trial, 100,1, 3,9, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/root/autodl-tmp/project/dp_simple/ckpts/d3c_net_epoch=54.ckpt\"\n",
    "# refine_model.load_state_dict(torch.load(path)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict\n",
    "from datasets.utils import save_pfm, read_pfm\n",
    "import cv2\n",
    "import torch\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# for depth prediction\n",
    "from models.mvsnet import CascadeMVSNet\n",
    "from utils import load_ckpt\n",
    "from inplace_abn import ABN\n",
    "\n",
    "# for point cloud fusion\n",
    "from numba import jit\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # this increases inference speed a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True # this increases inference speed a little\n",
    "\n",
    "def get_opts():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--root_dir', type=str,\n",
    "                        default='/root/autodl-tmp/mvs_training/dtu',\n",
    "                        help='root directory of dtu dataset')\n",
    "    parser.add_argument('--dataset_name', type=str, default='dtu',\n",
    "                        choices=['dtu', 'tanks', 'blendedmvs'],\n",
    "                        help='which dataset to train/val')\n",
    "    parser.add_argument('--split', type=str, default='train',\n",
    "                        help='which split to evaluate')\n",
    "    parser.add_argument('--scan', type=str, default='scan7',\n",
    "                        help='specify scan to evaluate (must be in the split)')\n",
    "    parser.add_argument('--cpu', default=False, action='store_true',\n",
    "                        help='''use cpu to do depth inference.\n",
    "                                WARNING: It is going to be EXTREMELY SLOW!\n",
    "                                about 37s/view, so in total 30min/scan. \n",
    "                             ''')\n",
    "    # for depth prediction\n",
    "    parser.add_argument('--n_views', type=int, default=3,\n",
    "                        help='number of views (including ref) to be used in testing')\n",
    "    parser.add_argument('--depth_interval', type=float, default=2.65,\n",
    "                        help='depth interval unit in mm')\n",
    "    parser.add_argument('--n_depths', nargs='+', type=int, default=[8,32,48],\n",
    "                        help='number of depths in each level')\n",
    "    parser.add_argument('--interval_ratios', nargs='+', type=float, default=[1.0,2.0,4.0],\n",
    "                        help='depth interval ratio to multiply with --depth_interval in each level')\n",
    "    parser.add_argument('--num_groups', type=int, default=1, choices=[1, 2, 4, 8],\n",
    "                        help='number of groups in groupwise correlation, must be a divisor of 8')\n",
    "    parser.add_argument('--img_wh', nargs=\"+\", type=int, default=[640,512],\n",
    "                        help='resolution (img_w, img_h) of the image, must be multiples of 32')\n",
    "    parser.add_argument('--ckpt_path', type=str, default='/root/autodl-tmp/project/dp_simple/CasMVSNet_pl/ckpts/_ckpt_epoch_10.ckpt',\n",
    "                        help='pretrained checkpoint path to load')\n",
    "    parser.add_argument('--save_visual', default=False, action='store_true',\n",
    "                        help='save depth and proba visualization or not')\n",
    "\n",
    "    # for point cloud fusion\n",
    "    parser.add_argument('--conf', type=float, default=0.999,\n",
    "                        help='min confidence for pixel to be valid')\n",
    "    parser.add_argument('--min_geo_consistent', type=int, default=5,\n",
    "                        help='min number of consistent views for pixel to be valid')\n",
    "    parser.add_argument('--max_ref_views', type=int, default=400,\n",
    "                        help='max number of ref views (to limit RAM usage)')\n",
    "    parser.add_argument('--skip', type=int, default=1,\n",
    "                        help='''how many points to skip when creating the point cloud.\n",
    "                                Larger = fewer points and smaller file size.\n",
    "                                Ref: skip=10 creates ~= 3M points = 50MB file\n",
    "                                     skip=1 creates ~= 30M points = 500MB file\n",
    "                             ''')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "  \n",
    "\n",
    "\n",
    "def decode_batch(batch):\n",
    "    imgs = batch['imgs']\n",
    "    proj_mats = batch['proj_mats']\n",
    "    init_depth_min = batch['init_depth_min'].item()\n",
    "    depth_interval = batch['depth_interval'].item()\n",
    "    scan, vid = batch['scan_vid']\n",
    "    return imgs, proj_mats, init_depth_min, depth_interval, \\\n",
    "           scan, vid\n",
    "\n",
    "\n",
    "# define read_image and read_proj_mat for each dataset\n",
    "\n",
    "def read_image(dataset_name, root_dir, scan, vid):\n",
    "    if dataset_name == 'dtu':\n",
    "        return cv2.imread(os.path.join(root_dir,\n",
    "                    f'Rectified/{scan}_train/rect_{vid+1:03d}_3_r5000.png'))\n",
    "    if dataset_name == 'tanks':\n",
    "        return cv2.imread(os.path.join(root_dir, scan,\n",
    "                    f'images/{vid:08d}.jpg'))\n",
    "    if dataset_name == 'blendedmvs':\n",
    "        return cv2.imread(os.path.join(root_dir, scan,\n",
    "                    f'blended_images/{vid:08d}.jpg'))\n",
    "\n",
    "\n",
    "def read_refined_image(dataset_name, scan, vid):\n",
    "    return cv2.imread(f'results/{dataset_name}/image_refined/{scan}/{vid:08d}.png')\n",
    "\n",
    "\n",
    "def save_refined_image(image_refined, dataset_name, scan, vid):\n",
    "    cv2.imwrite(f'results/{dataset_name}/image_refined/{scan}/{vid:08d}.png',\n",
    "                image_refined)\n",
    "\n",
    "\n",
    "def read_proj_mat(dataset_name, dataset, scan, vid):\n",
    "    if dataset_name == 'dtu':\n",
    "        return dataset.proj_mats[vid][0][0].numpy()\n",
    "    if dataset_name in ['tanks', 'blendedmvs']:\n",
    "        return dataset.proj_mats[scan][vid][0][0].numpy()\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def xy_ref2src(xy_ref, depth_ref, P_world2ref,\n",
    "               depth_src, P_world2src, img_wh):\n",
    "    # create ref grid and project to ref 3d coordinate using depth_ref\n",
    "    xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_ref\n",
    "    xyz_ref_h = np.vstack((xyz_ref, np.ones_like(xy_ref[:1])))\n",
    "\n",
    "    P = (P_world2src @ np.ascontiguousarray(np.linalg.inv(P_world2ref)))[:3]\n",
    "    # project to src 3d coordinate using P_world2ref and P_world2src\n",
    "    xyz_src_h = P @ xyz_ref_h.reshape(4,-1)\n",
    "    xy_src = xyz_src_h[:2]/xyz_src_h[2:3]\n",
    "    xy_src = xy_src.reshape(2, img_wh[1], img_wh[0])\n",
    "\n",
    "    return xy_src\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def xy_src2ref(xy_ref, xy_src, depth_ref, P_world2ref,\n",
    "               depth_src2ref, P_world2src, img_wh):\n",
    "    # project xy_src back to ref view using the sampled depth\n",
    "    xyz_src = np.vstack((xy_src, np.ones_like(xy_src[:1]))) * depth_src2ref\n",
    "    xyz_src_h = np.vstack((xyz_src, np.ones_like(xy_src[:1])))\n",
    "    P = (P_world2ref @ np.ascontiguousarray(np.linalg.inv(P_world2src)))[:3]\n",
    "    xyz_ref_h = P @ xyz_src_h.reshape(4,-1)\n",
    "    depth_ref_reproj = xyz_ref_h[2].reshape(img_wh[1], img_wh[0])\n",
    "    xy_ref_reproj = xyz_ref_h[:2]/xyz_ref_h[2:3]\n",
    "    xy_ref_reproj = xy_ref_reproj.reshape(2, img_wh[1], img_wh[0])\n",
    "\n",
    "    # check |p_reproj-p_1| < 1\n",
    "    pixel_diff = xy_ref_reproj - xy_ref\n",
    "    mask_pixel_reproj = (pixel_diff[0]**2+pixel_diff[1]**2)<1\n",
    "\n",
    "    # check |d_reproj-d_1| / d_1 < 0.01\n",
    "    mask_depth_reproj = np.abs((depth_ref_reproj-depth_ref)/depth_ref)<0.01\n",
    "\n",
    "    mask_geo = mask_pixel_reproj & mask_depth_reproj\n",
    "\n",
    "    return depth_ref_reproj, mask_geo\n",
    "\n",
    "\n",
    "def check_geo_consistency(depth_ref, P_world2ref,\n",
    "                          depth_src, P_world2src,\n",
    "                          image_ref, image_src,\n",
    "                          img_wh):\n",
    "    \"\"\"\n",
    "    Check the geometric consistency between ref and src views.\n",
    "    \"\"\"\n",
    "    xy_ref = np.mgrid[:img_wh[1],:img_wh[0]][::-1].astype(np.float32)\n",
    "    xy_src = xy_ref2src(xy_ref, depth_ref, P_world2ref,\n",
    "                        depth_src, P_world2src, img_wh)\n",
    "\n",
    "    # Sample the depth of xy_src using bilinear interpolation\n",
    "    depth_src2ref = cv2.remap(depth_src,\n",
    "                              xy_src[0].astype(np.float32),\n",
    "                              xy_src[1].astype(np.float32),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    image_src2ref = cv2.remap(image_src,\n",
    "                              xy_src[0].astype(np.float32),\n",
    "                              xy_src[1].astype(np.float32),\n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    depth_ref_reproj, mask_geo = \\\n",
    "        xy_src2ref(xy_ref, xy_src, depth_ref, P_world2ref, \n",
    "                   depth_src2ref, P_world2src, img_wh)\n",
    "\n",
    "    depth_ref_reproj[~mask_geo] = 0\n",
    "    image_src2ref[~mask_geo] = 0\n",
    "    \n",
    "    return depth_ref_reproj, mask_geo, image_src2ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refine_model = refine_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_opts()\n",
    "dataset = dataset_dict['dtu'] \\\n",
    "            (args.root_dir, args.split,\n",
    "                n_views=args.n_views, depth_interval=args.depth_interval,\n",
    "                img_wh=tuple(args.img_wh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.scan = 'scan70'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine = False\n",
    "read_gt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating depth and confidence predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:04<00:00, 11.14it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.scan:\n",
    "    scans = [args.scan]\n",
    "else: # evaluate on all scans in dataset\n",
    "    scans = dataset.scans\n",
    "\n",
    "# Step 1. Create depth estimation and probability for each scan\n",
    "model = CascadeMVSNet(n_depths=args.n_depths,\n",
    "                        interval_ratios=args.interval_ratios,\n",
    "                        num_groups=args.num_groups,\n",
    "                        norm_act=ABN)\n",
    "device = 'cpu' if args.cpu else 'cuda:0'\n",
    "model.to(device)\n",
    "load_ckpt(model, args.ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "depth_dir = f'./results/{args.dataset_name}/depth'\n",
    "print('Creating depth and confidence predictions...')\n",
    "if args.scan:\n",
    "    data_range = [i for i, x in enumerate(dataset.metas) if x[0] == args.scan and x[1]==3]\n",
    "else:\n",
    "    data_range = range(len(dataset))\n",
    "for i in tqdm(data_range):\n",
    "    imgs, proj_mats, init_depth_min, depth_interval, \\\n",
    "        scan, vid = decode_batch(dataset[i])\n",
    "    \n",
    "    \n",
    "    os.makedirs(os.path.join(depth_dir, scan), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.unsqueeze(0).to(device)\n",
    "        if refine == True:\n",
    "            imgs = refine_model.predict_step(imgs)\n",
    "        proj_mats = proj_mats.unsqueeze(0).to(device)\n",
    "        results = model(imgs, proj_mats, init_depth_min, depth_interval)\n",
    "    \n",
    "    depth = results['depth_0'][0].cpu().numpy()\n",
    "    depth = np.nan_to_num(depth) # change nan to 0\n",
    "    proba = results['confidence_2'][0].cpu().numpy() # NOTE: this is 1/4 scale!\n",
    "    proba = np.nan_to_num(proba) # change nan to 0\n",
    "    save_pfm(os.path.join(depth_dir, f'{scan}/depth_{vid:04d}.pfm'), depth)\n",
    "    save_pfm(os.path.join(depth_dir, f'{scan}/proba_{vid:04d}.pfm'), proba)\n",
    "    if args.save_visual:\n",
    "        mi = np.min(depth[depth>0])\n",
    "        ma = np.max(depth)\n",
    "        depth = (depth-mi)/(ma-mi+1e-8)\n",
    "        depth = (255*depth).astype(np.uint8)\n",
    "        depth_img = cv2.applyColorMap(depth, cv2.COLORMAP_JET)\n",
    "        cv2.imwrite(os.path.join(depth_dir, f'{scan}/depth_visual_{vid:04d}.jpg'),\n",
    "                    depth_img)\n",
    "        cv2.imwrite(os.path.join(depth_dir, f'{scan}/proba_visual_{vid:04d}.jpg'),\n",
    "                    (255*(proba>args.conf)).astype(np.uint8))\n",
    "    del imgs, proj_mats, results\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing point clouds...\n",
      "Processing scan70 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:10<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan70 contains 0.02 M points\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Perform depth filtering and fusion\n",
    "point_dir = f'results/{args.dataset_name}/points'\n",
    "os.makedirs(point_dir, exist_ok=True)\n",
    "print('Fusing point clouds...')\n",
    "\n",
    "for scan in scans:\n",
    "    print(f'Processing {scan} ...')\n",
    "    # buffers for the final vertices of this scan\n",
    "    vs = []\n",
    "    v_colors = []\n",
    "    # buffers storing the refined data of each ref view\n",
    "    os.makedirs(f'results/{args.dataset_name}/image_refined/{scan}', exist_ok=True)\n",
    "    image_refined = set()\n",
    "    depth_refined = {}\n",
    "    for meta in tqdm(list(filter(lambda x: x[0]==scan and x[1]==3, dataset.metas))[:args.max_ref_views]):\n",
    "        try:\n",
    "            ref_vid = meta[2]\n",
    "            if ref_vid in image_refined: # not yet refined actually\n",
    "                image_ref = read_refined_image(args.dataset_name, scan, ref_vid)\n",
    "                depth_ref = depth_refined[ref_vid]\n",
    "            else:\n",
    "                image_ref = read_image(args.dataset_name, args.root_dir, scan, ref_vid)\n",
    "                image_ref = cv2.resize(image_ref, tuple(args.img_wh),\n",
    "                                        interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB\n",
    "                if read_gt:\n",
    "                    depth_ref = read_pfm(f'{args.root_dir}/Depths/{scan}_train/depth_map_{ref_vid:04d}.pfm')[0]\n",
    "                    depth_ref = cv2.resize(depth_ref, tuple(args.img_wh),\n",
    "                                            interpolation=cv2.INTER_LINEAR)\n",
    "                else:\n",
    "                    depth_ref = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                            f'{scan}/depth_{ref_vid:04d}.pfm')[0]\n",
    "            if read_gt:\n",
    "                proba_ref = np.ones_like(depth_ref)\n",
    "            proba_ref = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                    f'{scan}/proba_{ref_vid:04d}.pfm')[0]\n",
    "            proba_ref = cv2.resize(proba_ref, None, fx=4, fy=4,\n",
    "                                    interpolation=cv2.INTER_LINEAR)\n",
    "            mask_conf = proba_ref > args.conf # confidence mask\n",
    "            P_world2ref = read_proj_mat(args.dataset_name, dataset, scan, ref_vid)\n",
    "            \n",
    "            src_vids = meta[3]\n",
    "            mask_geos = []\n",
    "            depth_ref_reprojs = [depth_ref]\n",
    "            image_src2refs = [image_ref]\n",
    "            # for each src view, check the consistency and refine depth\n",
    "            for src_vid in src_vids:\n",
    "                if src_vid in image_refined: # use refined data of previous runs\n",
    "                    image_src = read_refined_image(args.dataset_name, scan, src_vid)\n",
    "                    depth_src = depth_refined[src_vid]\n",
    "                else:\n",
    "                    image_src = read_image(args.dataset_name, args.root_dir, scan, src_vid)\n",
    "                    image_src = cv2.resize(image_src, tuple(args.img_wh),\n",
    "                                            interpolation=cv2.INTER_LINEAR)[:,:,::-1] # to RGB\n",
    "                    depth_src = read_pfm(f'results/{args.dataset_name}/depth/' \\\n",
    "                                            f'{scan}/depth_{src_vid:04d}.pfm')[0]\n",
    "                    depth_refined[src_vid] = depth_src\n",
    "                P_world2src = read_proj_mat(args.dataset_name, dataset, scan, src_vid)\n",
    "                depth_ref_reproj, mask_geo, image_src2ref = \\\n",
    "                    check_geo_consistency(depth_ref, P_world2ref,\n",
    "                                            depth_src, P_world2src,\n",
    "                                            image_ref, image_src, tuple(args.img_wh))\n",
    "                depth_ref_reprojs += [depth_ref_reproj]\n",
    "                image_src2refs += [image_src2ref]\n",
    "                mask_geos += [mask_geo]\n",
    "            mask_geo_sum = np.sum(mask_geos, 0)\n",
    "            mask_geo_final = mask_geo_sum >= args.min_geo_consistent\n",
    "            depth_refined[ref_vid] = \\\n",
    "                (np.sum(depth_ref_reprojs, 0)/(mask_geo_sum+1)).astype(np.float32)\n",
    "            image_refined_ = \\\n",
    "                np.sum(image_src2refs, 0)/np.expand_dims((mask_geo_sum+1), -1)\n",
    "\n",
    "            image_refined.add(ref_vid)\n",
    "            save_refined_image(image_refined_, args.dataset_name, scan, ref_vid)\n",
    "            mask_final = mask_conf & mask_geo_final\n",
    "            \n",
    "            # create the final points\n",
    "            xy_ref = np.mgrid[:args.img_wh[1],:args.img_wh[0]][::-1]\n",
    "            xyz_ref = np.vstack((xy_ref, np.ones_like(xy_ref[:1]))) * depth_refined[ref_vid]\n",
    "            xyz_ref = xyz_ref.transpose(1,2,0)[mask_final].T # (3, N)\n",
    "            color = image_refined_[mask_final] # (N, 3)\n",
    "            xyz_ref_h = np.vstack((xyz_ref, np.ones_like(xyz_ref[:1])))\n",
    "            xyz_world = (np.linalg.inv(P_world2ref) @ xyz_ref_h).T # (N, 4)\n",
    "            xyz_world = xyz_world[::args.skip, :3]\n",
    "            color = color[::args.skip]\n",
    "            \n",
    "            # append to buffers\n",
    "            vs += [xyz_world]\n",
    "            v_colors += [color]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # some scenes might not have depth prediction due to too few valid src views\n",
    "            print(f'Skipping view {ref_vid} due to too few valid source views...')\n",
    "            continue\n",
    "\n",
    "    # clear refined buffer\n",
    "    image_refined.clear()\n",
    "    depth_refined.clear()\n",
    "    shutil.rmtree(f'results/{args.dataset_name}/image_refined/{scan}')\n",
    "\n",
    "    # process all points in the buffers\n",
    "    vs = np.ascontiguousarray(np.vstack(vs).astype(np.float32))\n",
    "    v_colors = np.vstack(v_colors).astype(np.uint8)\n",
    "    print(f'{scan} contains {len(vs)/1e6:.2f} M points')\n",
    "    vs.dtype = [('x', 'f4'), ('y', 'f4'), ('z', 'f4')]\n",
    "    v_colors.dtype = [('red', 'u1'), ('green', 'u1'), ('blue', 'u1')]\n",
    "\n",
    "    vertex_all = np.empty(len(vs), vs.dtype.descr+v_colors.dtype.descr)\n",
    "    for prop in vs.dtype.names:\n",
    "        vertex_all[prop] = vs[prop][:, 0]\n",
    "    for prop in v_colors.dtype.names:\n",
    "        vertex_all[prop] = v_colors[prop][:, 0]\n",
    "    if read_gt:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}_gt.ply')\n",
    "    elif refine:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}_refine.ply')\n",
    "    \n",
    "    else:\n",
    "        el = PlyElement.describe(vertex_all, 'vertex')\n",
    "        PlyData([el]).write(f'{point_dir}/{scan}.ply')\n",
    "    del vertex_all, vs, v_colors\n",
    "shutil.rmtree(f'results/{args.dataset_name}/image_refined')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
